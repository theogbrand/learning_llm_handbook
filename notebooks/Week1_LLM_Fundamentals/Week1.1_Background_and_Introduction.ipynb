{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/theogbrand/learning_llm_handbook/blob/main/notebooks/Week1_LLM_Fundamentals/Week1_LLM_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: LLM Fundamentals and Development Environment Setup\n",
    "\n",
    "**Resource Required**: Google Colab (T4) GPU or Provisioned GPU (RunPod 1x A10G/L40s recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "* Set up your development environment and get familiar with your development environment set up\n",
    "* Run ``nvidia-smi`` and see the details of your GPU\n",
    "* Get familiar with dependency management, either though pip or conda\n",
    "* Run the simple example below to make sure your development environment is set up correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ NOTE: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers>=4.40.1 accelerate>=0.27.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we will use Hugging Face's Transformers library to train and test our own models. Let's get familiar with the library by loading a pre-trained model and using it to generate text.\n",
    "\n",
    "`Phi-3-mini-4k-instruct` is a small, fast, and efficient model that can generate text in a variety of languages. It is a decoder-only model that uses the GPT architecture.\n",
    "\n",
    "The first interesting thing to note is that the model is loaded onto the GPU. This is because the `device_map` argument is set to `cuda`, which is the GPU.\n",
    "\n",
    "The second interesting thing to note is that the model is loaded using the `AutoModelForCausalLM` class. This is a convenience class that loads the model and the associated tokenizer. There are other classes that HuggingFace offers for different model types.\n",
    "\n",
    "The third interesting thing to note is that the model is loaded using the `pipeline` function. This is a convenience function that loads the model and the associated tokenizer and returns a pipeline for easy inference.\n",
    "\n",
    "The reason to use the `pipeline` function for simple inference will become more obvious later, when we analyze the sequential steps taken for a model to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Phi-3-mini-4k-instruct` is a chat-style/instruction-tuned model, which means it is trained to generate text in a conversational manner. This is why it is capable of chatting with us like a person would.\n",
    "\n",
    "Chat-style models are trained from base models through a process called instruction tuning, a type of supervised finetuning. The difference between vanilla finetuning and instruction tuning is that finetuning involves training the model on a specific task, while instruction tuning involves training the model to follow a specific set of instructions.\n",
    "\n",
    "Without instruction tuning, the model is only trained to generate text to complete the next word in a sequence. Before a model is instruction-tuned, the model is usually known as a base model, which was trained using a training objective to predict the next token in a sequence. We will explore the mechanics of how this works in greater detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", # chosen for its small size and fast inference speed on smaller GPUs\n",
    "    device_map=\"cuda\", # requires GPU\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the nature of reality?\"}\n",
    "]\n",
    "\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load a base model, GPT-2 by OpenAI, which has not been instruction tuned and see how the model generates text for the same given prompt.\n",
    "\n",
    "You can see that the model is not able to generate a coherent response, and the text is not very meaningful. This is because the model is not trained to generate text in a conversational manner, but only to predict the words following the prompt.\n",
    "\n",
    "After instruction tuning was discovered, it became the standard way for researchers and developers to train models capable of accomplishing a wide range of tasks. The products we know and love today like ChatGPT, Claude, and Gemini are all based on instruction tuned models.\n",
    "\n",
    "Instruction tuning was one of the fundamental breakthroughs which resulted in the usefulness of LLMs we know today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token # the details to what these are will be covered in detail later, feel free to read ahead for now\n",
    "model.config.pad_token_id = model.config.eos_token_id \n",
    "\n",
    "prompt = \"What is the nature of reality?\"\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=100,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "# Instead of pipeline, we use the model.generate method to generate text\n",
    "gen_tokens = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "print(gen_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
