{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing PyTorch\n",
    "\n",
    "This section runs through the PyTorch API you will commonly use for building neural network.\n",
    "We will start with how PyTorch works for an image prediction task and then move on to how they are used in LLMs.\n",
    "The same concepts will be used for building the first LLM in this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offeres many datasets which can be used for training and testing. The `torchvision.datasets` module contains `Dataset` objects for many\n",
    "real-world vision data like CIFAR, COCO ([full list\n",
    "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
    "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
    "includes two arguments: `transform` and `target_transform` to modify the\n",
    "samples and labels respectively.\n",
    "\n",
    "For this example, the images are converted to tensors and the labels are converted to one-hot encoded vectors to be trained on a classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
    "iterable over our dataset, and supports automatic batching, sampling,\n",
    "shuffling and multiprocess data loading. Here we define a batch size of\n",
    "64, i.e. each element in the dataloader iterable will return a batch of\n",
    "64 features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models\n",
    "\n",
    "When we are creating a neural network to train on a task in PyTorch, we create a class that inherits\n",
    "from\n",
    "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "We define the layers of the network in the `__init__` function and\n",
    "specify how data will pass through the network in the `forward`\n",
    "function. To accelerate operations in the neural network, we move it to\n",
    "the\n",
    "[accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
    "such as CUDA, MPS, MTIA, or XPU. If the current accelerator is\n",
    "available, we will use it. Otherwise, we use the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the Model Parameters\n",
    "\n",
    "To train a model, we need a [loss\n",
    "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
    "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training\n",
    "dataset (fed to it in batches), and backpropagates the prediction error\n",
    "to adjust the model\\'s parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader): # a batch is a parallel set of images and labels. Instead of computing the loss and gradient for each image individually, we compute the loss and gradient for a batch of images at once. This helps speed up the training process. Doing so became possible when GPUs became more powerful and more memory was available.\n",
    "        X, y = X.to(device), y.to(device) # common practice to move data to the device (GPU/CPU)\n",
    "\n",
    "        # Compute prediction error for the batch\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation (we will cover this in more detail later in the course, get the general intuition here)\n",
    "        loss.backward() # compute the gradient of the loss function with respect to all the model parameters. \n",
    "        optimizer.step() # update all the weights of the model, to reduce the loss.\n",
    "        optimizer.zero_grad() # zero the gradients after each iteration, so they can be computed again for the next iteration. If you don't do this, the gradients would accumulate and the model will not learn properly.\n",
    "\n",
    "        if batch % 100 == 0: # we only print the loss every 100 batches to avoid cluttering the console with the loss value for every batch.\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the model\\'s performance against the test dataset to\n",
    "ensure it is learning and not overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (_epochs_).\n",
    "During each epoch, the model learns parameters to make better\n",
    "predictions. We print the model\\'s accuracy and loss at each epoch;\n",
    "we\\'d like to see the accuracy increase and the loss decrease with every\n",
    "epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Models\n",
    "\n",
    "A common way to save a model is to serialize the internal state\n",
    "dictionary (containing the model parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models\n",
    "\n",
    "The process for loading a model includes re-creating the model structure\n",
    "and loading the state dictionary into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can now be used to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval() # set the model to evaluation mode, this is important to do when we are not training the model\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPs for Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll explore how MLPs (Multi-Layer Perceptrons) can be used for regression tasks. Unlike the classification example above where we predicted discrete categories, regression predicts continuous values. This concept is fundamental to understanding how Language Models work and how we can adapt them for different tasks.\n",
    "\n",
    "MLPs for regression follow the same basic architecture as those for classification, with a few key differences:\n",
    "1. The output layer typically has a single neuron (for single-value prediction)\n",
    "2. There's no activation function in the final layer (to allow for unbounded continuous output)\n",
    "3. We use a different loss function, like Mean Squared Error (MSE) instead of Cross Entropy\n",
    "\n",
    "This flexibility in the output layer design is especially relevant when we later explore Language Models, where we can replace or adapt the \"head\" of the model for different downstream tasks while keeping the transformer backbone intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train = torch.tensor(\n",
    "    [258.0, 270.0, 294.0, 320.0, 342.0, 368.0, 396.0, 446.0, 480.0, 586.0]\n",
    ").view(-1, 1)\n",
    "\n",
    "y_train = torch.tensor(\n",
    "    [236.4, 234.4, 252.8, 298.6, 314.2, 342.2, 360.8, 368.0, 391.2, 390.8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.xlabel(\"Feature variable\")\n",
    "plt.ylabel(\"Target variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "- No architecture changes besides the output unit\n",
    "\n",
    "When adapting neural networks for regression, the key change is in the output layer. Notice how the PyTorchMLP class below uses a single output unit with no activation function, allowing it to predict continuous values instead of class probabilities.\n",
    "\n",
    "This pattern of adapting the \"head\" (final layers) of a neural network while keeping the main architecture intact is essential in Language Model fine-tuning, where we often:\n",
    "1. Keep the pre-trained transformer backbone frozen (containing the learned knowledge)\n",
    "2. Replace or modify only the output layers for specific tasks (classification, regression, etc.)\n",
    "3. Train only these new layers, or fine-tune the entire model with a small learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data\n",
    "\n",
    "Normalization is a critical step for both regression tasks and language modeling. In regression, we normalize inputs and targets to improve convergence and performance. Similarly, in language models:\n",
    "\n",
    "1. Input text tokens are embedded and normalized within the model\n",
    "2. Layer normalization is used throughout transformer architectures\n",
    "3. Output logits may be scaled or normalized depending on the task\n",
    "\n",
    "The normalization process below converts our data to have zero mean and unit variance, making the training process more stable and efficient. This same concept applies when working with language model outputs, where proper scaling helps maintain numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = X_train.mean(), X_train.std()\n",
    "y_mean, y_std = y_train.mean(), y_train.std()\n",
    "\n",
    "X_train_norm = (X_train - x_mean) / x_std\n",
    "y_train_norm = (y_train - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.features = X\n",
    "        self.targets = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.targets.shape[0]\n",
    "\n",
    "\n",
    "train_ds = MyDataset(X_train_norm, y_train_norm)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "The training loop for regression models follows the same pattern used for classification and language models:\n",
    "\n",
    "1. Forward pass: Generate predictions from inputs\n",
    "2. Calculate loss: Measure the error between predictions and targets (using MSE for regression)\n",
    "3. Backward pass: Compute gradients with respect to model parameters\n",
    "4. Update weights: Adjust parameters to minimize the loss\n",
    "\n",
    "When fine-tuning language models, we follow this exact same process, with these key differences:\n",
    "- We typically use a language modeling loss (like cross-entropy over token predictions)\n",
    "- We might freeze certain layers to preserve pretrained knowledge\n",
    "- We often use specialized optimizers like AdamW with weight decay and learning rate schedulers\n",
    "\n",
    "The principle remains consistent: iteratively update model parameters to minimize prediction error. This fundamental approach works across all neural network architectures, from simple MLPs to complex transformer-based language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = PyTorchMLP(num_features=1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "loss_list = []\n",
    "train_acc_list, val_acc_list = [], []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "        logits = model(features)\n",
    "        loss = F.mse_loss(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not batch_idx % 250:\n",
    "            ### LOGGING\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "                f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "                f\" | Train Loss: {loss:.2f}\"\n",
    "            )\n",
    "        loss_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and Generate Predictions\n",
    "\n",
    "A key aspect of working with both regression models and language models is handling data at inference time. For regression, we need to:\n",
    "\n",
    "1. Normalize new inputs using the same statistics from training\n",
    "2. Run the inference in evaluation mode (model.eval())\n",
    "3. Convert the normalized predictions back to the original scale\n",
    "\n",
    "For language models, a similar process applies when generating text or making predictions:\n",
    "1. We transform input text to token IDs\n",
    "2. We run the model in evaluation mode, often with torch.no_grad() for efficiency\n",
    "3. We may use sampling strategies like temperature scaling or top-k filtering on the output logits\n",
    "\n",
    "This step of properly preparing inputs and processing outputs is crucial for applying models to real-world data. In later weeks, we'll see how this inference process works with language models when generating text or extracting embeddings for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "X_range = torch.arange(150, 800, 0.1).view(-1, 1)\n",
    "X_range_norm = (X_range - x_mean) / x_std\n",
    "\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    y_mlp_norm = model(X_range_norm)\n",
    "\n",
    "# MLP returns normalized predictions\n",
    "# undo normalization of preditions for plotting\n",
    "y_mlp = y_mlp_norm * y_std + y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.scatter(X_train, y_train, label=\"Training points\")\n",
    "plt.plot(X_range, y_mlp, color=\"C1\", label=\"MLP fit\", linestyle=\"-\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Feature variable\")\n",
    "plt.ylabel(\"Target variable\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"mlp.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Language Model Heads\n",
    "\n",
    "The regression example above demonstrates several concepts that directly transfer to working with language models:\n",
    "\n",
    "1. **Adaptable output layers**: Just as we modified the MLP's output layer for regression, language models can be adapted with different \"heads\" for various tasks:\n",
    "   - Text generation: Uses the standard language modeling head\n",
    "   - Classification: Replaces the head with a classifier (like sentiment analysis)\n",
    "   - Regression: Uses a head similar to our example (for tasks like text quality scoring)\n",
    "   - Embedding extraction: Often removes the head entirely to get vector representations\n",
    "\n",
    "2. **Transfer learning pattern**: The workflow follows a consistent pattern:\n",
    "   - Start with a pre-trained model (like our MLP architecture or a transformer)\n",
    "   - Adapt the output layer for the specific task\n",
    "   - Train on task-specific data, possibly freezing earlier layers\n",
    "   - Apply the model to new data with proper pre/post-processing\n",
    "\n",
    "3. **PyTorch patterns**: The same PyTorch patterns apply across all neural network types:\n",
    "   - Building models by subclassing nn.Module\n",
    "   - Using DataLoader for efficient batch processing\n",
    "   - Following the forward-loss-backward-update training loop\n",
    "   - Setting model.eval() for inference\n",
    "   - Saving and loading model weights\n",
    "\n",
    "In the coming weeks, we'll build on these foundations to work with transformer-based language models, where the principles remain the same but the architectures become more sophisticated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
