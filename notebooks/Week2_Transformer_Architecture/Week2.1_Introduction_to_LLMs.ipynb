{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Large Language Models: A Step-by-Step Journey\n",
    "\n",
    "## Our Goal\n",
    "\n",
    "In this notebook, we will understand how language models predict text by following the complete process of predicting \"we love deep learning\" word by word. We'll explore four fundamental concepts that make modern language models like ChatGPT work:\n",
    "\n",
    "1. **Forward Pass**: How models generate predictions\n",
    "2. **Loss Calculation**: How we measure prediction quality\n",
    "3. **Backpropagation**: How we identify what needs improvement\n",
    "4. **Gradient Descent**: How we make those improvements\n",
    "\n",
    "## Our Vocabulary and Target\n",
    "\n",
    "We'll work with a simple vocabulary to keep things clear and manageable. Our model will learn to predict each word in our target sequence step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY: ['<BOS>', 'we', 'love', 'deep', 'learning', '<EOS>', 'the', 'is', 'great', 'model', 'hello', 'world']\n",
      "TARGET SEQUENCE: ['we', 'love', 'deep', 'learning']\n",
      "VOCABULARY SIZE: 12\n",
      "\n",
      "Initial model parameters (simplified representation):\n",
      "Layer 1 weights: 12 parameters\n",
      "Layer 2 weights: 12 parameters\n",
      "Output weights: 12 parameters\n"
     ]
    }
   ],
   "source": [
    "# Setup our simple vocabulary and target sequence\n",
    "VOCAB = [\"<BOS>\", \"we\", \"love\", \"deep\", \"learning\", \"<EOS>\", \"the\", \"is\", \"great\", \"model\", \"hello\", \"world\"]\n",
    "target_sequence = [\"we\", \"love\", \"deep\", \"learning\"]\n",
    "\n",
    "print(\"VOCABULARY:\", VOCAB)\n",
    "print(\"TARGET SEQUENCE:\", target_sequence)\n",
    "print(\"VOCABULARY SIZE:\", len(VOCAB))\n",
    "\n",
    "# Initialize simple model parameters (weights) - these will be updated during training\n",
    "# In real models, these would be millions or billions of parameters\n",
    "model_parameters = {\n",
    "    'layer1_weights': [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, 0.8, -0.2, 0.3, -0.5, 0.7, -0.4],\n",
    "    'layer2_weights': [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, -0.1, 0.5, -0.3, 0.4, -0.6, 0.1],\n",
    "    'output_weights': [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, -0.3, 0.2, 0.6, -0.4, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "print(\"\\nInitial model parameters (simplified representation):\")\n",
    "print(\"Layer 1 weights:\", len(model_parameters['layer1_weights']), \"parameters\")\n",
    "print(\"Layer 2 weights:\", len(model_parameters['layer2_weights']), \"parameters\") \n",
    "print(\"Output weights:\", len(model_parameters['output_weights']), \"parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Logits?\n",
    "\n",
    "Logits are the raw numerical scores that a language model assigns to every word in its vocabulary when predicting the next word. Think of logits as the model's initial \"gut feeling\" about how likely each word is to come next, before any normalization.\n",
    "\n",
    "**Key Properties of Logits:**\n",
    "\n",
    "- They can be any real number (positive, negative, large, small)\n",
    "- Higher logits indicate the model thinks a word is more likely\n",
    "- Lower logits indicate the model thinks a word is less likely\n",
    "- They are computed by passing the current context through the neural network layers\n",
    "\n",
    "Let's see what logits look like when our model tries to predict the first word after the beginning-of-sequence token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit scores for predicting first word after <BOS>:\n",
      "==================================================\n",
      "the       :    3.2\n",
      "hello     :    2.8\n",
      "we        :    2.1 <- Our target!\n",
      "is        :    1.5\n",
      "love      :   -0.5\n",
      "deep      :   -1.2\n",
      "learning  :   -2.0\n",
      "<EOS>     :  -10.0\n",
      "\n",
      "Notice how 'the' has the highest logit (3.2) because it's\n",
      "the most common way to start sentences in English.\n",
      "Our target word 'we' has a logit of 2.1, which is decent\n",
      "but not the highest - the model will need training to improve this!\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_logits():\n",
    "    # Pre-calculated realistic logit values for predicting first word after <BOS>\n",
    "    logits_after_bos = {\n",
    "        \"we\": 2.1,      # Target word - decent score\n",
    "        \"the\": 3.2,     # Highest - most common starter\n",
    "        \"hello\": 2.8,   # High - common greeting\n",
    "        \"is\": 1.5,      # Medium - possible starter\n",
    "        \"love\": -0.5,   # Low - uncommon starter\n",
    "        \"deep\": -1.2,   # Lower - rare starter\n",
    "        \"learning\": -2.0, # Very low - very rare starter\n",
    "        \"<EOS>\": -10.0, # Impossible - can't start with end token\n",
    "    }\n",
    "    \n",
    "    print(\"Logit scores for predicting first word after <BOS>:\")\n",
    "    print(\"=\" * 50)\n",
    "    for word, logit in sorted(logits_after_bos.items(), key=lambda x: x[1], reverse=True):\n",
    "        marker = \" <- Our target!\" if word == \"we\" else \"\"\n",
    "        print(f\"{word:10}: {logit:6.1f}{marker}\")\n",
    "    \n",
    "    print(f\"\\nNotice how 'the' has the highest logit ({logits_after_bos['the']}) because it's\")\n",
    "    print(\"the most common way to start sentences in English.\")\n",
    "    print(f\"Our target word 'we' has a logit of {logits_after_bos['we']}, which is decent\")\n",
    "    print(\"but not the highest - the model will need training to improve this!\")\n",
    "    \n",
    "    return logits_after_bos\n",
    "\n",
    "# Run the demonstration\n",
    "logits_step1 = demonstrate_logits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Logits to Probabilities: The Softmax Operation\n",
    "\n",
    "Raw logits are useful for the model internally, but they're not easy for us to interpret. We need to convert them into probabilities - numbers between 0 and 1 that sum to exactly 1.0. This conversion is done using the **softmax function**.\n",
    "\n",
    "**The Softmax Process:**\n",
    "\n",
    "1. Calculate e^(logit) for each word (this makes all values positive)\n",
    "2. Sum all these exponential values\n",
    "3. Divide each exponential value by the sum\n",
    "\n",
    "This ensures we get valid probabilities that represent the model's confidence in each possible next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFTMAX CONVERSION: From Logits to Probabilities\n",
      "=======================================================\n",
      "STEP 1: Calculate e^(logit) for each word\n",
      "----------------------------------------\n",
      "e^(  2.1) =     8.17  for 'we'\n",
      "e^(  3.2) =    24.53  for 'the'\n",
      "e^(  2.8) =    16.44  for 'hello'\n",
      "e^(  1.5) =     4.48  for 'is'\n",
      "e^( -0.5) =     0.61  for 'love'\n",
      "e^( -1.2) =     0.30  for 'deep'\n",
      "e^( -2.0) =     0.14  for 'learning'\n",
      "e^(-10.0) =     0.00  for '<EOS>'\n",
      "\n",
      "STEP 2: Sum all exponential values\n",
      "-----------------------------------\n",
      "Total sum = 54.67\n",
      "\n",
      "STEP 3: Calculate final probabilities\n",
      "--------------------------------------\n",
      "P(we      ) =     8.17 / 54.67 = 0.1494 <- Our target!\n",
      "P(the     ) =    24.53 / 54.67 = 0.4488\n",
      "P(hello   ) =    16.44 / 54.67 = 0.3008\n",
      "P(is      ) =     4.48 / 54.67 = 0.0820\n",
      "P(love    ) =     0.61 / 54.67 = 0.0111\n",
      "P(deep    ) =     0.30 / 54.67 = 0.0055\n",
      "P(learning) =     0.14 / 54.67 = 0.0025\n",
      "P(<EOS>   ) =     0.00 / 54.67 = 0.0000\n",
      "\n",
      "Verification: All probabilities sum to 1.000000\n",
      "\n",
      "Key insight: 'the' had the highest logit\n",
      "and now has the highest probability (0.4488)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def demonstrate_softmax(logits_dict):\n",
    "    print(\"SOFTMAX CONVERSION: From Logits to Probabilities\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    words = list(logits_dict.keys())\n",
    "    logits = list(logits_dict.values())\n",
    "    \n",
    "    print(\"STEP 1: Calculate e^(logit) for each word\")\n",
    "    print(\"-\" * 40)\n",
    "    exp_logits = []\n",
    "    for word, logit in zip(words, logits):\n",
    "        exp_val = math.exp(logit)\n",
    "        exp_logits.append(exp_val)\n",
    "        print(f\"e^({logit:5.1f}) = {exp_val:8.2f}  for '{word}'\")\n",
    "    \n",
    "    print(f\"\\nSTEP 2: Sum all exponential values\")\n",
    "    print(\"-\" * 35)\n",
    "    total = sum(exp_logits)\n",
    "    print(f\"Total sum = {total:.2f}\")\n",
    "    \n",
    "    print(f\"\\nSTEP 3: Calculate final probabilities\")\n",
    "    print(\"-\" * 38)\n",
    "    probabilities = {}\n",
    "    for word, exp_val in zip(words, exp_logits):\n",
    "        prob = exp_val / total\n",
    "        probabilities[word] = prob\n",
    "        marker = \" <- Our target!\" if word == \"we\" else \"\"\n",
    "        print(f\"P({word:8}) = {exp_val:8.2f} / {total:.2f} = {prob:.4f}{marker}\")\n",
    "    \n",
    "    print(f\"\\nVerification: All probabilities sum to {sum(probabilities.values()):.6f}\")\n",
    "    print(f\"\\nKey insight: '{max(logits_dict, key=logits_dict.get)}' had the highest logit\")\n",
    "    print(f\"and now has the highest probability ({max(probabilities.values()):.4f})\")\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "# Convert our logits to probabilities\n",
    "probabilities_step1 = demonstrate_softmax(logits_step1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Prediction: How Context Shapes Predictions\n",
    "\n",
    "Language models don't just predict single words in isolation - they use the entire preceding context to make increasingly informed predictions. As we build up the sequence \"we love deep learning,\" each new word provides more context that helps the model make better predictions for the next word.\n",
    "\n",
    "Let's observe how the model's logit scores change as we provide more context at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE SEQUENCE PREDICTION: 'we love deep learning'\n",
      "============================================================\n",
      "\n",
      "STEP 1: Context = '<BOS>' -> Predicting 'we'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. the       :    3.2\n",
      "  2. hello     :    2.8\n",
      "  3. we        :    2.1 ** TARGET **\n",
      "  4. is        :    1.5\n",
      "  5. love      :   -0.5\n",
      "\n",
      "Target word 'we' probability: 0.149\n",
      "Context effect: 'we' is a common sentence starter, though 'the' is even more common in general text\n",
      "\n",
      "STEP 2: Context = 'we' -> Predicting 'love'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. love      :    2.8 ** TARGET **\n",
      "  2. is        :    2.1\n",
      "  3. the       :    1.2\n",
      "  4. deep      :   -0.5\n",
      "  5. learning  :   -1.5\n",
      "\n",
      "Target word 'love' probability: 0.571\n",
      "Context effect: After 'we', action words like 'love', 'are', 'have' become much more likely than nouns\n",
      "\n",
      "STEP 3: Context = 'we love' -> Predicting 'deep'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. deep      :    1.9 ** TARGET **\n",
      "  2. learning  :    0.8\n",
      "  3. the       :    0.5\n",
      "  4. is        :   -2.0\n",
      "  5. love      :   -4.0\n",
      "\n",
      "Target word 'deep' probability: 0.623\n",
      "Context effect: After 'we love', we expect objects or concepts; 'deep' scores well as it often precedes 'learning'\n",
      "\n",
      "STEP 4: Context = 'we love deep' -> Predicting 'learning'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. learning  :    3.5 ** TARGET **\n",
      "  2. the       :   -2.0\n",
      "  3. is        :   -3.0\n",
      "  4. deep      :   -4.0\n",
      "  5. love      :   -5.0\n",
      "\n",
      "Target word 'learning' probability: 0.994\n",
      "Context effect: 'deep learning' is a common collocation - 'learning' becomes very likely after 'deep' in this context\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_sequence_prediction():\n",
    "    print(\"COMPLETE SEQUENCE PREDICTION: 'we love deep learning'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Pre-calculated logits for each prediction step showing how context improves predictions\n",
    "    prediction_steps = [\n",
    "        {\n",
    "            \"step\": 1,\n",
    "            \"context\": \"<BOS>\",\n",
    "            \"target\": \"we\", \n",
    "            \"logits\": {\"we\": 2.1, \"love\": -0.5, \"deep\": -1.2, \"learning\": -2.0, \"the\": 3.2, \"hello\": 2.8, \"is\": 1.5, \"<EOS>\": -10.0},\n",
    "            \"explanation\": \"'we' is a common sentence starter, though 'the' is even more common in general text\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 2,\n",
    "            \"context\": \"we\",\n",
    "            \"target\": \"love\",\n",
    "            \"logits\": {\"we\": -3.0, \"love\": 2.8, \"deep\": -0.5, \"learning\": -1.5, \"the\": 1.2, \"hello\": -5.0, \"is\": 2.1, \"<EOS>\": -8.0},\n",
    "            \"explanation\": \"After 'we', action words like 'love', 'are', 'have' become much more likely than nouns\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 3,\n",
    "            \"context\": \"we love\",\n",
    "            \"target\": \"deep\",\n",
    "            \"logits\": {\"we\": -5.0, \"love\": -4.0, \"deep\": 1.9, \"learning\": 0.8, \"the\": 0.5, \"hello\": -6.0, \"is\": -2.0, \"<EOS>\": -7.0},\n",
    "            \"explanation\": \"After 'we love', we expect objects or concepts; 'deep' scores well as it often precedes 'learning'\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 4,\n",
    "            \"context\": \"we love deep\",\n",
    "            \"target\": \"learning\",\n",
    "            \"logits\": {\"we\": -6.0, \"love\": -5.0, \"deep\": -4.0, \"learning\": 3.5, \"the\": -2.0, \"hello\": -8.0, \"is\": -3.0, \"<EOS>\": -6.0},\n",
    "            \"explanation\": \"'deep learning' is a common collocation - 'learning' becomes very likely after 'deep' in this context\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    all_step_probabilities = []\n",
    "    \n",
    "    for step_info in prediction_steps:\n",
    "        print(f\"\\nSTEP {step_info['step']}: Context = '{step_info['context']}' -> Predicting '{step_info['target']}'\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Show top 5 logits\n",
    "        sorted_logits = sorted(step_info['logits'].items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Top 5 logit scores:\")\n",
    "        for i, (word, logit) in enumerate(sorted_logits[:5]):\n",
    "            marker = \" ** TARGET **\" if word == step_info['target'] else \"\"\n",
    "            print(f\"  {i+1}. {word:10}: {logit:6.1f}{marker}\")\n",
    "        \n",
    "        # Calculate probability for target word\n",
    "        target_logit = step_info['logits'][step_info['target']]\n",
    "        exp_values = [math.exp(logit) for logit in step_info['logits'].values()]\n",
    "        total_exp = sum(exp_values)\n",
    "        target_prob = math.exp(target_logit) / total_exp\n",
    "        all_step_probabilities.append(target_prob)\n",
    "        \n",
    "        print(f\"\\nTarget word '{step_info['target']}' probability: {target_prob:.3f}\")\n",
    "        print(f\"Context effect: {step_info['explanation']}\")\n",
    "    \n",
    "    return prediction_steps, all_step_probabilities\n",
    "\n",
    "# Generate predictions for the complete sequence\n",
    "steps_data, step_probabilities = demonstrate_sequence_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function: Quantifying Prediction Quality\n",
    "\n",
    "The **loss function** is how we measure how well our model is performing. Specifically, it measures how \"surprised\" the model is when it sees the correct answer. The mathematical formulation we use is called **cross-entropy loss** or **log-likelihood loss**.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Lower loss = better predictions = less surprise when seeing the correct answer\n",
    "- Higher loss = worse predictions = more surprise when seeing the correct answer  \n",
    "- Loss is calculated as: Loss = -log(probability of correct word)\n",
    "- Training aims to minimize the total loss across all predictions\n",
    "\n",
    "The logarithm has a **useful property**: it heavily penalizes very low probabilities. If the model assigns a probability of 0.01 to the correct word, the loss is much higher than if it assigns 0.1.\n",
    "\n",
    "### The Training Dataset as Ground Truth\n",
    "\n",
    "Before we dive into the mathematical details, it's crucial to understand what the loss function is actually measuring and why it works. The loss function compares our model's predictions against what we call the **\"ground truth\"** - the training dataset that contains the correct answers.\n",
    "\n",
    "**Training Dataset as Golden Labels:**\n",
    "\n",
    "When we train a language model, we provide it with a massive dataset of text (books, articles, websites, etc.). This dataset represents our best approximation of \"correct\" language use. Each sequence in this dataset serves as:\n",
    "\n",
    "- **Ground Truth**: The actual words that should come next in real language\n",
    "- **Golden Labels**: The target answers our model should learn to predict\n",
    "- **Reference Distribution**: The pattern of language we want our model to internalize\n",
    "\n",
    "For our example sequence \"we love deep learning\", this represents a small piece of ground truth data. When we calculate loss, we're measuring how well our model's predictions align with this established pattern of language.\n",
    "\n",
    "**From Random Weights to Meaningful Predictions:**\n",
    "\n",
    "Remember our initial model parameters from Section 1? Those weights were randomly initialized:\n",
    "\n",
    "```python\n",
    "model_parameters = {\n",
    "    'layer1_weights': [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, ...],\n",
    "    'layer2_weights': [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, ...],\n",
    "    'output_weights': [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, ...]\n",
    "}\n",
    "```\n",
    "\n",
    "These random numbers produce the predictions we saw in previous sections. The model doesn't \"know\" anything about language initially - its probabilities for predicting \"we\" (0.149) or \"love\" (0.571) come purely from these random mathematical operations.\n",
    "\n",
    "**The Training Objective: Fitting to the Dataset Distribution**\n",
    "\n",
    "The fundamental goal of training is to **fit the model to the training dataset's distribution**. This means:\n",
    "\n",
    "1. **Statistical Matching**: Adjust the weights so the model's predicted probabilities match the frequency patterns in the training data\n",
    "2. **Contextual Learning**: Teach the model that certain word combinations (like \"deep learning\") are more common than others\n",
    "3. **Pattern Internalization**: Embed the statistical regularities of human language into the neural network weights\n",
    "\n",
    "When we minimize the cross-entropy loss across millions of training examples, we're essentially forcing the model to internalize the probability distribution of the training dataset. The model learns that \"the\" is more common after \"<BOS>\" because it appears frequently in training data, not because it has any inherent understanding of language.\n",
    "\n",
    "**Inductive Bias: The Hidden Consequence of Training Data**\n",
    "\n",
    "This training process creates what we call an **inductive bias** - a set of assumptions and tendencies that the model develops based on its training data. This bias manifests in several ways:\n",
    "\n",
    "1. **Domain Bias**: If trained on academic papers, the model will favor formal language\n",
    "2. **Cultural Bias**: Training data reflects the cultural context of its sources\n",
    "3. **Temporal Bias**: Models reflect the time period when their training data was collected\n",
    "4. **Statistical Bias**: Common patterns in training data become \"preferred\" by the model\n",
    "\n",
    "Our model's tendency to predict \"the\" with high probability after \"<BOS>\" isn't based on understanding - it's an inductive bias learned from statistical patterns in training data where \"the\" frequently starts sentences.\n",
    "\n",
    "**The Circular Nature of Language Modeling:**\n",
    "\n",
    "There's a profound circularity here: we use human-written text to train models to predict human-like text. The model doesn't learn \"language\" in an abstract sense - it learns to mimic the specific patterns present in its training distribution. This is why the choice of training data is so critical and why different models can have vastly different behaviors based on their training datasets.\n",
    "\n",
    "The loss function, therefore, isn't just measuring prediction accuracy - it's measuring how well the model has absorbed and can reproduce the inductive biases present in its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mathematical Foundation: Next Token Prediction Loss\n",
    "\n",
    "Now let's dive deep into the precise mathematical formulation used in Large Language Models. The loss function we use is called **Negative Log-Likelihood (NLL)** for **Next Token Prediction (NTP)**.\n",
    "\n",
    "#### The Complete NLL Formula\n",
    "\n",
    "For **Next Token Prediction** in Large Language Models, the negative log-likelihood formula is:\n",
    "\n",
    "**NLL = -∑ᵢ₌₁ⁿ log P(xᵢ | x₁, x₂, ..., xᵢ₋₁; θ)**\n",
    "\n",
    "**Where:**\n",
    "- **xᵢ** = the i-th token in the sequence (e.g., \"we\", \"love\", \"deep\", \"learning\")\n",
    "- **x₁, x₂, ..., xᵢ₋₁** = all previous tokens providing context\n",
    "- **θ** = model parameters (our weights from Section 1)\n",
    "- **n** = sequence length (4 tokens in our example)\n",
    "\n",
    "#### Breaking Down Each Component\n",
    "\n",
    "**1. Per-Token Probability Computation:**\n",
    "\n",
    "The probability of each token given its context is computed using softmax:\n",
    "\n",
    "```\n",
    "P(xᵢ | context) = softmax(logits)ₓᵢ = exp(zₓᵢ) / ∑ⱼ₌₁ᵛ exp(zⱼ)\n",
    "```\n",
    "\n",
    "**Where:**\n",
    "- **zₓᵢ** = logit score for token xᵢ (like our 2.1 for \"we\")\n",
    "- **V** = vocabulary size (12 in our case)\n",
    "- **j** indexes over entire vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregressive Nature\n",
    "\n",
    "The key insight is that each prediction depends on **all previous tokens**:\n",
    "\n",
    "- **Step 1**: P(\"we\" | \"<BOS>\") - no previous content words\n",
    "- **Step 2**: P(\"love\" | \"we\") - uses \"we\" as context  \n",
    "- **Step 3**: P(\"deep\" | \"we\", \"love\") - uses both previous tokens\n",
    "- **Step 4**: P(\"learning\" | \"we\", \"love\", \"deep\") - uses all three previous tokens\n",
    "\n",
    "This is why our predictions improved with more context in Section 4 - the model had more information to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to Perplexity\n",
    "\n",
    "The loss is often reported as **perplexity**: \n",
    "\n",
    "```\n",
    "Perplexity = exp(NLL/n)\n",
    "```\n",
    "\n",
    "Lower perplexity indicates better language modeling performance.\n",
    "\n",
    "This mathematical foundation captures the core training objective of all Large Language Models: **maximize the probability of the correct next token given the context**. Every word prediction in ChatGPT, GPT-4, or any other LLM follows this exact mathematical framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-ENTROPY LOSS FORMULA: Step-by-Step Breakdown\n",
      "============================================================\n",
      "The cross-entropy loss formula is: Loss = -log(P(correct_word))\n",
      "Let's break this down with a concrete example:\n",
      "\n",
      "TOY EXAMPLE: Predicting 'cat' from vocabulary ['dog', 'cat', 'bird']\n",
      "-----------------------------------------------------------------\n",
      "Model predictions (probabilities):\n",
      "  P(dog) = 0.2\n",
      "  P(cat) = 0.6 <- CORRECT ANSWER\n",
      "  P(bird) = 0.2\n",
      "\n",
      "Verification: 1.0 (probabilities sum to 1.0)\n",
      "\n",
      "STEP-BY-STEP LOSS CALCULATION:\n",
      "-----------------------------------\n",
      "Step 1: Identify the correct word\n",
      "        Correct word = 'cat'\n",
      "\n",
      "Step 2: Find its probability\n",
      "        P(cat) = 0.6\n",
      "\n",
      "Step 3: Take the natural logarithm\n",
      "        log(P(cat)) = log(0.6) = -0.5108\n",
      "\n",
      "Step 4: Apply the negative sign\n",
      "        Loss = -log(P(cat)) = -(-0.5108) = 0.5108\n",
      "\n",
      "FINAL RESULT: Loss = 0.5108\n",
      "\n",
      "==================================================\n",
      "WHY NEGATIVE LOGARITHM?\n",
      "==================================================\n",
      "Probability -> log(prob) -> -log(prob) (Loss)\n",
      "---------------------------------------------\n",
      "   0.90    ->    -0.11   ->     0.11  (Excellent)\n",
      "   0.50    ->    -0.69   ->     0.69  (Good)\n",
      "   0.10    ->    -2.30   ->     2.30  (Poor)\n",
      "   0.01    ->    -4.61   ->     4.61  (Terrible)\n",
      "\n",
      "Key insight: Higher probability -> Lower loss (what we want!)\n",
      "            Lower probability -> Higher loss (penalty for bad predictions)\n",
      "\n",
      "======================================================================\n",
      "APPLYING LOSS TO OUR SEQUENCE: 'we love deep learning'\n",
      "======================================================================\n",
      "Now let's calculate loss for each word in our target sequence:\n",
      "Formula: Loss = -log(P(correct_word))\n",
      "\n",
      "STEP 1: Predicting 'we'\n",
      "-------------------------\n",
      "  Model prediction: P(we) = 0.149\n",
      "  Take logarithm:   log(0.149) = -1.9013\n",
      "  Apply negative:   Loss = -(-1.9013) = 1.9013\n",
      "  Interpretation:   Decent prediction\n",
      "\n",
      "STEP 2: Predicting 'love'\n",
      "-------------------------\n",
      "  Model prediction: P(love) = 0.571\n",
      "  Take logarithm:   log(0.571) = -0.5610\n",
      "  Apply negative:   Loss = -(-0.5610) = 0.5610\n",
      "  Interpretation:   Good prediction\n",
      "\n",
      "STEP 3: Predicting 'deep'\n",
      "-------------------------\n",
      "  Model prediction: P(deep) = 0.623\n",
      "  Take logarithm:   log(0.623) = -0.4725\n",
      "  Apply negative:   Loss = -(-0.4725) = 0.4725\n",
      "  Interpretation:   Excellent prediction!\n",
      "\n",
      "STEP 4: Predicting 'learning'\n",
      "-------------------------\n",
      "  Model prediction: P(learning) = 0.994\n",
      "  Take logarithm:   log(0.994) = -0.0065\n",
      "  Apply negative:   Loss = -(-0.0065) = 0.0065\n",
      "  Interpretation:   Excellent prediction!\n",
      "\n",
      "TOTAL SEQUENCE LOSS:\n",
      "--------------------\n",
      "Sum of all losses = 2.9412\n",
      "Average per word  = 0.7353\n",
      "\n",
      "Training goal: Reduce this total loss by improving individual word predictions!\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_cross_entropy_formula():\n",
    "    print(\"CROSS-ENTROPY LOSS FORMULA: Step-by-Step Breakdown\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"The cross-entropy loss formula is: Loss = -log(P(correct_word))\")\n",
    "    print(\"Let's break this down with a concrete example:\\n\")\n",
    "    \n",
    "    # Use a simple toy example\n",
    "    print(\"TOY EXAMPLE: Predicting 'cat' from vocabulary ['dog', 'cat', 'bird']\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # Toy probabilities for demonstration\n",
    "    toy_probabilities = {\n",
    "        \"dog\": 0.2,\n",
    "        \"cat\": 0.6,   # correct word\n",
    "        \"bird\": 0.2\n",
    "    }\n",
    "    \n",
    "    print(\"Model predictions (probabilities):\")\n",
    "    for word, prob in toy_probabilities.items():\n",
    "        marker = \" <- CORRECT ANSWER\" if word == \"cat\" else \"\"\n",
    "        print(f\"  P({word}) = {prob:.1f}{marker}\")\n",
    "    \n",
    "    print(f\"\\nVerification: {sum(toy_probabilities.values()):.1f} (probabilities sum to 1.0)\")\n",
    "    \n",
    "    print(f\"\\nSTEP-BY-STEP LOSS CALCULATION:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    correct_word = \"cat\"\n",
    "    correct_prob = toy_probabilities[correct_word]\n",
    "    \n",
    "    print(f\"Step 1: Identify the correct word\")\n",
    "    print(f\"        Correct word = '{correct_word}'\")\n",
    "    \n",
    "    print(f\"\\nStep 2: Find its probability\")\n",
    "    print(f\"        P({correct_word}) = {correct_prob}\")\n",
    "    \n",
    "    print(f\"\\nStep 3: Take the natural logarithm\")\n",
    "    log_prob = math.log(correct_prob)\n",
    "    print(f\"        log(P({correct_word})) = log({correct_prob}) = {log_prob:.4f}\")\n",
    "    \n",
    "    print(f\"\\nStep 4: Apply the negative sign\")\n",
    "    loss = -log_prob\n",
    "    print(f\"        Loss = -log(P({correct_word})) = -({log_prob:.4f}) = {loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFINAL RESULT: Loss = {loss:.4f}\")\n",
    "    \n",
    "    # Show why we use negative log\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"WHY NEGATIVE LOGARITHM?\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    prob_examples = [0.9, 0.5, 0.1, 0.01]\n",
    "    print(\"Probability -> log(prob) -> -log(prob) (Loss)\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for prob in prob_examples:\n",
    "        log_val = math.log(prob)\n",
    "        loss_val = -log_val\n",
    "        quality = \"Excellent\" if prob > 0.8 else \"Good\" if prob > 0.4 else \"Poor\" if prob > 0.05 else \"Terrible\"\n",
    "        print(f\"   {prob:4.2f}    ->   {log_val:6.2f}   ->   {loss_val:6.2f}  ({quality})\")\n",
    "    \n",
    "    print(f\"\\nKey insight: Higher probability -> Lower loss (what we want!)\")\n",
    "    print(f\"            Lower probability -> Higher loss (penalty for bad predictions)\")\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def demonstrate_loss_calculation():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"APPLYING LOSS TO OUR SEQUENCE: 'we love deep learning'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"Now let's calculate loss for each word in our target sequence:\")\n",
    "    print(\"Formula: Loss = -log(P(correct_word))\")\n",
    "    print()\n",
    "    \n",
    "    words = [\"we\", \"love\", \"deep\", \"learning\"]\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (word, prob) in enumerate(zip(words, step_probabilities)):\n",
    "        print(f\"STEP {i+1}: Predicting '{word}'\")\n",
    "        print(\"-\" * 25)\n",
    "        print(f\"  Model prediction: P({word}) = {prob:.3f}\")\n",
    "        print(f\"  Take logarithm:   log({prob:.3f}) = {math.log(prob):.4f}\")\n",
    "        \n",
    "        loss = -math.log(prob)\n",
    "        total_loss += loss\n",
    "        \n",
    "        print(f\"  Apply negative:   Loss = -({math.log(prob):.4f}) = {loss:.4f}\")\n",
    "        \n",
    "        # Interpret the loss\n",
    "        if loss < 0.5:\n",
    "            interpretation = \"Excellent prediction!\"\n",
    "        elif loss < 1.0:\n",
    "            interpretation = \"Good prediction\"\n",
    "        elif loss < 2.0:\n",
    "            interpretation = \"Decent prediction\"\n",
    "        else:\n",
    "            interpretation = \"Poor prediction - needs improvement\"\n",
    "            \n",
    "        print(f\"  Interpretation:   {interpretation}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"TOTAL SEQUENCE LOSS:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Sum of all losses = {total_loss:.4f}\")\n",
    "    print(f\"Average per word  = {total_loss/len(words):.4f}\")\n",
    "    print(f\"\\nTraining goal: Reduce this total loss by improving individual word predictions!\")\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# First demonstrate the formula with toy example\n",
    "toy_loss = demonstrate_cross_entropy_formula()\n",
    "\n",
    "# Then apply to our actual sequence\n",
    "sequence_loss = demonstrate_loss_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation: Finding What Needs to Change\n",
    "\n",
    "Now that we understand how to measure our model's performance with the loss function, we need to figure out **which model parameters should be adjusted and by how much** to improve our predictions. This process is called **backpropagation** - it literally means \"propagating the error backwards\" through the neural network.\n",
    "\n",
    "**The Central Question:** Our total loss is 2.94. We want to reduce this to improve our predictions. But our model has thousands or millions of parameters. Which ones should we change? And in which direction (increase or decrease)?\n",
    "\n",
    "**The Answer:** Backpropagation calculates the **gradient** - the mathematical derivative of the loss with respect to each parameter. The gradient tells us:\n",
    "\n",
    "1. **Direction**: Should we increase or decrease each parameter?\n",
    "2. **Magnitude**: How much impact will changing each parameter have on the loss?\n",
    "\n",
    "### The Mathematical Foundation: The Chain Rule\n",
    "\n",
    "Backpropagation is essentially a systematic application of the **chain rule** from calculus. The chain rule tells us how to compute the derivative of composite functions.\n",
    "\n",
    "**For our model:** Loss = f(probabilities) = f(softmax(logits)) = f(softmax(neural_network(parameters)))\n",
    "\n",
    "To find how the loss changes with respect to any parameter, we need to trace the path backwards:\n",
    "\n",
    "```\n",
    "∂Loss/∂parameter = ∂Loss/∂probabilities × ∂probabilities/∂logits × ∂logits/∂parameter\n",
    "```\n",
    "\n",
    "This \"chaining\" of derivatives is what gives backpropagation its name - we start from the loss and work backwards through each layer.\n",
    "\n",
    "### Understanding Gradients: Direction and Magnitude\n",
    "\n",
    "Let's explore what gradients actually represent with concrete examples from our model.\n",
    "\n",
    "**Key Gradient Properties:**\n",
    "\n",
    "- **Positive gradient**: Increasing this parameter increases the loss (bad direction)\n",
    "- **Negative gradient**: Increasing this parameter decreases the loss (good direction)  \n",
    "- **Large gradient magnitude**: This parameter has high impact on loss\n",
    "- **Small gradient magnitude**: This parameter has low impact on loss\n",
    "\n",
    "The gradient essentially answers: \"If I increase this parameter by a tiny amount, how much will the loss change?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT CONCEPT: What Does a Gradient Tell Us?\n",
      "=======================================================\n",
      "Imagine we have a simple parameter that affects our model's prediction for 'we'\n",
      "Let's call this parameter 'weight_for_we' and see how changing it affects loss\n",
      "\n",
      "CURRENT SITUATION:\n",
      "  weight_for_we = 0.5\n",
      "  P(we) = 0.149\n",
      "  Loss for 'we' = -log(0.149) = 1.9038\n",
      "\n",
      "TESTING SMALL CHANGES (this is what gradients approximate):\n",
      "----------------------------------------------------------\n",
      "\n",
      "DECREASE weight by 0.010:\n",
      "  New weight: 0.5 -> 0.49\n",
      "  New P(we): 0.149 -> 0.146\n",
      "  New loss: 1.9038 -> 1.9240\n",
      "  Loss change: +0.0202\n",
      "  Result: LOSS INCREASED - this is a BAD direction!\n",
      "\n",
      "INCREASE weight by 0.010:\n",
      "  New weight: 0.5 -> 0.51\n",
      "  New P(we): 0.149 -> 0.152\n",
      "  New loss: 1.9038 -> 1.8840\n",
      "  Loss change: -0.0198\n",
      "  Result: LOSS DECREASED - this is a GOOD direction!\n",
      "\n",
      "==================================================\n",
      "GRADIENT CALCULATION:\n",
      "==================================================\n",
      "Approximate gradient = (loss_change) / (weight_change)\n",
      "                    ≈ -2.00\n",
      "\n",
      "INTERPRETATION: Negative gradient means:\n",
      "  - Increasing this weight DECREASES loss (good)\n",
      "  - Decreasing this weight INCREASES loss (bad)\n",
      "  - We should INCREASE this weight during training\n",
      "\n",
      "======================================================================\n",
      "BACKPROPAGATION PROCESS: Step-by-Step Through Our Network\n",
      "======================================================================\n",
      "Let's trace how gradients flow backwards through our model\n",
      "for the first prediction: '<BOS>' -> 'we'\n",
      "\n",
      "STEP 1: Start with the loss\n",
      "------------------------------\n",
      "Loss = -log(P(we)) = -log(0.149) = 1.9038\n",
      "This is our starting point - we want to reduce this loss\n",
      "\n",
      "STEP 2: Gradient with respect to logits\n",
      "------------------------------------------\n",
      "For softmax + cross-entropy, the gradient has a beautiful form:\n",
      "\n",
      "∂Loss/∂logit_i = P(word_i) - target_i\n",
      "\n",
      "Where target_i = 1 if word_i is correct, 0 otherwise\n",
      "\n",
      "For our prediction '<BOS>' -> 'we':\n",
      "  ∂Loss/∂logit_we       = 0.149 - 1 = -0.851 <- TARGET WORD\n",
      "  ∂Loss/∂logit_love     = 0.011 - 0 = +0.011\n",
      "  ∂Loss/∂logit_the      = 0.449 - 0 = +0.449\n",
      "  ∂Loss/∂logit_hello    = 0.301 - 0 = +0.301\n",
      "  ∂Loss/∂logit_is       = 0.082 - 0 = +0.082\n",
      "\n",
      "Key insights:\n",
      "- Target word 'we': gradient = 0.149 - 1 = -0.851 (NEGATIVE)\n",
      "  This means increasing logit_we will DECREASE loss (good!)\n",
      "- Other words: positive gradients\n",
      "  This means increasing their logits will INCREASE loss (bad!)\n",
      "\n",
      "STEP 3: Gradients flow to network weights\n",
      "----------------------------------------\n",
      "The gradients we calculated above now flow backwards to:\n",
      "- Output layer weights (that produce logits)\n",
      "- Hidden layer weights\n",
      "- Input embedding weights\n",
      "\n",
      "Each weight's gradient tells us:\n",
      "- How much that weight contributed to the current loss\n",
      "- Which direction to adjust it to improve predictions\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_gradient_concept():\n",
    "    print(\"GRADIENT CONCEPT: What Does a Gradient Tell Us?\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    print(\"Imagine we have a simple parameter that affects our model's prediction for 'we'\")\n",
    "    print(\"Let's call this parameter 'weight_for_we' and see how changing it affects loss\")\n",
    "    print()\n",
    "    \n",
    "    # Current situation\n",
    "    current_weight = 0.5\n",
    "    current_prob_we = 0.149  # From our earlier calculation\n",
    "    current_loss_we = -math.log(current_prob_we)\n",
    "    \n",
    "    print(f\"CURRENT SITUATION:\")\n",
    "    print(f\"  weight_for_we = {current_weight}\")\n",
    "    print(f\"  P(we) = {current_prob_we:.3f}\")\n",
    "    print(f\"  Loss for 'we' = -log({current_prob_we:.3f}) = {current_loss_we:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Test small changes\n",
    "    print(\"TESTING SMALL CHANGES (this is what gradients approximate):\")\n",
    "    print(\"-\" * 58)\n",
    "    \n",
    "    small_change = 0.01\n",
    "    weight_changes = [-small_change, small_change]\n",
    "    \n",
    "    for i, change in enumerate(weight_changes):\n",
    "        new_weight = current_weight + change\n",
    "        \n",
    "        # Simulate how the probability might change (simplified)\n",
    "        # In reality, this involves complex neural network forward pass\n",
    "        prob_change_factor = 1 + change * 2  # Simplified relationship\n",
    "        new_prob_we = min(current_prob_we * prob_change_factor, 0.999)\n",
    "        new_loss_we = -math.log(new_prob_we)\n",
    "        \n",
    "        loss_change = new_loss_we - current_loss_we\n",
    "        \n",
    "        direction = \"DECREASE\" if change < 0 else \"INCREASE\"\n",
    "        print(f\"\\n{direction} weight by {abs(change):.3f}:\")\n",
    "        print(f\"  New weight: {current_weight} -> {new_weight}\")\n",
    "        print(f\"  New P(we): {current_prob_we:.3f} -> {new_prob_we:.3f}\")\n",
    "        print(f\"  New loss: {current_loss_we:.4f} -> {new_loss_we:.4f}\")\n",
    "        print(f\"  Loss change: {loss_change:+.4f}\")\n",
    "        \n",
    "        if loss_change < 0:\n",
    "            print(f\"  Result: LOSS DECREASED - this is a GOOD direction!\")\n",
    "        else:\n",
    "            print(f\"  Result: LOSS INCREASED - this is a BAD direction!\")\n",
    "    \n",
    "    # Calculate approximate gradient\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"GRADIENT CALCULATION:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Approximate gradient using finite differences\n",
    "    loss_at_plus = -math.log(min(current_prob_we * (1 + small_change * 2), 0.999))\n",
    "    loss_at_minus = -math.log(current_prob_we * (1 - small_change * 2))\n",
    "    approximate_gradient = (loss_at_plus - loss_at_minus) / (2 * small_change)\n",
    "    \n",
    "    print(f\"Approximate gradient = (loss_change) / (weight_change)\")\n",
    "    print(f\"                    ≈ {approximate_gradient:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    if approximate_gradient > 0:\n",
    "        print(\"INTERPRETATION: Positive gradient means:\")\n",
    "        print(\"  - Increasing this weight INCREASES loss (bad)\")\n",
    "        print(\"  - Decreasing this weight DECREASES loss (good)\")\n",
    "        print(\"  - We should DECREASE this weight during training\")\n",
    "    else:\n",
    "        print(\"INTERPRETATION: Negative gradient means:\")\n",
    "        print(\"  - Increasing this weight DECREASES loss (good)\")\n",
    "        print(\"  - Decreasing this weight INCREASES loss (bad)\")  \n",
    "        print(\"  - We should INCREASE this weight during training\")\n",
    "    \n",
    "    return approximate_gradient\n",
    "\n",
    "def demonstrate_backprop_process():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BACKPROPAGATION PROCESS: Step-by-Step Through Our Network\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"Let's trace how gradients flow backwards through our model\")\n",
    "    print(\"for the first prediction: '<BOS>' -> 'we'\")\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Loss gradient\n",
    "    print(\"STEP 1: Start with the loss\")\n",
    "    print(\"-\" * 30)\n",
    "    target_prob = 0.149  # P(we) from our calculation\n",
    "    loss = -math.log(target_prob)\n",
    "    print(f\"Loss = -log(P(we)) = -log({target_prob:.3f}) = {loss:.4f}\")\n",
    "    print(f\"This is our starting point - we want to reduce this loss\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Gradient with respect to logits\n",
    "    print(\"STEP 2: Gradient with respect to logits\")\n",
    "    print(\"-\" * 42)\n",
    "    print(\"For softmax + cross-entropy, the gradient has a beautiful form:\")\n",
    "    print()\n",
    "    print(\"∂Loss/∂logit_i = P(word_i) - target_i\")\n",
    "    print()\n",
    "    print(\"Where target_i = 1 if word_i is correct, 0 otherwise\")\n",
    "    print()\n",
    "    \n",
    "    # Show gradients for each word in vocabulary\n",
    "    vocab_subset = [\"we\", \"love\", \"the\", \"hello\", \"is\"]\n",
    "    probs_step1 = {\"we\": 0.149, \"love\": 0.011, \"the\": 0.449, \"hello\": 0.301, \"is\": 0.082}\n",
    "    \n",
    "    print(\"For our prediction '<BOS>' -> 'we':\")\n",
    "    for word in vocab_subset:\n",
    "        prob = probs_step1[word]\n",
    "        target = 1 if word == \"we\" else 0\n",
    "        gradient = prob - target\n",
    "        \n",
    "        marker = \" <- TARGET WORD\" if word == \"we\" else \"\"\n",
    "        print(f\"  ∂Loss/∂logit_{word:8} = {prob:.3f} - {target} = {gradient:+.3f}{marker}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Key insights:\")\n",
    "    print(\"- Target word 'we': gradient = 0.149 - 1 = -0.851 (NEGATIVE)\")\n",
    "    print(\"  This means increasing logit_we will DECREASE loss (good!)\")\n",
    "    print(\"- Other words: positive gradients\")\n",
    "    print(\"  This means increasing their logits will INCREASE loss (bad!)\")\n",
    "    \n",
    "    # Step 3: Gradient flows to earlier layers\n",
    "    print(f\"\\nSTEP 3: Gradients flow to network weights\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"The gradients we calculated above now flow backwards to:\")\n",
    "    print(\"- Output layer weights (that produce logits)\")\n",
    "    print(\"- Hidden layer weights\")\n",
    "    print(\"- Input embedding weights\")\n",
    "    print()\n",
    "    print(\"Each weight's gradient tells us:\")\n",
    "    print(\"- How much that weight contributed to the current loss\")\n",
    "    print(\"- Which direction to adjust it to improve predictions\")\n",
    "    \n",
    "    return {\"target_logit_gradient\": -0.851}\n",
    "\n",
    "# Run the demonstrations\n",
    "gradient_example = demonstrate_gradient_concept()\n",
    "backprop_results = demonstrate_backprop_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Full Backpropagation Picture\n",
    "\n",
    "Let's put together the complete picture of how backpropagation works in our language model by examining what happens when we process our entire sequence \"we love deep learning\".\n",
    "\n",
    "**The Process:**\n",
    "\n",
    "1. **Forward Pass**: Compute predictions and loss for each word\n",
    "2. **Backward Pass**: Calculate gradients for each parameter\n",
    "3. **Accumulate Gradients**: Sum gradients from all prediction steps\n",
    "4. **Parameter Updates**: Use gradients to improve model weights\n",
    "\n",
    "This happens for every training example, and the gradients tell us exactly how to adjust our model to make better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE BACKPROPAGATION: All Four Prediction Steps\n",
      "==========================================================\n",
      "For each prediction step, backpropagation calculates:\n",
      "- How much each parameter contributed to the loss\n",
      "- Which direction to adjust each parameter\n",
      "\n",
      "STEP 1: '<BOS>' -> 'we'\n",
      "--------------------------------------------------\n",
      "  Forward pass:  P(we) = 0.149\n",
      "  Loss:          1.9013\n",
      "  Logit gradient: -0.851\n",
      "  Interpretation: High priority - adjust weights to boost 'we'\n",
      "\n",
      "STEP 2: 'we' -> 'love'\n",
      "--------------------------------------------------\n",
      "  Forward pass:  P(love) = 0.571\n",
      "  Loss:          0.5610\n",
      "  Logit gradient: -0.429\n",
      "  Interpretation: Medium priority - adjust weights to boost 'love'\n",
      "\n",
      "STEP 3: 'we love' -> 'deep'\n",
      "--------------------------------------------------\n",
      "  Forward pass:  P(deep) = 0.623\n",
      "  Loss:          0.4725\n",
      "  Logit gradient: -0.377\n",
      "  Interpretation: Medium priority - adjust weights to boost 'deep'\n",
      "\n",
      "STEP 4: 'we love deep' -> 'learning'\n",
      "--------------------------------------------------\n",
      "  Forward pass:  P(learning) = 0.994\n",
      "  Loss:          0.0065\n",
      "  Logit gradient: -0.006\n",
      "  Interpretation: Low priority - adjust weights to boost 'learning'\n",
      "\n",
      "GRADIENT ACCUMULATION:\n",
      "-------------------------\n",
      "In a real training step, we would:\n",
      "1. Sum up all the gradients from each prediction step\n",
      "2. Apply these combined gradients to update model parameters\n",
      "3. The model gets slightly better at each prediction\n",
      "\n",
      "Total loss across all steps: 2.9413\n",
      "This is what we're trying to minimize through training!\n",
      "\n",
      "============================================================\n",
      "IMPACT ON MODEL PARAMETERS:\n",
      "============================================================\n",
      "Our original parameters (from Section 1):\n",
      "  layer1_weights: [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, ...]\n",
      "  layer2_weights: [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, ...]\n",
      "  output_weights: [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, ...]\n",
      "\n",
      "After backpropagation, each weight gets a gradient:\n",
      "  gradient_layer1: [-0.05, 0.12, -0.08, 0.03, 0.07, -0.15, ...]\n",
      "  gradient_layer2: [0.09, -0.04, 0.11, -0.18, 0.06, 0.08, ...]\n",
      "  gradient_output: [-0.21, 0.15, -0.12, 0.09, -0.17, 0.11, ...]\n",
      "\n",
      "These gradients tell us:\n",
      "- Negative gradient: Increase this weight to reduce loss\n",
      "- Positive gradient: Decrease this weight to reduce loss\n",
      "- Large magnitude: This weight has high impact\n",
      "- Small magnitude: This weight has low impact\n",
      "\n",
      "=================================================================\n",
      "GRADIENT FLOW: How Information Travels Backwards\n",
      "=================================================================\n",
      "Think of gradients as 'error signals' that flow backwards:\n",
      "\n",
      "FORWARD PASS (left to right):\n",
      "-----------------------------------\n",
      "→ INPUT: Token embeddings ('<BOS>', 'we', 'love', etc.)\n",
      "  → LAYER 1: First transformation (hidden layer)\n",
      "    → LAYER 2: Second transformation (hidden layer)\n",
      "      → OUTPUT: Logits for each vocabulary word\n",
      "        → SOFTMAX: Convert logits to probabilities\n",
      "          → LOSS: Compare with target word\n",
      "\n",
      "BACKWARD PASS (right to left - gradients flow backwards):\n",
      "------------------------------------------------------------\n",
      "← LAYER 1 GRADIENT: ∂Loss/∂layer1_weights\n",
      "  ← LAYER 2 GRADIENT: ∂Loss/∂layer2_weights\n",
      "    ← OUTPUT GRADIENT: ∂Loss/∂output_weights\n",
      "      ← LOGIT GRADIENT: ∂Loss/∂logits (we calculated these!)\n",
      "        ← SOFTMAX GRADIENT: ∂Loss/∂probabilities\n",
      "          ← LOSS GRADIENT: ∂Loss/∂Loss = 1 (starting point)\n",
      "\n",
      "Key insight: Each layer's gradient depends on the gradients\n",
      "from all layers that come after it. This is the 'chain rule' in action!\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_full_backpropagation():\n",
    "    print(\"COMPLETE BACKPROPAGATION: All Four Prediction Steps\")\n",
    "    print(\"=\" * 58)\n",
    "    \n",
    "    # Data from our previous calculations\n",
    "    sequence_data = [\n",
    "        {\n",
    "            \"step\": 1,\n",
    "            \"context\": \"<BOS>\",\n",
    "            \"target\": \"we\",\n",
    "            \"probability\": 0.149,\n",
    "            \"loss\": 1.9013,\n",
    "            \"logit_gradient\": -0.851  # P(we) - 1 = 0.149 - 1\n",
    "        },\n",
    "        {\n",
    "            \"step\": 2,\n",
    "            \"context\": \"we\",\n",
    "            \"target\": \"love\", \n",
    "            \"probability\": 0.571,\n",
    "            \"loss\": 0.5610,\n",
    "            \"logit_gradient\": -0.429  # P(love) - 1 = 0.571 - 1\n",
    "        },\n",
    "        {\n",
    "            \"step\": 3,\n",
    "            \"context\": \"we love\",\n",
    "            \"target\": \"deep\",\n",
    "            \"probability\": 0.623,\n",
    "            \"loss\": 0.4725,\n",
    "            \"logit_gradient\": -0.377  # P(deep) - 1 = 0.623 - 1\n",
    "        },\n",
    "        {\n",
    "            \"step\": 4,\n",
    "            \"context\": \"we love deep\",\n",
    "            \"target\": \"learning\",\n",
    "            \"probability\": 0.994,\n",
    "            \"loss\": 0.0065,\n",
    "            \"logit_gradient\": -0.006  # P(learning) - 1 = 0.994 - 1\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"For each prediction step, backpropagation calculates:\")\n",
    "    print(\"- How much each parameter contributed to the loss\")\n",
    "    print(\"- Which direction to adjust each parameter\")\n",
    "    print()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in sequence_data:\n",
    "        print(f\"STEP {data['step']}: '{data['context']}' -> '{data['target']}'\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"  Forward pass:  P({data['target']}) = {data['probability']:.3f}\")\n",
    "        print(f\"  Loss:          {data['loss']:.4f}\")\n",
    "        print(f\"  Logit gradient: {data['logit_gradient']:.3f}\")\n",
    "        \n",
    "        # Interpret the gradient\n",
    "        if abs(data['logit_gradient']) > 0.5:\n",
    "            urgency = \"High priority\"\n",
    "        elif abs(data['logit_gradient']) > 0.1:\n",
    "            urgency = \"Medium priority\"\n",
    "        else:\n",
    "            urgency = \"Low priority\"\n",
    "            \n",
    "        print(f\"  Interpretation: {urgency} - adjust weights to boost '{data['target']}'\")\n",
    "        print()\n",
    "        \n",
    "        total_loss += data['loss']\n",
    "    \n",
    "    print(\"GRADIENT ACCUMULATION:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"In a real training step, we would:\")\n",
    "    print(\"1. Sum up all the gradients from each prediction step\")\n",
    "    print(\"2. Apply these combined gradients to update model parameters\")\n",
    "    print(\"3. The model gets slightly better at each prediction\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Total loss across all steps: {total_loss:.4f}\")\n",
    "    print(f\"This is what we're trying to minimize through training!\")\n",
    "    \n",
    "    # Show how gradients would affect our original parameters\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"IMPACT ON MODEL PARAMETERS:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"Our original parameters (from Section 1):\")\n",
    "    print(\"  layer1_weights: [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, ...]\")\n",
    "    print(\"  layer2_weights: [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, ...]\")\n",
    "    print(\"  output_weights: [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, ...]\")\n",
    "    print()\n",
    "    \n",
    "    print(\"After backpropagation, each weight gets a gradient:\")\n",
    "    print(\"  gradient_layer1: [-0.05, 0.12, -0.08, 0.03, 0.07, -0.15, ...]\")\n",
    "    print(\"  gradient_layer2: [0.09, -0.04, 0.11, -0.18, 0.06, 0.08, ...]\")  \n",
    "    print(\"  gradient_output: [-0.21, 0.15, -0.12, 0.09, -0.17, 0.11, ...]\")\n",
    "    print()\n",
    "    \n",
    "    print(\"These gradients tell us:\")\n",
    "    print(\"- Negative gradient: Increase this weight to reduce loss\")\n",
    "    print(\"- Positive gradient: Decrease this weight to reduce loss\")\n",
    "    print(\"- Large magnitude: This weight has high impact\")\n",
    "    print(\"- Small magnitude: This weight has low impact\")\n",
    "    \n",
    "    return sequence_data\n",
    "\n",
    "def demonstrate_gradient_flow():\n",
    "    print(f\"\\n\" + \"=\"*65)\n",
    "    print(\"GRADIENT FLOW: How Information Travels Backwards\")\n",
    "    print(\"=\"*65)\n",
    "    \n",
    "    print(\"Think of gradients as 'error signals' that flow backwards:\")\n",
    "    print()\n",
    "    \n",
    "    network_layers = [\n",
    "        \"INPUT: Token embeddings ('<BOS>', 'we', 'love', etc.)\",\n",
    "        \"LAYER 1: First transformation (hidden layer)\", \n",
    "        \"LAYER 2: Second transformation (hidden layer)\",\n",
    "        \"OUTPUT: Logits for each vocabulary word\",\n",
    "        \"SOFTMAX: Convert logits to probabilities\",\n",
    "        \"LOSS: Compare with target word\"\n",
    "    ]\n",
    "    \n",
    "    print(\"FORWARD PASS (left to right):\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, layer in enumerate(network_layers):\n",
    "        indent = \"  \" * i\n",
    "        print(f\"{indent}→ {layer}\")\n",
    "    \n",
    "    print(f\"\\nBACKWARD PASS (right to left - gradients flow backwards):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    gradient_flow = [\n",
    "        \"LOSS GRADIENT: ∂Loss/∂Loss = 1 (starting point)\",\n",
    "        \"SOFTMAX GRADIENT: ∂Loss/∂probabilities\",\n",
    "        \"LOGIT GRADIENT: ∂Loss/∂logits (we calculated these!)\",\n",
    "        \"OUTPUT GRADIENT: ∂Loss/∂output_weights\", \n",
    "        \"LAYER 2 GRADIENT: ∂Loss/∂layer2_weights\",\n",
    "        \"LAYER 1 GRADIENT: ∂Loss/∂layer1_weights\"\n",
    "    ]\n",
    "    \n",
    "    for i, gradient in enumerate(reversed(gradient_flow)):\n",
    "        indent = \"  \" * i\n",
    "        print(f\"{indent}← {gradient}\")\n",
    "    \n",
    "    print(f\"\\nKey insight: Each layer's gradient depends on the gradients\")\n",
    "    print(f\"from all layers that come after it. This is the 'chain rule' in action!\")\n",
    "    \n",
    "    return gradient_flow\n",
    "\n",
    "# Run the demonstrations\n",
    "sequence_gradients = demonstrate_full_backpropagation()\n",
    "gradient_flow_info = demonstrate_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We intentionally kept the mathematical details of backpropagation simple to make it easier to understand. It is highly recommended to deeply understand the mathematical details of backpropagation, through something like Kaparthy's Neural Network Zero-to-Hero course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
