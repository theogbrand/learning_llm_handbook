{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Large Language Models: A Step-by-Step Journey\n",
    "\n",
    "## Our Goal\n",
    "\n",
    "In this notebook, we will understand how language models predict text by following the complete process of predicting \"we love deep learning\" word by word. We'll explore four fundamental concepts that make modern language models like ChatGPT work:\n",
    "\n",
    "1. **Forward Pass**: How models generate predictions\n",
    "2. **Loss Calculation**: How we measure prediction quality\n",
    "3. **Backpropagation**: How we identify what needs improvement\n",
    "4. **Gradient Descent**: How we make those improvements\n",
    "\n",
    "## Our Vocabulary and Target\n",
    "\n",
    "We'll work with a simple vocabulary to keep things clear and manageable. Our model will learn to predict each word in our target sequence step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY: ['<BOS>', 'we', 'love', 'deep', 'learning', '<EOS>', 'the', 'is', 'great', 'model', 'hello', 'world']\n",
      "TARGET SEQUENCE: ['we', 'love', 'deep', 'learning']\n",
      "VOCABULARY SIZE: 12\n",
      "\n",
      "Initial model parameters (simplified representation):\n",
      "Layer 1 weights: 12 parameters\n",
      "Layer 2 weights: 12 parameters\n",
      "Output weights: 12 parameters\n"
     ]
    }
   ],
   "source": [
    "# Setup our simple vocabulary and target sequence\n",
    "VOCAB = [\"<BOS>\", \"we\", \"love\", \"deep\", \"learning\", \"<EOS>\", \"the\", \"is\", \"great\", \"model\", \"hello\", \"world\"]\n",
    "target_sequence = [\"we\", \"love\", \"deep\", \"learning\"]\n",
    "\n",
    "print(\"VOCABULARY:\", VOCAB)\n",
    "print(\"TARGET SEQUENCE:\", target_sequence)\n",
    "print(\"VOCABULARY SIZE:\", len(VOCAB))\n",
    "\n",
    "# Initialize simple model parameters (weights) - these will be updated during training\n",
    "# In real models, these would be millions or billions of parameters\n",
    "model_parameters = {\n",
    "    'layer1_weights': [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, 0.8, -0.2, 0.3, -0.5, 0.7, -0.4],\n",
    "    'layer2_weights': [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, -0.1, 0.5, -0.3, 0.4, -0.6, 0.1],\n",
    "    'output_weights': [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, -0.3, 0.2, 0.6, -0.4, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "print(\"\\nInitial model parameters (simplified representation):\")\n",
    "print(\"Layer 1 weights:\", len(model_parameters['layer1_weights']), \"parameters\")\n",
    "print(\"Layer 2 weights:\", len(model_parameters['layer2_weights']), \"parameters\") \n",
    "print(\"Output weights:\", len(model_parameters['output_weights']), \"parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Logits?\n",
    "\n",
    "Logits are the raw numerical scores that a language model assigns to every word in its vocabulary when predicting the next word. Think of logits as the model's initial \"gut feeling\" about how likely each word is to come next, before any normalization.\n",
    "\n",
    "**Key Properties of Logits:**\n",
    "\n",
    "- They can be any real number (positive, negative, large, small)\n",
    "- Higher logits indicate the model thinks a word is more likely\n",
    "- Lower logits indicate the model thinks a word is less likely\n",
    "- They are computed by passing the current context through the neural network layers\n",
    "\n",
    "Let's see what logits look like when our model tries to predict the first word after the beginning-of-sequence token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit scores for predicting first word after <BOS>:\n",
      "==================================================\n",
      "the       :    3.2\n",
      "hello     :    2.8\n",
      "we        :    2.1 <- Our target!\n",
      "is        :    1.5\n",
      "love      :   -0.5\n",
      "deep      :   -1.2\n",
      "learning  :   -2.0\n",
      "<EOS>     :  -10.0\n",
      "\n",
      "Notice how 'the' has the highest logit (3.2) because it's\n",
      "the most common way to start sentences in English.\n",
      "Our target word 'we' has a logit of 2.1, which is decent\n",
      "but not the highest - the model will need training to improve this!\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_logits():\n",
    "    # Pre-calculated realistic logit values for predicting first word after <BOS>\n",
    "    logits_after_bos = {\n",
    "        \"we\": 2.1,      # Target word - decent score\n",
    "        \"the\": 3.2,     # Highest - most common starter\n",
    "        \"hello\": 2.8,   # High - common greeting\n",
    "        \"is\": 1.5,      # Medium - possible starter\n",
    "        \"love\": -0.5,   # Low - uncommon starter\n",
    "        \"deep\": -1.2,   # Lower - rare starter\n",
    "        \"learning\": -2.0, # Very low - very rare starter\n",
    "        \"<EOS>\": -10.0, # Impossible - can't start with end token\n",
    "    }\n",
    "    \n",
    "    print(\"Logit scores for predicting first word after <BOS>:\")\n",
    "    print(\"=\" * 50)\n",
    "    for word, logit in sorted(logits_after_bos.items(), key=lambda x: x[1], reverse=True):\n",
    "        marker = \" <- Our target!\" if word == \"we\" else \"\"\n",
    "        print(f\"{word:10}: {logit:6.1f}{marker}\")\n",
    "    \n",
    "    print(f\"\\nNotice how 'the' has the highest logit ({logits_after_bos['the']}) because it's\")\n",
    "    print(\"the most common way to start sentences in English.\")\n",
    "    print(f\"Our target word 'we' has a logit of {logits_after_bos['we']}, which is decent\")\n",
    "    print(\"but not the highest - the model will need training to improve this!\")\n",
    "    \n",
    "    return logits_after_bos\n",
    "\n",
    "# Run the demonstration\n",
    "logits_step1 = demonstrate_logits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Logits to Probabilities: The Softmax Operation\n",
    "\n",
    "Raw logits are useful for the model internally, but they're not easy for us to interpret. We need to convert them into probabilities - numbers between 0 and 1 that sum to exactly 1.0. This conversion is done using the **softmax function**.\n",
    "\n",
    "**The Softmax Process:**\n",
    "\n",
    "1. Calculate e^(logit) for each word (this makes all values positive)\n",
    "2. Sum all these exponential values\n",
    "3. Divide each exponential value by the sum\n",
    "\n",
    "This ensures we get valid probabilities that represent the model's confidence in each possible next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFTMAX CONVERSION: From Logits to Probabilities\n",
      "=======================================================\n",
      "STEP 1: Calculate e^(logit) for each word\n",
      "----------------------------------------\n",
      "e^(  2.1) =     8.17  for 'we'\n",
      "e^(  3.2) =    24.53  for 'the'\n",
      "e^(  2.8) =    16.44  for 'hello'\n",
      "e^(  1.5) =     4.48  for 'is'\n",
      "e^( -0.5) =     0.61  for 'love'\n",
      "e^( -1.2) =     0.30  for 'deep'\n",
      "e^( -2.0) =     0.14  for 'learning'\n",
      "e^(-10.0) =     0.00  for '<EOS>'\n",
      "\n",
      "STEP 2: Sum all exponential values\n",
      "-----------------------------------\n",
      "Total sum = 54.67\n",
      "\n",
      "STEP 3: Calculate final probabilities\n",
      "--------------------------------------\n",
      "P(we      ) =     8.17 / 54.67 = 0.1494 <- Our target!\n",
      "P(the     ) =    24.53 / 54.67 = 0.4488\n",
      "P(hello   ) =    16.44 / 54.67 = 0.3008\n",
      "P(is      ) =     4.48 / 54.67 = 0.0820\n",
      "P(love    ) =     0.61 / 54.67 = 0.0111\n",
      "P(deep    ) =     0.30 / 54.67 = 0.0055\n",
      "P(learning) =     0.14 / 54.67 = 0.0025\n",
      "P(<EOS>   ) =     0.00 / 54.67 = 0.0000\n",
      "\n",
      "Verification: All probabilities sum to 1.000000\n",
      "\n",
      "Key insight: 'the' had the highest logit\n",
      "and now has the highest probability (0.4488)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def demonstrate_softmax(logits_dict):\n",
    "    print(\"SOFTMAX CONVERSION: From Logits to Probabilities\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    words = list(logits_dict.keys())\n",
    "    logits = list(logits_dict.values())\n",
    "    \n",
    "    print(\"STEP 1: Calculate e^(logit) for each word\")\n",
    "    print(\"-\" * 40)\n",
    "    exp_logits = []\n",
    "    for word, logit in zip(words, logits):\n",
    "        exp_val = math.exp(logit)\n",
    "        exp_logits.append(exp_val)\n",
    "        print(f\"e^({logit:5.1f}) = {exp_val:8.2f}  for '{word}'\")\n",
    "    \n",
    "    print(f\"\\nSTEP 2: Sum all exponential values\")\n",
    "    print(\"-\" * 35)\n",
    "    total = sum(exp_logits)\n",
    "    print(f\"Total sum = {total:.2f}\")\n",
    "    \n",
    "    print(f\"\\nSTEP 3: Calculate final probabilities\")\n",
    "    print(\"-\" * 38)\n",
    "    probabilities = {}\n",
    "    for word, exp_val in zip(words, exp_logits):\n",
    "        prob = exp_val / total\n",
    "        probabilities[word] = prob\n",
    "        marker = \" <- Our target!\" if word == \"we\" else \"\"\n",
    "        print(f\"P({word:8}) = {exp_val:8.2f} / {total:.2f} = {prob:.4f}{marker}\")\n",
    "    \n",
    "    print(f\"\\nVerification: All probabilities sum to {sum(probabilities.values()):.6f}\")\n",
    "    print(f\"\\nKey insight: '{max(logits_dict, key=logits_dict.get)}' had the highest logit\")\n",
    "    print(f\"and now has the highest probability ({max(probabilities.values()):.4f})\")\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "# Convert our logits to probabilities\n",
    "probabilities_step1 = demonstrate_softmax(logits_step1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Prediction: How Context Shapes Predictions\n",
    "\n",
    "Language models don't just predict single words in isolation - they use the entire preceding context to make increasingly informed predictions. As we build up the sequence \"we love deep learning,\" each new word provides more context that helps the model make better predictions for the next word.\n",
    "\n",
    "Let's observe how the model's logit scores change as we provide more context at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE SEQUENCE PREDICTION: 'we love deep learning'\n",
      "============================================================\n",
      "\n",
      "STEP 1: Context = '<BOS>' -> Predicting 'we'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. the       :    3.2\n",
      "  2. hello     :    2.8\n",
      "  3. we        :    2.1 ** TARGET **\n",
      "  4. is        :    1.5\n",
      "  5. love      :   -0.5\n",
      "\n",
      "Target word 'we' probability: 0.149\n",
      "Context effect: 'we' is a common sentence starter, though 'the' is even more common in general text\n",
      "\n",
      "STEP 2: Context = 'we' -> Predicting 'love'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. love      :    2.8 ** TARGET **\n",
      "  2. is        :    2.1\n",
      "  3. the       :    1.2\n",
      "  4. deep      :   -0.5\n",
      "  5. learning  :   -1.5\n",
      "\n",
      "Target word 'love' probability: 0.571\n",
      "Context effect: After 'we', action words like 'love', 'are', 'have' become much more likely than nouns\n",
      "\n",
      "STEP 3: Context = 'we love' -> Predicting 'deep'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. deep      :    1.9 ** TARGET **\n",
      "  2. learning  :    0.8\n",
      "  3. the       :    0.5\n",
      "  4. is        :   -2.0\n",
      "  5. love      :   -4.0\n",
      "\n",
      "Target word 'deep' probability: 0.623\n",
      "Context effect: After 'we love', we expect objects or concepts; 'deep' scores well as it often precedes 'learning'\n",
      "\n",
      "STEP 4: Context = 'we love deep' -> Predicting 'learning'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. learning  :    3.5 ** TARGET **\n",
      "  2. the       :   -2.0\n",
      "  3. is        :   -3.0\n",
      "  4. deep      :   -4.0\n",
      "  5. love      :   -5.0\n",
      "\n",
      "Target word 'learning' probability: 0.994\n",
      "Context effect: 'deep learning' is a common collocation - 'learning' becomes very likely after 'deep' in this context\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_sequence_prediction():\n",
    "    print(\"COMPLETE SEQUENCE PREDICTION: 'we love deep learning'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Pre-calculated logits for each prediction step showing how context improves predictions\n",
    "    prediction_steps = [\n",
    "        {\n",
    "            \"step\": 1,\n",
    "            \"context\": \"<BOS>\",\n",
    "            \"target\": \"we\", \n",
    "            \"logits\": {\"we\": 2.1, \"love\": -0.5, \"deep\": -1.2, \"learning\": -2.0, \"the\": 3.2, \"hello\": 2.8, \"is\": 1.5, \"<EOS>\": -10.0},\n",
    "            \"explanation\": \"'we' is a common sentence starter, though 'the' is even more common in general text\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 2,\n",
    "            \"context\": \"we\",\n",
    "            \"target\": \"love\",\n",
    "            \"logits\": {\"we\": -3.0, \"love\": 2.8, \"deep\": -0.5, \"learning\": -1.5, \"the\": 1.2, \"hello\": -5.0, \"is\": 2.1, \"<EOS>\": -8.0},\n",
    "            \"explanation\": \"After 'we', action words like 'love', 'are', 'have' become much more likely than nouns\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 3,\n",
    "            \"context\": \"we love\",\n",
    "            \"target\": \"deep\",\n",
    "            \"logits\": {\"we\": -5.0, \"love\": -4.0, \"deep\": 1.9, \"learning\": 0.8, \"the\": 0.5, \"hello\": -6.0, \"is\": -2.0, \"<EOS>\": -7.0},\n",
    "            \"explanation\": \"After 'we love', we expect objects or concepts; 'deep' scores well as it often precedes 'learning'\"\n",
    "        },\n",
    "        {\n",
    "            \"step\": 4,\n",
    "            \"context\": \"we love deep\",\n",
    "            \"target\": \"learning\",\n",
    "            \"logits\": {\"we\": -6.0, \"love\": -5.0, \"deep\": -4.0, \"learning\": 3.5, \"the\": -2.0, \"hello\": -8.0, \"is\": -3.0, \"<EOS>\": -6.0},\n",
    "            \"explanation\": \"'deep learning' is a common collocation - 'learning' becomes very likely after 'deep' in this context\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    all_step_probabilities = []\n",
    "    \n",
    "    for step_info in prediction_steps:\n",
    "        print(f\"\\nSTEP {step_info['step']}: Context = '{step_info['context']}' -> Predicting '{step_info['target']}'\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Show top 5 logits\n",
    "        sorted_logits = sorted(step_info['logits'].items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Top 5 logit scores:\")\n",
    "        for i, (word, logit) in enumerate(sorted_logits[:5]):\n",
    "            marker = \" ** TARGET **\" if word == step_info['target'] else \"\"\n",
    "            print(f\"  {i+1}. {word:10}: {logit:6.1f}{marker}\")\n",
    "        \n",
    "        # Calculate probability for target word\n",
    "        target_logit = step_info['logits'][step_info['target']]\n",
    "        exp_values = [math.exp(logit) for logit in step_info['logits'].values()]\n",
    "        total_exp = sum(exp_values)\n",
    "        target_prob = math.exp(target_logit) / total_exp\n",
    "        all_step_probabilities.append(target_prob)\n",
    "        \n",
    "        print(f\"\\nTarget word '{step_info['target']}' probability: {target_prob:.3f}\")\n",
    "        print(f\"Context effect: {step_info['explanation']}\")\n",
    "    \n",
    "    return prediction_steps, all_step_probabilities\n",
    "\n",
    "# Generate predictions for the complete sequence\n",
    "steps_data, step_probabilities = demonstrate_sequence_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function: Quantifying Prediction Quality\n",
    "\n",
    "The **loss function** is how we measure how well our model is performing. Specifically, it measures how \"surprised\" the model is when it sees the correct answer. The mathematical formulation we use is called **cross-entropy loss** or **log-likelihood loss**.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Lower loss = better predictions = less surprise when seeing the correct answer\n",
    "- Higher loss = worse predictions = more surprise when seeing the correct answer  \n",
    "- Loss is calculated as: Loss = -log(probability of correct word)\n",
    "- Training aims to minimize the total loss across all predictions\n",
    "\n",
    "The logarithm has a **useful property**: it heavily penalizes very low probabilities. If the model assigns a probability of 0.01 to the correct word, the loss is much higher than if it assigns 0.1.\n",
    "\n",
    "### The Training Dataset as Ground Truth\n",
    "\n",
    "Before we dive into the mathematical details, it's crucial to understand what the loss function is actually measuring and why it works. The loss function compares our model's predictions against what we call the **\"ground truth\"** - the training dataset that contains the correct answers.\n",
    "\n",
    "**Training Dataset as Golden Labels:**\n",
    "\n",
    "When we train a language model, we provide it with a massive dataset of text (books, articles, websites, etc.). This dataset represents our best approximation of \"correct\" language use. Each sequence in this dataset serves as:\n",
    "\n",
    "- **Ground Truth**: The actual words that should come next in real language\n",
    "- **Golden Labels**: The target answers our model should learn to predict\n",
    "- **Reference Distribution**: The pattern of language we want our model to internalize\n",
    "\n",
    "For our example sequence \"we love deep learning\", this represents a small piece of ground truth data. When we calculate loss, we're measuring how well our model's predictions align with this established pattern of language.\n",
    "\n",
    "**From Random Weights to Meaningful Predictions:**\n",
    "\n",
    "Remember our initial model parameters from Section 1? Those weights were randomly initialized:\n",
    "\n",
    "```python\n",
    "model_parameters = {\n",
    "    'layer1_weights': [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, ...],\n",
    "    'layer2_weights': [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, ...],\n",
    "    'output_weights': [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, ...]\n",
    "}\n",
    "```\n",
    "\n",
    "These random numbers produce the predictions we saw in previous sections. The model doesn't \"know\" anything about language initially - its probabilities for predicting \"we\" (0.149) or \"love\" (0.571) come purely from these random mathematical operations.\n",
    "\n",
    "**The Training Objective: Fitting to the Dataset Distribution**\n",
    "\n",
    "The fundamental goal of training is to **fit the model to the training dataset's distribution**. This means:\n",
    "\n",
    "1. **Statistical Matching**: Adjust the weights so the model's predicted probabilities match the frequency patterns in the training data\n",
    "2. **Contextual Learning**: Teach the model that certain word combinations (like \"deep learning\") are more common than others\n",
    "3. **Pattern Internalization**: Embed the statistical regularities of human language into the neural network weights\n",
    "\n",
    "When we minimize the cross-entropy loss across millions of training examples, we're essentially forcing the model to internalize the probability distribution of the training dataset. The model learns that \"the\" is more common after \"<BOS>\" because it appears frequently in training data, not because it has any inherent understanding of language.\n",
    "\n",
    "**Inductive Bias: The Hidden Consequence of Training Data**\n",
    "\n",
    "This training process creates what we call an **inductive bias** - a set of assumptions and tendencies that the model develops based on its training data. This bias manifests in several ways:\n",
    "\n",
    "1. **Domain Bias**: If trained on academic papers, the model will favor formal language\n",
    "2. **Cultural Bias**: Training data reflects the cultural context of its sources\n",
    "3. **Temporal Bias**: Models reflect the time period when their training data was collected\n",
    "4. **Statistical Bias**: Common patterns in training data become \"preferred\" by the model\n",
    "\n",
    "Our model's tendency to predict \"the\" with high probability after \"<BOS>\" isn't based on understanding - it's an inductive bias learned from statistical patterns in training data where \"the\" frequently starts sentences.\n",
    "\n",
    "**The Circular Nature of Language Modeling:**\n",
    "\n",
    "There's a profound circularity here: we use human-written text to train models to predict human-like text. The model doesn't learn \"language\" in an abstract sense - it learns to mimic the specific patterns present in its training distribution. This is why the choice of training data is so critical and why different models can have vastly different behaviors based on their training datasets.\n",
    "\n",
    "The loss function, therefore, isn't just measuring prediction accuracy - it's measuring how well the model has absorbed and can reproduce the inductive biases present in its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mathematical Foundation: Next Token Prediction Loss\n",
    "\n",
    "Now let's dive deep into the precise mathematical formulation used in Large Language Models. The loss function we use is called **Negative Log-Likelihood (NLL)** for **Next Token Prediction (NTP)**.\n",
    "\n",
    "#### The Complete NLL Formula\n",
    "\n",
    "For **Next Token Prediction** in Large Language Models, the negative log-likelihood formula is:\n",
    "\n",
    "**NLL = -∑ᵢ₌₁ⁿ log P(xᵢ | x₁, x₂, ..., xᵢ₋₁; θ)**\n",
    "\n",
    "**Where:**\n",
    "- **xᵢ** = the i-th token in the sequence (e.g., \"we\", \"love\", \"deep\", \"learning\")\n",
    "- **x₁, x₂, ..., xᵢ₋₁** = all previous tokens providing context\n",
    "- **θ** = model parameters (our weights from Section 1)\n",
    "- **n** = sequence length (4 tokens in our example)\n",
    "\n",
    "#### Breaking Down Each Component\n",
    "\n",
    "**1. Per-Token Probability Computation:**\n",
    "\n",
    "The probability of each token given its context is computed using softmax:\n",
    "\n",
    "```\n",
    "P(xᵢ | context) = softmax(logits)ₓᵢ = exp(zₓᵢ) / ∑ⱼ₌₁ᵛ exp(zⱼ)\n",
    "```\n",
    "\n",
    "**Where:**\n",
    "- **zₓᵢ** = logit score for token xᵢ (like our 2.1 for \"we\")\n",
    "- **V** = vocabulary size (12 in our case)\n",
    "- **j** indexes over entire vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregressive Nature\n",
    "\n",
    "The key insight is that each prediction depends on **all previous tokens**:\n",
    "\n",
    "- **Step 1**: P(\"we\" | \"<BOS>\") - no previous content words\n",
    "- **Step 2**: P(\"love\" | \"we\") - uses \"we\" as context  \n",
    "- **Step 3**: P(\"deep\" | \"we\", \"love\") - uses both previous tokens\n",
    "- **Step 4**: P(\"learning\" | \"we\", \"love\", \"deep\") - uses all three previous tokens\n",
    "\n",
    "This is why our predictions improved with more context in Section 4 - the model had more information to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to Perplexity\n",
    "\n",
    "The loss is often reported as **perplexity**: \n",
    "\n",
    "```\n",
    "Perplexity = exp(NLL/n)\n",
    "```\n",
    "\n",
    "Lower perplexity indicates better language modeling performance.\n",
    "\n",
    "This mathematical foundation captures the core training objective of all Large Language Models: **maximize the probability of the correct next token given the context**. Every word prediction in ChatGPT, GPT-4, or any other LLM follows this exact mathematical framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS-ENTROPY LOSS FORMULA: Step-by-Step Breakdown\n",
      "============================================================\n",
      "The cross-entropy loss formula is: Loss = -log(P(correct_word))\n",
      "Let's break this down with a concrete example:\n",
      "\n",
      "TOY EXAMPLE: Predicting 'cat' from vocabulary ['dog', 'cat', 'bird']\n",
      "-----------------------------------------------------------------\n",
      "Model predictions (probabilities):\n",
      "  P(dog) = 0.2\n",
      "  P(cat) = 0.6 <- CORRECT ANSWER\n",
      "  P(bird) = 0.2\n",
      "\n",
      "Verification: 1.0 (probabilities sum to 1.0)\n",
      "\n",
      "STEP-BY-STEP LOSS CALCULATION:\n",
      "-----------------------------------\n",
      "Step 1: Identify the correct word\n",
      "        Correct word = 'cat'\n",
      "\n",
      "Step 2: Find its probability\n",
      "        P(cat) = 0.6\n",
      "\n",
      "Step 3: Take the natural logarithm\n",
      "        log(P(cat)) = log(0.6) = -0.5108\n",
      "\n",
      "Step 4: Apply the negative sign\n",
      "        Loss = -log(P(cat)) = -(-0.5108) = 0.5108\n",
      "\n",
      "FINAL RESULT: Loss = 0.5108\n",
      "\n",
      "==================================================\n",
      "WHY NEGATIVE LOGARITHM?\n",
      "==================================================\n",
      "Probability -> log(prob) -> -log(prob) (Loss)\n",
      "---------------------------------------------\n",
      "   0.90    ->    -0.11   ->     0.11  (Excellent)\n",
      "   0.50    ->    -0.69   ->     0.69  (Good)\n",
      "   0.10    ->    -2.30   ->     2.30  (Poor)\n",
      "   0.01    ->    -4.61   ->     4.61  (Terrible)\n",
      "\n",
      "Key insight: Higher probability -> Lower loss (what we want!)\n",
      "            Lower probability -> Higher loss (penalty for bad predictions)\n",
      "\n",
      "======================================================================\n",
      "APPLYING LOSS TO OUR SEQUENCE: 'we love deep learning'\n",
      "======================================================================\n",
      "Now let's calculate loss for each word in our target sequence:\n",
      "Formula: Loss = -log(P(correct_word))\n",
      "\n",
      "STEP 1: Predicting 'we'\n",
      "-------------------------\n",
      "  Model prediction: P(we) = 0.149\n",
      "  Take logarithm:   log(0.149) = -1.9013\n",
      "  Apply negative:   Loss = -(-1.9013) = 1.9013\n",
      "  Interpretation:   Decent prediction\n",
      "\n",
      "STEP 2: Predicting 'love'\n",
      "-------------------------\n",
      "  Model prediction: P(love) = 0.571\n",
      "  Take logarithm:   log(0.571) = -0.5610\n",
      "  Apply negative:   Loss = -(-0.5610) = 0.5610\n",
      "  Interpretation:   Good prediction\n",
      "\n",
      "STEP 3: Predicting 'deep'\n",
      "-------------------------\n",
      "  Model prediction: P(deep) = 0.623\n",
      "  Take logarithm:   log(0.623) = -0.4725\n",
      "  Apply negative:   Loss = -(-0.4725) = 0.4725\n",
      "  Interpretation:   Excellent prediction!\n",
      "\n",
      "STEP 4: Predicting 'learning'\n",
      "-------------------------\n",
      "  Model prediction: P(learning) = 0.994\n",
      "  Take logarithm:   log(0.994) = -0.0065\n",
      "  Apply negative:   Loss = -(-0.0065) = 0.0065\n",
      "  Interpretation:   Excellent prediction!\n",
      "\n",
      "TOTAL SEQUENCE LOSS:\n",
      "--------------------\n",
      "Sum of all losses = 2.9412\n",
      "Average per word  = 0.7353\n",
      "\n",
      "Training goal: Reduce this total loss by improving individual word predictions!\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_cross_entropy_formula():\n",
    "    print(\"CROSS-ENTROPY LOSS FORMULA: Step-by-Step Breakdown\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"The cross-entropy loss formula is: Loss = -log(P(correct_word))\")\n",
    "    print(\"Let's break this down with a concrete example:\\n\")\n",
    "    \n",
    "    # Use a simple toy example\n",
    "    print(\"TOY EXAMPLE: Predicting 'cat' from vocabulary ['dog', 'cat', 'bird']\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # Toy probabilities for demonstration\n",
    "    toy_probabilities = {\n",
    "        \"dog\": 0.2,\n",
    "        \"cat\": 0.6,   # correct word\n",
    "        \"bird\": 0.2\n",
    "    }\n",
    "    \n",
    "    print(\"Model predictions (probabilities):\")\n",
    "    for word, prob in toy_probabilities.items():\n",
    "        marker = \" <- CORRECT ANSWER\" if word == \"cat\" else \"\"\n",
    "        print(f\"  P({word}) = {prob:.1f}{marker}\")\n",
    "    \n",
    "    print(f\"\\nVerification: {sum(toy_probabilities.values()):.1f} (probabilities sum to 1.0)\")\n",
    "    \n",
    "    print(f\"\\nSTEP-BY-STEP LOSS CALCULATION:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    correct_word = \"cat\"\n",
    "    correct_prob = toy_probabilities[correct_word]\n",
    "    \n",
    "    print(f\"Step 1: Identify the correct word\")\n",
    "    print(f\"        Correct word = '{correct_word}'\")\n",
    "    \n",
    "    print(f\"\\nStep 2: Find its probability\")\n",
    "    print(f\"        P({correct_word}) = {correct_prob}\")\n",
    "    \n",
    "    print(f\"\\nStep 3: Take the natural logarithm\")\n",
    "    log_prob = math.log(correct_prob)\n",
    "    print(f\"        log(P({correct_word})) = log({correct_prob}) = {log_prob:.4f}\")\n",
    "    \n",
    "    print(f\"\\nStep 4: Apply the negative sign\")\n",
    "    loss = -log_prob\n",
    "    print(f\"        Loss = -log(P({correct_word})) = -({log_prob:.4f}) = {loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFINAL RESULT: Loss = {loss:.4f}\")\n",
    "    \n",
    "    # Show why we use negative log\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"WHY NEGATIVE LOGARITHM?\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    prob_examples = [0.9, 0.5, 0.1, 0.01]\n",
    "    print(\"Probability -> log(prob) -> -log(prob) (Loss)\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for prob in prob_examples:\n",
    "        log_val = math.log(prob)\n",
    "        loss_val = -log_val\n",
    "        quality = \"Excellent\" if prob > 0.8 else \"Good\" if prob > 0.4 else \"Poor\" if prob > 0.05 else \"Terrible\"\n",
    "        print(f\"   {prob:4.2f}    ->   {log_val:6.2f}   ->   {loss_val:6.2f}  ({quality})\")\n",
    "    \n",
    "    print(f\"\\nKey insight: Higher probability -> Lower loss (what we want!)\")\n",
    "    print(f\"            Lower probability -> Higher loss (penalty for bad predictions)\")\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def demonstrate_loss_calculation():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"APPLYING LOSS TO OUR SEQUENCE: 'we love deep learning'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"Now let's calculate loss for each word in our target sequence:\")\n",
    "    print(\"Formula: Loss = -log(P(correct_word))\")\n",
    "    print()\n",
    "    \n",
    "    words = [\"we\", \"love\", \"deep\", \"learning\"]\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (word, prob) in enumerate(zip(words, step_probabilities)):\n",
    "        print(f\"STEP {i+1}: Predicting '{word}'\")\n",
    "        print(\"-\" * 25)\n",
    "        print(f\"  Model prediction: P({word}) = {prob:.3f}\")\n",
    "        print(f\"  Take logarithm:   log({prob:.3f}) = {math.log(prob):.4f}\")\n",
    "        \n",
    "        loss = -math.log(prob)\n",
    "        total_loss += loss\n",
    "        \n",
    "        print(f\"  Apply negative:   Loss = -({math.log(prob):.4f}) = {loss:.4f}\")\n",
    "        \n",
    "        # Interpret the loss\n",
    "        if loss < 0.5:\n",
    "            interpretation = \"Excellent prediction!\"\n",
    "        elif loss < 1.0:\n",
    "            interpretation = \"Good prediction\"\n",
    "        elif loss < 2.0:\n",
    "            interpretation = \"Decent prediction\"\n",
    "        else:\n",
    "            interpretation = \"Poor prediction - needs improvement\"\n",
    "            \n",
    "        print(f\"  Interpretation:   {interpretation}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"TOTAL SEQUENCE LOSS:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Sum of all losses = {total_loss:.4f}\")\n",
    "    print(f\"Average per word  = {total_loss/len(words):.4f}\")\n",
    "    print(f\"\\nTraining goal: Reduce this total loss by improving individual word predictions!\")\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# First demonstrate the formula with toy example\n",
    "toy_loss = demonstrate_cross_entropy_formula()\n",
    "\n",
    "# Then apply to our actual sequence\n",
    "sequence_loss = demonstrate_loss_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
