{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Large Language Models: Understanding the Fundamentals by Simulated Examples\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will understand how language models predict text by following the complete process of predicting \"we love deep learning\" word by word. We'll explore four fundamental concepts that make modern language models like ChatGPT work. We will use simulated examples to illustrate these concepts, where functions will use static values instead of actual neural network calculations. We will explore the use real values in later notebooks in this course. The core concepts we will explore are:\n",
    "\n",
    "1. **Forward Pass**: How models generate predictions\n",
    "2. **Loss Calculation**: How we measure prediction quality\n",
    "3. **Backpropagation**: How we identify what needs improvement\n",
    "4. **Gradient Descent**: How we make those improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Vocabulary and Target\n",
    "\n",
    "We explored what a vocabulary is in the Tokenization notebook in week 1 previously, so we will not cover this topic in detail here.\n",
    "\n",
    "We'll work with a simple vocabulary to keep things clear and manageable for our learning example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY: ['<BOS>', 'we', 'love', 'deep', 'learning', '<EOS>', 'the', 'is', 'great', 'model', 'hello', 'world']\n",
      "TARGET SEQUENCE: ['we', 'love', 'deep', 'learning']\n",
      "VOCABULARY SIZE: 12\n",
      "\n",
      "Initial model parameters (simplified representation):\n",
      "Layer 1 weights: 12 parameters\n",
      "Layer 2 weights: 12 parameters\n",
      "Output weights: 12 parameters\n"
     ]
    }
   ],
   "source": [
    "# Setup our simple vocabulary and target sequence\n",
    "VOCAB = [\n",
    "    \"<BOS>\",\n",
    "    \"we\",\n",
    "    \"love\",\n",
    "    \"deep\",\n",
    "    \"learning\",\n",
    "    \"<EOS>\",\n",
    "    \"the\",\n",
    "    \"is\",\n",
    "    \"great\",\n",
    "    \"model\",\n",
    "    \"hello\",\n",
    "    \"world\",\n",
    "]\n",
    "target_sequence = [\"we\", \"love\", \"deep\", \"learning\"]\n",
    "\n",
    "print(\"VOCABULARY:\", VOCAB)\n",
    "print(\"TARGET SEQUENCE:\", target_sequence)\n",
    "print(\"VOCABULARY SIZE:\", len(VOCAB))\n",
    "\n",
    "# Initialize simple model parameters (weights) - these will be updated during training\n",
    "# In real models, these would be millions or billions of parameters\n",
    "model_parameters = {\n",
    "    \"layer1_weights\": [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, 0.8, -0.2, 0.3, -0.5, 0.7, -0.4],\n",
    "    \"layer2_weights\": [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, -0.1, 0.5, -0.3, 0.4, -0.6, 0.1],\n",
    "    \"output_weights\": [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, -0.3, 0.2, 0.6, -0.4, 0.1, 0.3],\n",
    "}\n",
    "\n",
    "print(\"\\nInitial model parameters (simplified representation):\")\n",
    "print(\"Layer 1 weights:\", len(model_parameters[\"layer1_weights\"]), \"parameters\")\n",
    "print(\"Layer 2 weights:\", len(model_parameters[\"layer2_weights\"]), \"parameters\")\n",
    "print(\"Output weights:\", len(model_parameters[\"output_weights\"]), \"parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Logits?\n",
    "\n",
    "Logits are the raw numerical scores that a language model assigns to every word in its vocabulary when predicting the next word. Think of logits as the model's initial \"gut feeling\" about how likely each word is to come next, before any normalization.\n",
    "\n",
    "**Key Properties of Logits:**\n",
    "\n",
    "- They can be any real number (positive, negative, large, small)\n",
    "- Higher logits indicate the model thinks a word is more likely\n",
    "- Lower logits indicate the model thinks a word is less likely\n",
    "- They are computed by passing the current context through the neural network layers\n",
    "\n",
    "Let's see what logits look like when our model tries to predict the first word after the beginning-of-sequence token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit scores for predicting first word after <BOS>:\n",
      "==================================================\n",
      "the       :    3.2\n",
      "hello     :    2.8\n",
      "we        :    2.1 <- Our target!\n",
      "is        :    1.5\n",
      "love      :   -0.5\n",
      "deep      :   -1.2\n",
      "learning  :   -2.0\n",
      "<EOS>     :  -10.0\n",
      "\n",
      "Notice how 'the' has the highest logit (3.2) because it's one of the most common ways to start sentences in English.\n",
      "Our target word 'we' has a logit of 2.1, which is decent but not the highest - the model will need training to improve this!\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_logits():\n",
    "    # Pre-calculated realistic logit values for predicting first word after <BOS>\n",
    "    logits_after_bos = {\n",
    "        \"we\": 2.1,  # Target word - decent score\n",
    "        \"the\": 3.2,  # Highest - most common starter\n",
    "        \"hello\": 2.8,  # High - common greeting\n",
    "        \"is\": 1.5,  # Medium - possible starter\n",
    "        \"love\": -0.5,  # Low - uncommon starter\n",
    "        \"deep\": -1.2,  # Lower - rare starter\n",
    "        \"learning\": -2.0,  # Very low - very rare starter\n",
    "        \"<EOS>\": -10.0,  # Impossible - can't start with end token\n",
    "    }\n",
    "\n",
    "    print(\"Logit scores for predicting first word after <BOS>:\")\n",
    "    print(\"=\" * 50)\n",
    "    for word, logit in sorted(\n",
    "        logits_after_bos.items(), key=lambda x: x[1], reverse=True\n",
    "    ):\n",
    "        marker = \" <- Our target!\" if word == \"we\" else \"\"\n",
    "        print(f\"{word:10}: {logit:6.1f}{marker}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nNotice how 'the' has the highest logit ({logits_after_bos['the']}) because it's one of the most common ways to start sentences in English.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Our target word 'we' has a logit of {logits_after_bos['we']}, which is decent but not the highest - the model will need training to improve this!\"\n",
    "    )\n",
    "\n",
    "    return logits_after_bos\n",
    "\n",
    "\n",
    "# Run the demonstration\n",
    "logits_step1 = demonstrate_logits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Logits to Probabilities: The Softmax Operation\n",
    "\n",
    "Raw logits are useful for the model internally, but they're not easy for us to interpret. We need to convert them into probabilities - numbers between 0 and 1 that sum to exactly 1.0. This conversion is done using the **softmax function**.\n",
    "\n",
    "**The Softmax Process:**\n",
    "\n",
    "1. Calculate e^(logit) for each word (this makes all values positive)\n",
    "2. Sum all these exponential values\n",
    "3. Divide each exponential value by the sum\n",
    "\n",
    "This ensures we get valid probabilities that represent the model's confidence in each possible next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFTMAX CONVERSION: From Logits to Probabilities\n",
      "=======================================================\n",
      "STEP 1: Calculate e^(logit) for each word\n",
      "----------------------------------------\n",
      "e^(  2.1) =     8.17  for 'we'\n",
      "e^(  3.2) =    24.53  for 'the'\n",
      "e^(  2.8) =    16.44  for 'hello'\n",
      "e^(  1.5) =     4.48  for 'is'\n",
      "e^( -0.5) =     0.61  for 'love'\n",
      "e^( -1.2) =     0.30  for 'deep'\n",
      "e^( -2.0) =     0.14  for 'learning'\n",
      "e^(-10.0) =     0.00  for '<EOS>'\n",
      "\n",
      "STEP 2: Sum all exponential values\n",
      "-----------------------------------\n",
      "Total sum = 54.67\n",
      "\n",
      "STEP 3: Calculate final probabilities\n",
      "--------------------------------------\n",
      "P(we      ) =     8.17 / 54.67 = 0.1494 <- Our target!\n",
      "P(the     ) =    24.53 / 54.67 = 0.4488\n",
      "P(hello   ) =    16.44 / 54.67 = 0.3008\n",
      "P(is      ) =     4.48 / 54.67 = 0.0820\n",
      "P(love    ) =     0.61 / 54.67 = 0.0111\n",
      "P(deep    ) =     0.30 / 54.67 = 0.0055\n",
      "P(learning) =     0.14 / 54.67 = 0.0025\n",
      "P(<EOS>   ) =     0.00 / 54.67 = 0.0000\n",
      "\n",
      "Verification: All probabilities sum to 1.000000\n",
      "\n",
      "Key insight: 'the' had the highest logit\n",
      "and now has the highest probability (0.4488)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def demonstrate_softmax(logits_dict):\n",
    "    print(\"SOFTMAX CONVERSION: From Logits to Probabilities\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    words = list(logits_dict.keys())\n",
    "    logits = list(logits_dict.values())\n",
    "\n",
    "    print(\"STEP 1: Calculate e^(logit) for each word\")\n",
    "    print(\"-\" * 40)\n",
    "    exp_logits = []\n",
    "    for word, logit in zip(words, logits):\n",
    "        exp_val = math.exp(logit)\n",
    "        exp_logits.append(exp_val)\n",
    "        print(f\"e^({logit:5.1f}) = {exp_val:8.2f}  for '{word}'\")\n",
    "\n",
    "    print(f\"\\nSTEP 2: Sum all exponential values\")\n",
    "    print(\"-\" * 35)\n",
    "    total = sum(exp_logits)\n",
    "    print(f\"Total sum = {total:.2f}\")\n",
    "\n",
    "    print(f\"\\nSTEP 3: Calculate final probabilities\")\n",
    "    print(\"-\" * 38)\n",
    "    probabilities = {}\n",
    "    for word, exp_val in zip(words, exp_logits):\n",
    "        prob = exp_val / total\n",
    "        probabilities[word] = prob\n",
    "        marker = \" <- Our target!\" if word == \"we\" else \"\"\n",
    "        print(f\"P({word:8}) = {exp_val:8.2f} / {total:.2f} = {prob:.4f}{marker}\")\n",
    "\n",
    "    print(f\"\\nVerification: All probabilities sum to {sum(probabilities.values()):.6f}\")\n",
    "    print(\n",
    "        f\"\\nKey insight: '{max(logits_dict, key=logits_dict.get)}' had the highest logit\"\n",
    "    )\n",
    "    print(f\"and now has the highest probability ({max(probabilities.values()):.4f})\")\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "# Convert our logits to probabilities\n",
    "probabilities_step1 = demonstrate_softmax(logits_step1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Prediction: How Context Shapes Predictions\n",
    "\n",
    "Language models don't just predict single words in isolation - they use the entire preceding context to make increasingly informed predictions. As we build up the sequence \"we love deep learning,\" each new word provides more context that helps the model make better predictions for the next word.\n",
    "\n",
    "Let's observe how the model's logit scores change as we provide more context at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE SEQUENCE PREDICTION: 'we love deep learning'\n",
      "============================================================\n",
      "\n",
      "STEP 1: Context = '<BOS>' -> Predicting 'we'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. the       :    3.2\n",
      "  2. hello     :    2.8\n",
      "  3. we        :    2.1 ** TARGET **\n",
      "  4. is        :    1.5\n",
      "  5. love      :   -0.5\n",
      "\n",
      "Target word 'we' probability: 0.149\n",
      "Context effect: 'we' is a common sentence starter, though 'the' is even more common in general text\n",
      "\n",
      "STEP 2: Context = 'we' -> Predicting 'love'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. love      :    2.8 ** TARGET **\n",
      "  2. is        :    2.1\n",
      "  3. the       :    1.2\n",
      "  4. deep      :   -0.5\n",
      "  5. learning  :   -1.5\n",
      "\n",
      "Target word 'love' probability: 0.571\n",
      "Context effect: After 'we', action words like 'love', 'are', 'have' become much more likely than nouns\n",
      "\n",
      "STEP 3: Context = 'we love' -> Predicting 'deep'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. deep      :    1.9 ** TARGET **\n",
      "  2. learning  :    0.8\n",
      "  3. the       :    0.5\n",
      "  4. is        :   -2.0\n",
      "  5. love      :   -4.0\n",
      "\n",
      "Target word 'deep' probability: 0.623\n",
      "Context effect: After 'we love', we expect objects or concepts; 'deep' scores well as it often precedes 'learning'\n",
      "\n",
      "STEP 4: Context = 'we love deep' -> Predicting 'learning'\n",
      "----------------------------------------------------------------------\n",
      "Top 5 logit scores:\n",
      "  1. learning  :    3.5 ** TARGET **\n",
      "  2. the       :   -2.0\n",
      "  3. is        :   -3.0\n",
      "  4. deep      :   -4.0\n",
      "  5. love      :   -5.0\n",
      "\n",
      "Target word 'learning' probability: 0.994\n",
      "Context effect: 'deep learning' is a common collocation - 'learning' becomes very likely after 'deep' in this context\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_sequence_prediction():\n",
    "    print(\"COMPLETE SEQUENCE PREDICTION: 'we love deep learning'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Pre-calculated logits for each prediction step showing how context improves predictions\n",
    "    prediction_steps = [\n",
    "        {\n",
    "            \"step\": 1,\n",
    "            \"context\": \"<BOS>\",\n",
    "            \"target\": \"we\",\n",
    "            \"logits\": {\n",
    "                \"we\": 2.1,\n",
    "                \"love\": -0.5,\n",
    "                \"deep\": -1.2,\n",
    "                \"learning\": -2.0,\n",
    "                \"the\": 3.2,\n",
    "                \"hello\": 2.8,\n",
    "                \"is\": 1.5,\n",
    "                \"<EOS>\": -10.0,\n",
    "            },\n",
    "            \"explanation\": \"'we' is a common sentence starter, though 'the' is even more common in general text\",\n",
    "        },\n",
    "        {\n",
    "            \"step\": 2,\n",
    "            \"context\": \"we\",\n",
    "            \"target\": \"love\",\n",
    "            \"logits\": {\n",
    "                \"we\": -3.0,\n",
    "                \"love\": 2.8,\n",
    "                \"deep\": -0.5,\n",
    "                \"learning\": -1.5,\n",
    "                \"the\": 1.2,\n",
    "                \"hello\": -5.0,\n",
    "                \"is\": 2.1,\n",
    "                \"<EOS>\": -8.0,\n",
    "            },\n",
    "            \"explanation\": \"After 'we', action words like 'love', 'are', 'have' become much more likely than nouns\",\n",
    "        },\n",
    "        {\n",
    "            \"step\": 3,\n",
    "            \"context\": \"we love\",\n",
    "            \"target\": \"deep\",\n",
    "            \"logits\": {\n",
    "                \"we\": -5.0,\n",
    "                \"love\": -4.0,\n",
    "                \"deep\": 1.9,\n",
    "                \"learning\": 0.8,\n",
    "                \"the\": 0.5,\n",
    "                \"hello\": -6.0,\n",
    "                \"is\": -2.0,\n",
    "                \"<EOS>\": -7.0,\n",
    "            },\n",
    "            \"explanation\": \"After 'we love', we expect objects or concepts; 'deep' scores well as it often precedes 'learning'\",\n",
    "        },\n",
    "        {\n",
    "            \"step\": 4,\n",
    "            \"context\": \"we love deep\",\n",
    "            \"target\": \"learning\",\n",
    "            \"logits\": {\n",
    "                \"we\": -6.0,\n",
    "                \"love\": -5.0,\n",
    "                \"deep\": -4.0,\n",
    "                \"learning\": 3.5,\n",
    "                \"the\": -2.0,\n",
    "                \"hello\": -8.0,\n",
    "                \"is\": -3.0,\n",
    "                \"<EOS>\": -6.0,\n",
    "            },\n",
    "            \"explanation\": \"'deep learning' is a common collocation - 'learning' becomes very likely after 'deep' in this context\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    all_step_probabilities = []\n",
    "\n",
    "    for step_info in prediction_steps:\n",
    "        print(\n",
    "            f\"\\nSTEP {step_info['step']}: Context = '{step_info['context']}' -> Predicting '{step_info['target']}'\"\n",
    "        )\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # Show top 5 logits\n",
    "        sorted_logits = sorted(\n",
    "            step_info[\"logits\"].items(), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        print(\"Top 5 logit scores:\")\n",
    "        for i, (word, logit) in enumerate(sorted_logits[:5]):\n",
    "            marker = \" ** TARGET **\" if word == step_info[\"target\"] else \"\"\n",
    "            print(f\"  {i + 1}. {word:10}: {logit:6.1f}{marker}\")\n",
    "\n",
    "        # Calculate probability for target word\n",
    "        target_logit = step_info[\"logits\"][step_info[\"target\"]]\n",
    "        exp_values = [math.exp(logit) for logit in step_info[\"logits\"].values()]\n",
    "        total_exp = sum(exp_values)\n",
    "        target_prob = math.exp(target_logit) / total_exp\n",
    "        all_step_probabilities.append(target_prob)\n",
    "\n",
    "        print(f\"\\nTarget word '{step_info['target']}' probability: {target_prob:.3f}\")\n",
    "        print(f\"Context effect: {step_info['explanation']}\")\n",
    "\n",
    "    return prediction_steps, all_step_probabilities\n",
    "\n",
    "\n",
    "# Generate predictions for the complete sequence\n",
    "steps_data, step_probabilities = demonstrate_sequence_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function: Quantifying Prediction Quality\n",
    "\n",
    "The **loss function** is how we measure how well our model is performing. Specifically, it measures how \"surprised\" the model is when it sees the correct answer. The mathematical formulation we use is called **cross-entropy loss** or **log-likelihood loss**.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- Lower loss = better predictions = less surprise when seeing the correct answer\n",
    "- Higher loss = worse predictions = more surprise when seeing the correct answer\n",
    "- Loss is calculated as: Loss = -log(probability of correct word)\n",
    "- Training aims to minimize the total loss across all predictions\n",
    "\n",
    "The logarithm has a **useful property** when used in the loss function: it heavily penalizes very low probabilities. If the model assigns a probability of 0.01 to the correct word, the loss is much higher than if it assigns 0.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding what the Training Dataset is and its implications\n",
    "\n",
    "Before we dive into the mathematical details, it's crucial to understand what the loss function is actually measuring and why it works. The loss function compares our model's predictions against what we call the **\"ground truth\"** - the training dataset that contains the correct answers.\n",
    "\n",
    "**Training Dataset as Golden Labels:**\n",
    "\n",
    "When we train a language model, we provide it with a massive dataset of text (books, articles, websites, etc.). This dataset represents our best approximation of \"correct\" language use. Each sequence in this dataset serves as:\n",
    "\n",
    "- **Ground Truth**: The actual words that should come next in real language\n",
    "- **Golden Labels**: The target answers our model should learn to predict\n",
    "- **Reference Distribution**: The pattern of language we want our model to internalize\n",
    "\n",
    "For our example sequence \"we love deep learning\", this represents a small piece of ground truth data. When we calculate loss, we're measuring how well our model's predictions align with this established pattern of language.\n",
    "\n",
    "**From Random Weights to Meaningful Predictions:**\n",
    "\n",
    "Remember our initial model parameters from Section 1? Those weights were randomly initialized:\n",
    "\n",
    "```python\n",
    "model_parameters = {\n",
    "    'layer1_weights': [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, ...],\n",
    "    'layer2_weights': [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, ...],\n",
    "    'output_weights': [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, ...]\n",
    "}\n",
    "```\n",
    "\n",
    "These random numbers produce the predictions we saw in previous sections. The model doesn't \"know\" anything about language initially - its probabilities for predicting \"we\" (0.149) or \"love\" (0.571) come purely from these random mathematical operations.\n",
    "\n",
    "**The Training Objective: Fitting to the Dataset Distribution**\n",
    "\n",
    "The fundamental goal of training is to **fit the model to the training dataset's distribution**. This means:\n",
    "\n",
    "1. **Statistical Matching**: Adjust the weights so the model's predicted probabilities match the frequency patterns in the training data\n",
    "2. **Contextual Learning**: Teach the model that certain word combinations (like \"deep learning\") are more common than others\n",
    "3. **Pattern Internalization**: Embed the statistical regularities of human language into the neural network weights\n",
    "\n",
    "When we minimize the cross-entropy loss across millions of training examples, we're essentially forcing the model to internalize the probability distribution of the training dataset. The model learns that \"the\" is more common after \"<BOS>\" because it appears frequently in training data, not because it has any inherent understanding of language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inductive Bias: The Hidden Consequence of Training Data**\n",
    "\n",
    "This training process creates what we call an **inductive bias** - a set of assumptions and tendencies that the model develops based on its training data. Some also refer to this as the \"priors\" of the model. This bias manifests in several ways:\n",
    "\n",
    "1. **Domain Bias**: If trained on academic papers, the model will favor formal language\n",
    "2. **Cultural Bias**: Training data reflects the cultural context of its sources\n",
    "3. **Temporal Bias**: Models reflect the time period when their training data was collected\n",
    "4. **Statistical Bias**: Common patterns in training data become \"preferred\" by the model\n",
    "\n",
    "Our model's tendency to predict \"the\" with high probability after \"a BOS token\" isn't based on understanding - it's an inductive bias learned from statistical patterns in training data where \"the\" frequently starts sentences.\n",
    "\n",
    "**The Philosophical Question of Language Modeling:**\n",
    "\n",
    "We use human-written text to train models to predict human-like text. The model doesn't learn \"language\" in an abstract sense explicitly - it learns to mimic the specific patterns present in its training distribution. This is why the choice of training data is so critical and why different models can have vastly different behaviors based on their training datasets.\n",
    "\n",
    "The loss function, therefore, isn't just measuring prediction accuracy - it's measuring how well the model has absorbed and can reproduce the inductive biases present in its training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mathematical Foundation: Next Token Prediction Loss\n",
    "\n",
    "Now let's dive deep into the precise mathematical formulation used in Large Language Models. The loss function we use is called **Negative Log-Likelihood (NLL)** for **Next Token Prediction (NTP)**.\n",
    "\n",
    "#### The Complete NLL Formula\n",
    "\n",
    "For **Next Token Prediction** in Large Language Models, the negative log-likelihood formula is:\n",
    "\n",
    "**NLL = -∑ᵢ₌₁ⁿ log P(xᵢ | x₁, x₂, ..., xᵢ₋₁; θ)**\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- **xᵢ** = the i-th token in the sequence (e.g., \"we\", \"love\", \"deep\", \"learning\")\n",
    "- **x₁, x₂, ..., xᵢ₋₁** = all previous tokens providing context\n",
    "- **θ** = model parameters (our weights from Section 1)\n",
    "- **n** = sequence length (4 tokens in our example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking Down Each Component of the NLL Formula\n",
    "\n",
    "**1. Per-Token Probability Computation:**\n",
    "\n",
    "The probability of each token given its context is computed using softmax:\n",
    "\n",
    "```\n",
    "P(xᵢ | context) = softmax(logits)ₓᵢ = exp(zₓᵢ) / ∑ⱼ₌₁ᵛ exp(zⱼ)\n",
    "```\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- **zₓᵢ** = logit score for token xᵢ (like our 2.1 for \"we\")\n",
    "- **V** = vocabulary size (12 in our case)\n",
    "- **j** indexes over entire vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Averaging Over the Dataset:**\n",
    "\n",
    "In practice, the NLL loss is computed for every token in every sequence in the dataset. The total loss is then averaged to provide a single scalar value for optimization and monitoring:\n",
    "\n",
    "**NLL = - (1/N) ∑{i=1}^N log P(xᵢ | context)**\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- **N** = total number of tokens (or sequences) in the dataset.\n",
    "- This averaging ensures the loss is independent of dataset size and comparable across different runs or datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregressive Nature\n",
    "\n",
    "The key insight is that each prediction depends on **all previous tokens**:\n",
    "\n",
    "- **Step 1**: P(\"we\" | \"<BOS>\") - no previous content words\n",
    "- **Step 2**: P(\"love\" | \"we\") - uses \"we\" as context\n",
    "- **Step 3**: P(\"deep\" | \"we\", \"love\") - uses both previous tokens\n",
    "- **Step 4**: P(\"learning\" | \"we\", \"love\", \"deep\") - uses all three previous tokens\n",
    "\n",
    "This is why our predictions improved with more context in Section 4 - the model had more information to work with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "### Example (from your notebook):\n",
    "\n",
    "Suppose for the sequence `\"we love deep learning\"`:\n",
    "\n",
    "- For **\"we\"** (after `<BOS>`):  \n",
    "  P(\"we\" | <BOS>) = 0.149  \n",
    "  **Loss:**  \n",
    "  -log(0.149) = 1.904\n",
    "\n",
    "- For **\"love\"** (after \"we\"):  \n",
    "  P(\"love\" | \"we\") = 0.571  \n",
    "  **Loss:**  \n",
    "  -log(0.571) = 0.560\n",
    "\n",
    "- For **\"deep\"** (after \"we\", \"love\"):  \n",
    "  P(\"deep\" | \"we\", \"love\") = 0.623  \n",
    "  **Loss:**  \n",
    "  -log(0.623) = 0.473\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'we': P = 0.149  ->  Loss = -log(0.149) = 1.904\n",
      "For 'love': P = 0.571  ->  Loss = -log(0.571) = 0.560\n",
      "For 'deep': P = 0.623  ->  Loss = -log(0.623) = 0.473\n",
      "For 'learning': P = 0.994  ->  Loss = -log(0.994) = 0.006\n",
      "\n",
      "Average NLL loss: 0.736\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "words = [\"we\", \"love\", \"deep\", \"learning\"]\n",
    "probs = [0.149, 0.571, 0.623, 0.994]\n",
    "losses = []\n",
    "\n",
    "for word, prob in zip(words, probs):\n",
    "    loss = -math.log(prob)\n",
    "    losses.append(loss)\n",
    "    print(f\"For '{word}': P = {prob}  ->  Loss = -log({prob}) = {loss:.3f}\")\n",
    "\n",
    "nll = sum(losses) / len(losses) # usually reported as the average of the losses, but could differ sometimes\n",
    "print(f\"\\nAverage NLL loss: {nll:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to Perplexity\n",
    "\n",
    "The loss is often reported as **perplexity**:\n",
    "\n",
    "```\n",
    "Perplexity = exp(NLL/n)\n",
    "```\n",
    "\n",
    "Lower perplexity indicates better language modeling performance.\n",
    "\n",
    "These mathematical foundations capture the core training objective of all Large Language Models: **maximize the probability of the correct next token given the context**. Every word prediction in ChatGPT, GPT-4, or any other LLM follows this exact mathematical framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL loss: 0.736\n",
      "Perplexity: 2.087\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "probs = [0.149, 0.571, 0.623, 0.994]\n",
    "losses = [-math.log(p) for p in probs]\n",
    "nll = sum(losses) / len(losses)\n",
    "perplexity = math.exp(nll)\n",
    "\n",
    "print(f\"Average NLL loss: {nll:.3f}\")\n",
    "print(f\"Perplexity: {perplexity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current total loss: 2.943401874883052\n"
     ]
    }
   ],
   "source": [
    "print(\"Current total loss:\", sum(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def demonstrate_cross_entropy_formula():\n",
    "#     print(\"CROSS-ENTROPY LOSS FORMULA: Step-by-Step Breakdown\")\n",
    "#     print(\"=\" * 60)\n",
    "\n",
    "#     print(\"The cross-entropy loss formula is: Loss = -log(P(correct_word))\")\n",
    "#     print(\"Let's break this down with a concrete example:\\n\")\n",
    "\n",
    "#     # Use a simple toy example\n",
    "#     print(\"TOY EXAMPLE: Predicting 'cat' from vocabulary ['dog', 'cat', 'bird']\")\n",
    "#     print(\"-\" * 65)\n",
    "\n",
    "#     # Toy probabilities for demonstration\n",
    "#     toy_probabilities = {\n",
    "#         \"dog\": 0.2,\n",
    "#         \"cat\": 0.6,  # correct word\n",
    "#         \"bird\": 0.2,\n",
    "#     }\n",
    "\n",
    "#     print(\"Model predictions (probabilities):\")\n",
    "#     for word, prob in toy_probabilities.items():\n",
    "#         marker = \" <- CORRECT ANSWER\" if word == \"cat\" else \"\"\n",
    "#         print(f\"  P({word}) = {prob:.1f}{marker}\")\n",
    "\n",
    "#     print(\n",
    "#         f\"\\nVerification: {sum(toy_probabilities.values()):.1f} (probabilities sum to 1.0)\"\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\nSTEP-BY-STEP LOSS CALCULATION:\")\n",
    "#     print(\"-\" * 35)\n",
    "\n",
    "#     correct_word = \"cat\"\n",
    "#     correct_prob = toy_probabilities[correct_word]\n",
    "\n",
    "#     print(f\"Step 1: Identify the correct word\")\n",
    "#     print(f\"        Correct word = '{correct_word}'\")\n",
    "\n",
    "#     print(f\"\\nStep 2: Find its probability\")\n",
    "#     print(f\"        P({correct_word}) = {correct_prob}\")\n",
    "\n",
    "#     print(f\"\\nStep 3: Take the natural logarithm\")\n",
    "#     log_prob = math.log(correct_prob)\n",
    "#     print(f\"        log(P({correct_word})) = log({correct_prob}) = {log_prob:.4f}\")\n",
    "\n",
    "#     print(f\"\\nStep 4: Apply the negative sign\")\n",
    "#     loss = -log_prob\n",
    "#     print(f\"        Loss = -log(P({correct_word})) = -({log_prob:.4f}) = {loss:.4f}\")\n",
    "\n",
    "#     print(f\"\\nFINAL RESULT: Loss = {loss:.4f}\")\n",
    "\n",
    "#     # Show why we use negative log\n",
    "#     print(f\"\\n\" + \"=\" * 50)\n",
    "#     print(\"WHY NEGATIVE LOGARITHM?\")\n",
    "#     print(\"=\" * 50)\n",
    "\n",
    "#     prob_examples = [0.9, 0.5, 0.1, 0.01]\n",
    "#     print(\"Probability -> log(prob) -> -log(prob) (Loss)\")\n",
    "#     print(\"-\" * 45)\n",
    "\n",
    "#     for prob in prob_examples:\n",
    "#         log_val = math.log(prob)\n",
    "#         loss_val = -log_val\n",
    "#         quality = (\n",
    "#             \"Excellent\"\n",
    "#             if prob > 0.8\n",
    "#             else \"Good\"\n",
    "#             if prob > 0.4\n",
    "#             else \"Poor\"\n",
    "#             if prob > 0.05\n",
    "#             else \"Terrible\"\n",
    "#         )\n",
    "#         print(\n",
    "#             f\"   {prob:4.2f}    ->   {log_val:6.2f}   ->   {loss_val:6.2f}  ({quality})\"\n",
    "#         )\n",
    "\n",
    "#     print(f\"\\nKey insight: Higher probability -> Lower loss (what we want!)\")\n",
    "#     print(f\"            Lower probability -> Higher loss (penalty for bad predictions)\")\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def demonstrate_loss_calculation():\n",
    "#     print(\"\\n\" + \"=\" * 70)\n",
    "#     print(\"APPLYING LOSS TO OUR SEQUENCE: 'we love deep learning'\")\n",
    "#     print(\"=\" * 70)\n",
    "\n",
    "#     print(\"Now let's calculate loss for each word in our target sequence:\")\n",
    "#     print(\"Formula: Loss = -log(P(correct_word))\")\n",
    "#     print()\n",
    "\n",
    "#     words = [\"we\", \"love\", \"deep\", \"learning\"]\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for i, (word, prob) in enumerate(zip(words, step_probabilities)):\n",
    "#         print(f\"STEP {i + 1}: Predicting '{word}'\")\n",
    "#         print(\"-\" * 25)\n",
    "#         print(f\"  Model prediction: P({word}) = {prob:.3f}\")\n",
    "#         print(f\"  Take logarithm:   log({prob:.3f}) = {math.log(prob):.4f}\")\n",
    "\n",
    "#         loss = -math.log(prob)\n",
    "#         total_loss += loss\n",
    "\n",
    "#         print(f\"  Apply negative:   Loss = -({math.log(prob):.4f}) = {loss:.4f}\")\n",
    "\n",
    "#         # Interpret the loss\n",
    "#         if loss < 0.5:\n",
    "#             interpretation = \"Excellent prediction!\"\n",
    "#         elif loss < 1.0:\n",
    "#             interpretation = \"Good prediction\"\n",
    "#         elif loss < 2.0:\n",
    "#             interpretation = \"Decent prediction\"\n",
    "#         else:\n",
    "#             interpretation = \"Poor prediction - needs improvement\"\n",
    "\n",
    "#         print(f\"  Interpretation:   {interpretation}\")\n",
    "#         print()\n",
    "\n",
    "#     print(f\"TOTAL SEQUENCE LOSS:\")\n",
    "#     print(\"-\" * 20)\n",
    "#     print(f\"Sum of all losses = {total_loss:.4f}\")\n",
    "#     print(f\"Average per word  = {total_loss / len(words):.4f}\")\n",
    "#     print(\n",
    "#         f\"\\nTraining goal: Reduce this total loss by improving individual word predictions!\"\n",
    "#     )\n",
    "\n",
    "#     return total_loss\n",
    "\n",
    "\n",
    "# # First demonstrate the formula with toy example\n",
    "# toy_loss = demonstrate_cross_entropy_formula()\n",
    "\n",
    "# # Then apply to our actual sequence\n",
    "# sequence_loss = demonstrate_loss_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation: Finding What Needs to Change\n",
    "\n",
    "Now that we understand how to measure our model's performance with the loss function, we need to figure out **which model parameters should be adjusted and by how much** to improve our predictions. This process is called **backpropagation** - it literally means \"propagating the error backwards\" through the neural network.\n",
    "\n",
    "**The Central Question:** Our total loss is 2.94. We want to reduce this to improve our predictions. But our model has thousands or millions of parameters. Which ones should we change? And in which direction (increase or decrease)?\n",
    "\n",
    "**The Answer:** Backpropagation calculates the **gradient** - the mathematical derivative of the loss with respect to each parameter. The gradient tells us:\n",
    "\n",
    "1. **Direction**: Should we increase or decrease each parameter?\n",
    "2. **Magnitude**: How much impact will changing each parameter have on the loss?\n",
    "\n",
    "### The Mathematical Foundation: The Chain Rule\n",
    "\n",
    "Backpropagation is essentially a systematic application of the **chain rule** from calculus. The chain rule tells us how to compute the derivative of composite functions.\n",
    "\n",
    "**For our model:** Loss = f(probabilities) = f(softmax(logits)) = f(softmax(neural_network(parameters)))\n",
    "\n",
    "To find how the loss changes with respect to any parameter, we need to trace the path backwards:\n",
    "\n",
    "```\n",
    "∂Loss/∂parameter = ∂Loss/∂probabilities × ∂probabilities/∂logits × ∂logits/∂parameter\n",
    "```\n",
    "\n",
    "This \"chaining\" of derivatives is what gives backpropagation its name - we start from the loss and work backwards through each layer.\n",
    "\n",
    "For this course, we will not go through the mathematical details of backpropagation. Instead we will use example static values to help us understand the core concept of this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Gradients: Direction and Magnitude\n",
    "\n",
    "Let's explore what gradients actually represent with concrete examples from our model.\n",
    "\n",
    "**Key Gradient Properties:**\n",
    "\n",
    "- **Positive gradient**: Increasing this parameter increases the loss (bad direction)\n",
    "- **Negative gradient**: Increasing this parameter decreases the loss (good direction)\n",
    "- **Large gradient magnitude**: This parameter has high impact on loss\n",
    "- **Small gradient magnitude**: This parameter has low impact on loss\n",
    "\n",
    "The gradient essentially answers: \"If I increase this parameter by a tiny amount, how much will the loss change?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT CONCEPT: What Does a Gradient Tell Us?\n",
      "=======================================================\n",
      "Imagine we have a simple parameter that affects our model's prediction for 'we'\n",
      "Let's call this parameter 'weight_for_we' and see how changing it affects loss\n",
      "\n",
      "CURRENT SITUATION:\n",
      "  weight_for_we = 0.5\n",
      "  P(we) = 0.149\n",
      "  Loss for 'we' = -log(0.149) = 1.9038\n",
      "\n",
      "TESTING SMALL CHANGES (this is what gradients approximate):\n",
      "----------------------------------------------------------\n",
      "\n",
      "DECREASE weight by 0.010:\n",
      "  New weight: 0.5 -> 0.49\n",
      "  New P(we): 0.149 -> 0.146\n",
      "  New loss: 1.9038 -> 1.9240\n",
      "  Loss change: +0.0202\n",
      "  Result: LOSS INCREASED - this is a BAD direction!\n",
      "\n",
      "INCREASE weight by 0.010:\n",
      "  New weight: 0.5 -> 0.51\n",
      "  New P(we): 0.149 -> 0.152\n",
      "  New loss: 1.9038 -> 1.8840\n",
      "  Loss change: -0.0198\n",
      "  Result: LOSS DECREASED - this is a GOOD direction!\n",
      "\n",
      "==================================================\n",
      "GRADIENT CALCULATION:\n",
      "==================================================\n",
      "Approximate gradient = (loss_change) / (weight_change)\n",
      "                    ≈ -2.00\n",
      "\n",
      "INTERPRETATION: Negative gradient means:\n",
      "  - Increasing this weight DECREASES loss (good)\n",
      "  - Decreasing this weight INCREASES loss (bad)\n",
      "  - We should INCREASE this weight during training\n",
      "\n",
      "======================================================================\n",
      "BACKPROPAGATION PROCESS: Step-by-Step Through Our Network\n",
      "======================================================================\n",
      "Let's trace how gradients flow backwards through our model\n",
      "for the first prediction: '<BOS>' -> 'we'\n",
      "\n",
      "STEP 1: Start with the loss\n",
      "------------------------------\n",
      "Loss = -log(P(we)) = -log(0.149) = 1.9038\n",
      "This is our starting point - we want to reduce this loss\n",
      "\n",
      "STEP 2: Gradient with respect to logits\n",
      "------------------------------------------\n",
      "For softmax + cross-entropy, the gradient has a beautiful form:\n",
      "\n",
      "∂Loss/∂logit_i = P(word_i) - target_i\n",
      "\n",
      "Where target_i = 1 if word_i is correct, 0 otherwise\n",
      "\n",
      "For our prediction '<BOS>' -> 'we':\n",
      "  ∂Loss/∂logit_we       = 0.149 - 1 = -0.851 <- TARGET WORD\n",
      "  ∂Loss/∂logit_love     = 0.011 - 0 = +0.011\n",
      "  ∂Loss/∂logit_the      = 0.449 - 0 = +0.449\n",
      "  ∂Loss/∂logit_hello    = 0.301 - 0 = +0.301\n",
      "  ∂Loss/∂logit_is       = 0.082 - 0 = +0.082\n",
      "\n",
      "Key insights:\n",
      "- Target word 'we': gradient = 0.149 - 1 = -0.851 (NEGATIVE)\n",
      "  This means increasing logit_we will DECREASE loss (good!)\n",
      "- Other words: positive gradients\n",
      "  This means increasing their logits will INCREASE loss (bad!)\n",
      "\n",
      "STEP 3: Gradients flow to network weights\n",
      "----------------------------------------\n",
      "The gradients we calculated above now flow backwards to:\n",
      "- Output layer weights (that produce logits)\n",
      "- Hidden layer weights\n",
      "- Input embedding weights\n",
      "\n",
      "Each weight's gradient tells us:\n",
      "- How much that weight contributed to the current loss\n",
      "- Which direction to adjust it to improve predictions\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_gradient_concept():\n",
    "    print(\"GRADIENT CONCEPT: What Does a Gradient Tell Us?\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    print(\n",
    "        \"Imagine we have a simple parameter that affects our model's prediction for 'we'\"\n",
    "    )\n",
    "    print(\n",
    "        \"Let's call this parameter 'weight_for_we' and see how changing it affects loss\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Current situation\n",
    "    current_weight = 0.5\n",
    "    current_prob_we = 0.149  # From our earlier calculation\n",
    "    current_loss_we = -math.log(current_prob_we)\n",
    "\n",
    "    print(f\"CURRENT SITUATION:\")\n",
    "    print(f\"  weight_for_we = {current_weight}\")\n",
    "    print(f\"  P(we) = {current_prob_we:.3f}\")\n",
    "    print(f\"  Loss for 'we' = -log({current_prob_we:.3f}) = {current_loss_we:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Test small changes\n",
    "    print(\"TESTING SMALL CHANGES (this is what gradients approximate):\")\n",
    "    print(\"-\" * 58)\n",
    "\n",
    "    small_change = 0.01\n",
    "    weight_changes = [-small_change, small_change]\n",
    "\n",
    "    for i, change in enumerate(weight_changes):\n",
    "        new_weight = current_weight + change\n",
    "\n",
    "        # Simulate how the probability might change (simplified)\n",
    "        # In reality, this involves complex neural network forward pass\n",
    "        prob_change_factor = 1 + change * 2  # Simplified relationship\n",
    "        new_prob_we = min(current_prob_we * prob_change_factor, 0.999)\n",
    "        new_loss_we = -math.log(new_prob_we)\n",
    "\n",
    "        loss_change = new_loss_we - current_loss_we\n",
    "\n",
    "        direction = \"DECREASE\" if change < 0 else \"INCREASE\"\n",
    "        print(f\"\\n{direction} weight by {abs(change):.3f}:\")\n",
    "        print(f\"  New weight: {current_weight} -> {new_weight}\")\n",
    "        print(f\"  New P(we): {current_prob_we:.3f} -> {new_prob_we:.3f}\")\n",
    "        print(f\"  New loss: {current_loss_we:.4f} -> {new_loss_we:.4f}\")\n",
    "        print(f\"  Loss change: {loss_change:+.4f}\")\n",
    "\n",
    "        if loss_change < 0:\n",
    "            print(f\"  Result: LOSS DECREASED - this is a GOOD direction!\")\n",
    "        else:\n",
    "            print(f\"  Result: LOSS INCREASED - this is a BAD direction!\")\n",
    "\n",
    "    # Calculate approximate gradient\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"GRADIENT CALCULATION:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Approximate gradient using finite differences\n",
    "    loss_at_plus = -math.log(min(current_prob_we * (1 + small_change * 2), 0.999))\n",
    "    loss_at_minus = -math.log(current_prob_we * (1 - small_change * 2))\n",
    "    approximate_gradient = (loss_at_plus - loss_at_minus) / (2 * small_change)\n",
    "\n",
    "    print(f\"Approximate gradient = (loss_change) / (weight_change)\")\n",
    "    print(f\"                    ≈ {approximate_gradient:.2f}\")\n",
    "    print()\n",
    "\n",
    "    if approximate_gradient > 0:\n",
    "        print(\"INTERPRETATION: Positive gradient means:\")\n",
    "        print(\"  - Increasing this weight INCREASES loss (bad)\")\n",
    "        print(\"  - Decreasing this weight DECREASES loss (good)\")\n",
    "        print(\"  - We should DECREASE this weight during training\")\n",
    "    else:\n",
    "        print(\"INTERPRETATION: Negative gradient means:\")\n",
    "        print(\"  - Increasing this weight DECREASES loss (good)\")\n",
    "        print(\"  - Decreasing this weight INCREASES loss (bad)\")\n",
    "        print(\"  - We should INCREASE this weight during training\")\n",
    "\n",
    "    return approximate_gradient\n",
    "\n",
    "\n",
    "def demonstrate_backprop_process():\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BACKPROPAGATION PROCESS: Step-by-Step Through Our Network\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"Let's trace how gradients flow backwards through our model\")\n",
    "    print(\"for the first prediction: '<BOS>' -> 'we'\")\n",
    "    print()\n",
    "\n",
    "    # Step 1: Loss gradient\n",
    "    print(\"STEP 1: Start with the loss\")\n",
    "    print(\"-\" * 30)\n",
    "    target_prob = 0.149  # P(we) from our calculation\n",
    "    loss = -math.log(target_prob)\n",
    "    print(f\"Loss = -log(P(we)) = -log({target_prob:.3f}) = {loss:.4f}\")\n",
    "    print(f\"This is our starting point - we want to reduce this loss\")\n",
    "    print()\n",
    "\n",
    "    # Step 2: Gradient with respect to logits\n",
    "    print(\"STEP 2: Gradient with respect to logits\")\n",
    "    print(\"-\" * 42)\n",
    "    print(\"For softmax + cross-entropy, the gradient has a beautiful form:\")\n",
    "    print()\n",
    "    print(\"∂Loss/∂logit_i = P(word_i) - target_i\")\n",
    "    print()\n",
    "    print(\"Where target_i = 1 if word_i is correct, 0 otherwise\")\n",
    "    print()\n",
    "\n",
    "    # Show gradients for each word in vocabulary\n",
    "    vocab_subset = [\"we\", \"love\", \"the\", \"hello\", \"is\"]\n",
    "    probs_step1 = {\n",
    "        \"we\": 0.149,\n",
    "        \"love\": 0.011,\n",
    "        \"the\": 0.449,\n",
    "        \"hello\": 0.301,\n",
    "        \"is\": 0.082,\n",
    "    }\n",
    "\n",
    "    print(\"For our prediction '<BOS>' -> 'we':\")\n",
    "    for word in vocab_subset:\n",
    "        prob = probs_step1[word]\n",
    "        target = 1 if word == \"we\" else 0\n",
    "        gradient = prob - target\n",
    "\n",
    "        marker = \" <- TARGET WORD\" if word == \"we\" else \"\"\n",
    "        print(\n",
    "            f\"  ∂Loss/∂logit_{word:8} = {prob:.3f} - {target} = {gradient:+.3f}{marker}\"\n",
    "        )\n",
    "\n",
    "    print()\n",
    "    print(\"Key insights:\")\n",
    "    print(\"- Target word 'we': gradient = 0.149 - 1 = -0.851 (NEGATIVE)\")\n",
    "    print(\"  This means increasing logit_we will DECREASE loss (good!)\")\n",
    "    print(\"- Other words: positive gradients\")\n",
    "    print(\"  This means increasing their logits will INCREASE loss (bad!)\")\n",
    "\n",
    "    # Step 3: Gradient flows to earlier layers\n",
    "    print(f\"\\nSTEP 3: Gradients flow to network weights\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"The gradients we calculated above now flow backwards to:\")\n",
    "    print(\"- Output layer weights (that produce logits)\")\n",
    "    print(\"- Hidden layer weights\")\n",
    "    print(\"- Input embedding weights\")\n",
    "    print()\n",
    "    print(\"Each weight's gradient tells us:\")\n",
    "    print(\"- How much that weight contributed to the current loss\")\n",
    "    print(\"- Which direction to adjust it to improve predictions\")\n",
    "\n",
    "    return {\"target_logit_gradient\": -0.851}\n",
    "\n",
    "\n",
    "# Run the demonstrations\n",
    "gradient_example = demonstrate_gradient_concept()\n",
    "backprop_results = demonstrate_backprop_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Full Backpropagation Picture\n",
    "\n",
    "Let's put together the complete picture of how backpropagation works in our language model by examining what happens when we process our entire sequence \"we love deep learning\".\n",
    "\n",
    "**The Process:**\n",
    "\n",
    "1. **Forward Pass**: Compute predictions and loss for each word\n",
    "2. **Backward Pass**: Calculate gradients for each parameter\n",
    "3. **Accumulate Gradients**: Sum gradients from all prediction steps\n",
    "4. **Parameter Updates**: Use gradients to improve model weights\n",
    "\n",
    "This happens for every training example, and the gradients tell us exactly how to adjust our model to make better predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE BACKPROPAGATION: All Four Prediction Steps\n",
      "==========================================================\n",
      "For each prediction step, backpropagation calculates:\n",
      "- How much each parameter contributed to the loss\n",
      "- Which direction to adjust each parameter\n",
      "\n",
      "STEP 1: '<BOS>' -> 'we'\n",
      "--------------------------------------------------\n",
      "  Forward pass:  P(we) = 0.149\n",
      "  Loss:          1.9040\n",
      "  Logit gradient: -0.851\n",
      "  Interpretation: High priority - adjust weights to boost 'we'\n",
      "\n",
      "STEP 2: 'we' -> 'love'\n",
      "--------------------------------------------------\n",
      "  Forward pass:  P(love) = 0.571\n",
      "  Loss:          0.5600\n",
      "  Logit gradient: -0.429\n",
      "  Interpretation: Medium priority - adjust weights to boost 'love'\n",
      "\n",
      "STEP 3: 'we love' -> 'deep'\n",
      "--------------------------------------------------\n",
      "  Forward pass:  P(deep) = 0.623\n",
      "  Loss:          0.4730\n",
      "  Logit gradient: -0.377\n",
      "  Interpretation: Medium priority - adjust weights to boost 'deep'\n",
      "\n",
      "STEP 4: 'we love deep' -> 'learning'\n",
      "--------------------------------------------------\n",
      "  Forward pass:  P(learning) = 0.994\n",
      "  Loss:          0.0060\n",
      "  Logit gradient: -0.006\n",
      "  Interpretation: Low priority - adjust weights to boost 'learning'\n",
      "\n",
      "GRADIENT ACCUMULATION:\n",
      "-------------------------\n",
      "In a real training step, we would:\n",
      "1. Sum up all the gradients from each prediction step\n",
      "2. Apply these combined gradients to update model parameters\n",
      "3. The model gets slightly better at each prediction\n",
      "\n",
      "Total loss across all steps: 2.9430\n",
      "This is what we're trying to minimize through training!\n",
      "\n",
      "============================================================\n",
      "IMPACT ON MODEL PARAMETERS:\n",
      "============================================================\n",
      "Our original parameters (from Section 1):\n",
      "  layer1_weights: [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, ...]\n",
      "  layer2_weights: [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, ...]\n",
      "  output_weights: [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, ...]\n",
      "\n",
      "After backpropagation, each weight gets a gradient:\n",
      "  gradient_layer1: [-0.05, 0.12, -0.08, 0.03, 0.07, -0.15, ...]\n",
      "  gradient_layer2: [0.09, -0.04, 0.11, -0.18, 0.06, 0.08, ...]\n",
      "  gradient_output: [-0.21, 0.15, -0.12, 0.09, -0.17, 0.11, ...]\n",
      "\n",
      "These gradients tell us:\n",
      "- Negative gradient: Increase this weight to reduce loss\n",
      "- Positive gradient: Decrease this weight to reduce loss\n",
      "- Large magnitude: This weight has high impact\n",
      "- Small magnitude: This weight has low impact\n",
      "\n",
      "=================================================================\n",
      "GRADIENT FLOW: How Information Travels Backwards\n",
      "=================================================================\n",
      "Think of gradients as 'error signals' that flow backwards:\n",
      "\n",
      "FORWARD PASS (left to right):\n",
      "-----------------------------------\n",
      "→ INPUT: Token embeddings ('<BOS>', 'we', 'love', etc.)\n",
      "  → LAYER 1: First transformation (hidden layer)\n",
      "    → LAYER 2: Second transformation (hidden layer)\n",
      "      → OUTPUT: Logits for each vocabulary word\n",
      "        → SOFTMAX: Convert logits to probabilities\n",
      "          → LOSS: Compare with target word\n",
      "\n",
      "BACKWARD PASS (right to left - gradients flow backwards):\n",
      "------------------------------------------------------------\n",
      "← LAYER 1 GRADIENT: ∂Loss/∂layer1_weights\n",
      "  ← LAYER 2 GRADIENT: ∂Loss/∂layer2_weights\n",
      "    ← OUTPUT GRADIENT: ∂Loss/∂output_weights\n",
      "      ← LOGIT GRADIENT: ∂Loss/∂logits (we calculated these!)\n",
      "        ← SOFTMAX GRADIENT: ∂Loss/∂probabilities\n",
      "          ← LOSS GRADIENT: ∂Loss/∂Loss = 1 (starting point)\n",
      "\n",
      "Key insight: Each layer's gradient depends on the gradients\n",
      "from all layers that come after it. This is the 'chain rule' in action!\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_full_backpropagation():\n",
    "    print(\"COMPLETE BACKPROPAGATION: All Four Prediction Steps\")\n",
    "    print(\"=\" * 58)\n",
    "\n",
    "    # Data from our previous calculations\n",
    "    sequence_data = [\n",
    "        {\n",
    "            \"step\": 1,\n",
    "            \"context\": \"<BOS>\",\n",
    "            \"target\": \"we\",\n",
    "            \"probability\": 0.149,\n",
    "            \"loss\": 1.904,\n",
    "            \"logit_gradient\": -0.851,  # P(we) - 1 = 0.149 - 1\n",
    "        },\n",
    "        {\n",
    "            \"step\": 2,\n",
    "            \"context\": \"we\",\n",
    "            \"target\": \"love\",\n",
    "            \"probability\": 0.571,\n",
    "            \"loss\": 0.560,\n",
    "            \"logit_gradient\": -0.429,  # P(love) - 1 = 0.571 - 1\n",
    "        },\n",
    "        {\n",
    "            \"step\": 3,\n",
    "            \"context\": \"we love\",\n",
    "            \"target\": \"deep\",\n",
    "            \"probability\": 0.623,\n",
    "            \"loss\": 0.473,\n",
    "            \"logit_gradient\": -0.377,  # P(deep) - 1 = 0.623 - 1\n",
    "        },\n",
    "        {\n",
    "            \"step\": 4,\n",
    "            \"context\": \"we love deep\",\n",
    "            \"target\": \"learning\",\n",
    "            \"probability\": 0.994,\n",
    "            \"loss\": 0.006,\n",
    "            \"logit_gradient\": -0.006,  # P(learning) - 1 = 0.994 - 1\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    print(\"For each prediction step, backpropagation calculates:\")\n",
    "    print(\"- How much each parameter contributed to the loss\")\n",
    "    print(\"- Which direction to adjust each parameter\")\n",
    "    print()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in sequence_data:\n",
    "        print(f\"STEP {data['step']}: '{data['context']}' -> '{data['target']}'\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"  Forward pass:  P({data['target']}) = {data['probability']:.3f}\")\n",
    "        print(f\"  Loss:          {data['loss']:.4f}\")\n",
    "        print(f\"  Logit gradient: {data['logit_gradient']:.3f}\")\n",
    "\n",
    "        # Interpret the gradient\n",
    "        if abs(data[\"logit_gradient\"]) > 0.5:\n",
    "            urgency = \"High priority\"\n",
    "        elif abs(data[\"logit_gradient\"]) > 0.1:\n",
    "            urgency = \"Medium priority\"\n",
    "        else:\n",
    "            urgency = \"Low priority\"\n",
    "\n",
    "        print(\n",
    "            f\"  Interpretation: {urgency} - adjust weights to boost '{data['target']}'\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "        total_loss += data[\"loss\"]\n",
    "\n",
    "    print(\"GRADIENT ACCUMULATION:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"In a real training step, we would:\")\n",
    "    print(\"1. Sum up all the gradients from each prediction step\")\n",
    "    print(\"2. Apply these combined gradients to update model parameters\")\n",
    "    print(\"3. The model gets slightly better at each prediction\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Total loss across all steps: {total_loss:.4f}\")\n",
    "    print(f\"This is what we're trying to minimize through training!\")\n",
    "\n",
    "    # Show how gradients would affect our original parameters\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"IMPACT ON MODEL PARAMETERS:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"Our original parameters (from Section 1):\")\n",
    "    print(\"  layer1_weights: [0.1, -0.3, 0.5, 0.2, -0.1, 0.4, ...]\")\n",
    "    print(\"  layer2_weights: [0.2, 0.1, -0.4, 0.6, 0.3, -0.2, ...]\")\n",
    "    print(\"  output_weights: [0.3, -0.1, 0.4, -0.2, 0.5, 0.1, ...]\")\n",
    "    print()\n",
    "\n",
    "    print(\"After backpropagation, each weight gets a gradient:\")\n",
    "    print(\"  gradient_layer1: [-0.05, 0.12, -0.08, 0.03, 0.07, -0.15, ...]\")\n",
    "    print(\"  gradient_layer2: [0.09, -0.04, 0.11, -0.18, 0.06, 0.08, ...]\")\n",
    "    print(\"  gradient_output: [-0.21, 0.15, -0.12, 0.09, -0.17, 0.11, ...]\")\n",
    "    print()\n",
    "\n",
    "    print(\"These gradients tell us:\")\n",
    "    print(\"- Negative gradient: Increase this weight to reduce loss\")\n",
    "    print(\"- Positive gradient: Decrease this weight to reduce loss\")\n",
    "    print(\"- Large magnitude: This weight has high impact\")\n",
    "    print(\"- Small magnitude: This weight has low impact\")\n",
    "\n",
    "    return sequence_data\n",
    "\n",
    "\n",
    "def demonstrate_gradient_flow():\n",
    "    print(f\"\\n\" + \"=\" * 65)\n",
    "    print(\"GRADIENT FLOW: How Information Travels Backwards\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    print(\"Think of gradients as 'error signals' that flow backwards:\")\n",
    "    print()\n",
    "\n",
    "    network_layers = [\n",
    "        \"INPUT: Token embeddings ('<BOS>', 'we', 'love', etc.)\",\n",
    "        \"LAYER 1: First transformation (hidden layer)\",\n",
    "        \"LAYER 2: Second transformation (hidden layer)\",\n",
    "        \"OUTPUT: Logits for each vocabulary word\",\n",
    "        \"SOFTMAX: Convert logits to probabilities\",\n",
    "        \"LOSS: Compare with target word\",\n",
    "    ]\n",
    "\n",
    "    print(\"FORWARD PASS (left to right):\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, layer in enumerate(network_layers):\n",
    "        indent = \"  \" * i\n",
    "        print(f\"{indent}→ {layer}\")\n",
    "\n",
    "    print(f\"\\nBACKWARD PASS (right to left - gradients flow backwards):\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    gradient_flow = [\n",
    "        \"LOSS GRADIENT: ∂Loss/∂Loss = 1 (starting point)\",\n",
    "        \"SOFTMAX GRADIENT: ∂Loss/∂probabilities\",\n",
    "        \"LOGIT GRADIENT: ∂Loss/∂logits (we calculated these!)\",\n",
    "        \"OUTPUT GRADIENT: ∂Loss/∂output_weights\",\n",
    "        \"LAYER 2 GRADIENT: ∂Loss/∂layer2_weights\",\n",
    "        \"LAYER 1 GRADIENT: ∂Loss/∂layer1_weights\",\n",
    "    ]\n",
    "\n",
    "    for i, gradient in enumerate(reversed(gradient_flow)):\n",
    "        indent = \"  \" * i\n",
    "        print(f\"{indent}← {gradient}\")\n",
    "\n",
    "    print(f\"\\nKey insight: Each layer's gradient depends on the gradients\")\n",
    "    print(f\"from all layers that come after it. This is the 'chain rule' in action!\")\n",
    "\n",
    "    return gradient_flow\n",
    "\n",
    "\n",
    "# Run the demonstrations\n",
    "sequence_gradients = demonstrate_full_backpropagation()\n",
    "gradient_flow_info = demonstrate_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We intentionally kept the mathematical details of backpropagation simple to make it easier to understand. It is highly recommended to deeply understand the mathematical details of backpropagation, through something like Kaparthy's Neural Network Zero-to-Hero course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent: Making Improvements\n",
    "\n",
    "Now that we have calculated the gradients that tell us which direction to adjust each parameter, we need to actually make those adjustments. This process is called **gradient descent** - it's the algorithm that uses our gradients to systematically improve the model's parameters.\n",
    "\n",
    "**The Core Idea:** If a gradient tells us that increasing a parameter will decrease the loss, then we should increase that parameter. If a gradient tells us that decreasing a parameter will decrease the loss, then we should decrease that parameter.\n",
    "\n",
    "**The Gradient Descent Update Rule:**\n",
    "\n",
    "```\n",
    "new_parameter = old_parameter - learning_rate × gradient\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "- **Learning Rate**: Controls how big steps we take. Too big and we might overshoot; too small and training takes forever\n",
    "- **Gradient**: The direction and magnitude of change (calculated via backpropagation)\n",
    "- **Negative Sign**: We subtract the gradient because we want to move in the direction that reduces loss\n",
    "\n",
    "**Why the Negative Sign?**\n",
    "\n",
    "- Positive gradient = increasing parameter increases loss → we want to decrease parameter\n",
    "- Negative gradient = increasing parameter decreases loss → we want to increase parameter\n",
    "- Subtracting the gradient automatically handles both cases correctly\n",
    "\n",
    "Let's see this in action with our model parameters!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LEARNING RATE: The Most Important Hyperparameter\n",
      "============================================================\n",
      "The learning rate controls how big steps we take during training.\n",
      "Let's see what happens with different learning rates:\n",
      "\n",
      "Example: parameter = 0.5, gradient = -0.2\n",
      "Update rule: new_param = 0.5 - learning_rate × -0.2\n",
      "\n",
      "Learning Rate | New Parameter | Change      | Assessment\n",
      "------------------------------------------------------------\n",
      "      0.001 |       0.500 |   +0.000 | Too small - very slow learning\n",
      "      0.010 |       0.502 |   +0.002 | Good - steady progress\n",
      "      0.100 |       0.520 |   +0.020 | Good - steady progress\n",
      "      1.000 |       0.700 |   +0.200 | Large - might be unstable\n",
      "     10.000 |       2.500 |   +2.000 | Too large - likely to diverge\n",
      "\n",
      "Key insights:\n",
      "- Too small: Learning is very slow, might never converge\n",
      "- Too large: Might overshoot and make loss worse\n",
      "- Just right: Makes steady progress toward better predictions\n",
      "\n",
      "In real training:\n",
      "- Start with learning rates like 0.001 or 0.01\n",
      "- Monitor loss - if it's not decreasing, try smaller learning rate\n",
      "- If loss is decreasing too slowly, try larger learning rate\n",
      "- Often use learning rate schedules that decrease over time\n",
      "\n",
      "=================================================================\n",
      "ITERATIVE IMPROVEMENT: How Training Actually Works\n",
      "=================================================================\n",
      "Training doesn't happen in one step - it's an iterative process:\n",
      "1. Forward pass: Make predictions\n",
      "2. Calculate loss: Measure prediction quality\n",
      "3. Backpropagation: Calculate gradients\n",
      "4. Gradient descent: Update parameters\n",
      "5. Repeat thousands/millions of times\n",
      "\n",
      "SIMULATED TRAINING PROGRESSION:\n",
      "-----------------------------------\n",
      "Iteration | Loss   | Improvement | What's Happening\n",
      "-------------------------------------------------------\n",
      "       0 |  2.94 |      0.0% | Starting point - random weights\n",
      "     100 |  2.22 |     24.5% | Rapid initial learning\n",
      "     500 |  1.71 |     22.8% | Rapid initial learning\n",
      "    1000 |  1.35 |     21.1% | Steady improvement\n",
      "    5000 |  1.14 |     15.8% | Fine-tuning, slower progress\n",
      "   10000 |  0.97 |     15.1% | Fine-tuning, slower progress\n",
      "\n",
      "Training characteristics:\n",
      "- Early iterations: Rapid improvement as model learns basic patterns\n",
      "- Later iterations: Slower improvement as model fine-tunes\n",
      "- Eventually: Loss plateaus when model has learned all it can from data\n",
      "\n",
      "Our sequence 'we love deep learning' would become increasingly likely:\n",
      "- Initially: Model predictions are nearly random\n",
      "- After training: Model confidently predicts each word in sequence\n",
      "- Final result: High probability for target sequence, low loss\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_learning_rate_effects():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LEARNING RATE: The Most Important Hyperparameter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"The learning rate controls how big steps we take during training.\")\n",
    "    print(\"Let's see what happens with different learning rates:\")\n",
    "    print()\n",
    "\n",
    "    # Example parameter and gradient\n",
    "    example_param = 0.5\n",
    "    example_gradient = -0.2  # Negative gradient means we should increase parameter\n",
    "\n",
    "    learning_rates = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "    print(f\"Example: parameter = {example_param}, gradient = {example_gradient}\")\n",
    "    print(\n",
    "        f\"Update rule: new_param = {example_param} - learning_rate × {example_gradient}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    print(\"Learning Rate | New Parameter | Change      | Assessment\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        new_param = example_param - lr * example_gradient\n",
    "        change = new_param - example_param\n",
    "\n",
    "        # Assess the learning rate\n",
    "        if lr < 0.01:\n",
    "            assessment = \"Too small - very slow learning\"\n",
    "        elif lr < 0.5:\n",
    "            assessment = \"Good - steady progress\"\n",
    "        elif lr < 2.0:\n",
    "            assessment = \"Large - might be unstable\"\n",
    "        else:\n",
    "            assessment = \"Too large - likely to diverge\"\n",
    "\n",
    "        print(f\"{lr:11.3f} | {new_param:11.3f} | {change:+8.3f} | {assessment}\")\n",
    "\n",
    "    print()\n",
    "    print(\"Key insights:\")\n",
    "    print(\"- Too small: Learning is very slow, might never converge\")\n",
    "    print(\"- Too large: Might overshoot and make loss worse\")\n",
    "    print(\"- Just right: Makes steady progress toward better predictions\")\n",
    "    print()\n",
    "\n",
    "    print(\"In real training:\")\n",
    "    print(\"- Start with learning rates like 0.001 or 0.01\")\n",
    "    print(\"- Monitor loss - if it's not decreasing, try smaller learning rate\")\n",
    "    print(\"- If loss is decreasing too slowly, try larger learning rate\")\n",
    "    print(\"- Often use learning rate schedules that decrease over time\")\n",
    "\n",
    "\n",
    "def demonstrate_iterative_improvement():\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"ITERATIVE IMPROVEMENT: How Training Actually Works\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    print(\"Training doesn't happen in one step - it's an iterative process:\")\n",
    "    print(\"1. Forward pass: Make predictions\")\n",
    "    print(\"2. Calculate loss: Measure prediction quality\")\n",
    "    print(\"3. Backpropagation: Calculate gradients\")\n",
    "    print(\"4. Gradient descent: Update parameters\")\n",
    "    print(\"5. Repeat thousands/millions of times\")\n",
    "    print()\n",
    "\n",
    "    # Simulate several training iterations\n",
    "    print(\"SIMULATED TRAINING PROGRESSION:\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    # Starting loss (from our calculation)\n",
    "    current_loss = 2.94\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    iterations = [0, 100, 500, 1000, 5000, 10000]\n",
    "\n",
    "    print(\"Iteration | Loss   | Improvement | What's Happening\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for i, iteration in enumerate(iterations):\n",
    "        if i == 0:\n",
    "            improvement = 0.0\n",
    "            description = \"Starting point - random weights\"\n",
    "        else:\n",
    "            # Simulate loss improvement (in reality this depends on data, model, etc.)\n",
    "            improvement_rate = 0.15 + 0.1 * math.exp(\n",
    "                -iteration / 2000\n",
    "            )  # Decreasing improvement rate\n",
    "            current_loss *= 1 - improvement_rate\n",
    "            improvement = improvement_rate * 100\n",
    "\n",
    "            if iteration <= 500:\n",
    "                description = \"Rapid initial learning\"\n",
    "            elif iteration <= 2000:\n",
    "                description = \"Steady improvement\"\n",
    "            else:\n",
    "                description = \"Fine-tuning, slower progress\"\n",
    "\n",
    "        print(\n",
    "            f\"{iteration:8d} | {current_loss:5.2f} | {improvement:8.1f}% | {description}\"\n",
    "        )\n",
    "\n",
    "    print()\n",
    "    print(\"Training characteristics:\")\n",
    "    print(\"- Early iterations: Rapid improvement as model learns basic patterns\")\n",
    "    print(\"- Later iterations: Slower improvement as model fine-tunes\")\n",
    "    print(\"- Eventually: Loss plateaus when model has learned all it can from data\")\n",
    "    print()\n",
    "\n",
    "    print(\"Our sequence 'we love deep learning' would become increasingly likely:\")\n",
    "    print(\"- Initially: Model predictions are nearly random\")\n",
    "    print(\"- After training: Model confidently predicts each word in sequence\")\n",
    "    print(\"- Final result: High probability for target sequence, low loss\")\n",
    "\n",
    "\n",
    "# Run the learning rate and iterative improvement demonstrations\n",
    "demonstrate_learning_rate_effects()\n",
    "demonstrate_iterative_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT DESCENT: Updating Model Parameters\n",
      "==================================================\n",
      "Learning rate: 0.1\n",
      "Update rule: new_param = old_param - learning_rate × gradient\n",
      "\n",
      "UPDATING LAYER1_WEIGHTS:\n",
      "-----------------------------------\n",
      "  Param 0:   0.10 - 0.1 ×  -0.05 =   0.11\n",
      "           Parameter increased by 0.005\n",
      "           (Negative gradient → increase param → reduce loss)\n",
      "\n",
      "  Param 1:  -0.30 - 0.1 ×   0.12 =  -0.31\n",
      "           Parameter decreased by 0.012\n",
      "           (Positive gradient → decrease param → reduce loss)\n",
      "\n",
      "  Param 2:   0.50 - 0.1 ×  -0.08 =   0.51\n",
      "           Parameter increased by 0.008\n",
      "           (Negative gradient → increase param → reduce loss)\n",
      "\n",
      "  ... (remaining parameters updated similarly)\n",
      "\n",
      "UPDATING LAYER2_WEIGHTS:\n",
      "-----------------------------------\n",
      "  Param 0:   0.20 - 0.1 ×   0.09 =   0.19\n",
      "           Parameter decreased by 0.009\n",
      "           (Positive gradient → decrease param → reduce loss)\n",
      "\n",
      "  Param 1:   0.10 - 0.1 ×  -0.04 =   0.10\n",
      "           Parameter increased by 0.004\n",
      "           (Negative gradient → increase param → reduce loss)\n",
      "\n",
      "  Param 2:  -0.40 - 0.1 ×   0.11 =  -0.41\n",
      "           Parameter decreased by 0.011\n",
      "           (Positive gradient → decrease param → reduce loss)\n",
      "\n",
      "  ... (remaining parameters updated similarly)\n",
      "\n",
      "UPDATING OUTPUT_WEIGHTS:\n",
      "-----------------------------------\n",
      "  Param 0:   0.30 - 0.1 ×  -0.21 =   0.32\n",
      "           Parameter increased by 0.021\n",
      "           (Negative gradient → increase param → reduce loss)\n",
      "\n",
      "  Param 1:  -0.10 - 0.1 ×   0.15 =  -0.12\n",
      "           Parameter decreased by 0.015\n",
      "           (Positive gradient → decrease param → reduce loss)\n",
      "\n",
      "  Param 2:   0.40 - 0.1 ×  -0.12 =   0.41\n",
      "           Parameter increased by 0.012\n",
      "           (Negative gradient → increase param → reduce loss)\n",
      "\n",
      "  ... (remaining parameters updated similarly)\n",
      "\n",
      "SUMMARY OF CHANGES:\n",
      "--------------------\n",
      "layer1_weights: Total absolute change = 0.050\n",
      "layer2_weights: Total absolute change = 0.056\n",
      "output_weights: Total absolute change = 0.085\n",
      "\n",
      "These updated parameters will now produce better predictions!\n",
      "The model has learned a small step toward predicting 'we love deep learning'\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_gradient_descent_step():\n",
    "    print(\"GRADIENT DESCENT: Updating Model Parameters\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Our original parameters from Section 1\n",
    "    original_params = {\n",
    "        \"layer1_weights\": [0.1, -0.3, 0.5, 0.2, -0.1, 0.4],\n",
    "        \"layer2_weights\": [0.2, 0.1, -0.4, 0.6, 0.3, -0.2],\n",
    "        \"output_weights\": [0.3, -0.1, 0.4, -0.2, 0.5, 0.1],\n",
    "    }\n",
    "\n",
    "    # Gradients calculated from backpropagation (simplified for demonstration)\n",
    "    gradients = {\n",
    "        \"layer1_weights\": [-0.05, 0.12, -0.08, 0.03, 0.07, -0.15],\n",
    "        \"layer2_weights\": [0.09, -0.04, 0.11, -0.18, 0.06, 0.08],\n",
    "        \"output_weights\": [-0.21, 0.15, -0.12, 0.09, -0.17, 0.11],\n",
    "    }\n",
    "\n",
    "    # Learning rate - controls step size\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Update rule: new_param = old_param - learning_rate × gradient\")\n",
    "    print()\n",
    "\n",
    "    updated_params = {}\n",
    "\n",
    "    for layer_name in original_params.keys():\n",
    "        print(f\"UPDATING {layer_name.upper()}:\")\n",
    "        print(\"-\" * 35)\n",
    "\n",
    "        original = original_params[layer_name]\n",
    "        grad = gradients[layer_name]\n",
    "        updated = []\n",
    "\n",
    "        for i, (old_val, gradient_val) in enumerate(zip(original, grad)):\n",
    "            # Apply gradient descent update rule\n",
    "            new_val = old_val - learning_rate * gradient_val\n",
    "            updated.append(new_val)\n",
    "\n",
    "            # Show calculation for first few parameters\n",
    "            if i < 3:\n",
    "                print(\n",
    "                    f\"  Param {i}: {old_val:6.2f} - {learning_rate} × {gradient_val:6.2f} = {new_val:6.2f}\"\n",
    "                )\n",
    "\n",
    "                # Explain the change\n",
    "                change = new_val - old_val\n",
    "                direction = \"increased\" if change > 0 else \"decreased\"\n",
    "                print(f\"           Parameter {direction} by {abs(change):.3f}\")\n",
    "\n",
    "                # Explain why this helps\n",
    "                if gradient_val > 0:\n",
    "                    print(\n",
    "                        f\"           (Positive gradient → decrease param → reduce loss)\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"           (Negative gradient → increase param → reduce loss)\"\n",
    "                    )\n",
    "                print()\n",
    "\n",
    "        updated_params[layer_name] = updated\n",
    "        print(f\"  ... (remaining parameters updated similarly)\")\n",
    "        print()\n",
    "\n",
    "    print(\"SUMMARY OF CHANGES:\")\n",
    "    print(\"-\" * 20)\n",
    "    for layer_name in original_params.keys():\n",
    "        total_change = sum(\n",
    "            abs(updated_params[layer_name][i] - original_params[layer_name][i])\n",
    "            for i in range(len(original_params[layer_name]))\n",
    "        )\n",
    "        print(f\"{layer_name}: Total absolute change = {total_change:.3f}\")\n",
    "\n",
    "    print(f\"\\nThese updated parameters will now produce better predictions!\")\n",
    "    print(\n",
    "        f\"The model has learned a small step toward predicting 'we love deep learning'\"\n",
    "    )\n",
    "\n",
    "    return original_params, updated_params, gradients\n",
    "\n",
    "\n",
    "# Run the gradient descent demonstration\n",
    "original, updated, grads = demonstrate_gradient_descent_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Optimization: Beyond Basic Gradient Descent\n",
    "\n",
    "While the basic gradient descent algorithm we've demonstrated is conceptually correct, modern language models use more sophisticated optimization algorithms. The most common is **Adam** (Adaptive Moment Estimation), which addresses several limitations of basic gradient descent.\n",
    "\n",
    "**Problems with Basic Gradient Descent:**\n",
    "\n",
    "1. **Same learning rate for all parameters**: Some parameters might need larger updates, others smaller\n",
    "2. **No momentum**: Can get stuck in local minima or oscillate\n",
    "3. **Sensitive to learning rate choice**: Too high or too low can prevent convergence\n",
    "\n",
    "**Adam Optimizer Improvements:**\n",
    "\n",
    "1. **Adaptive learning rates**: Different learning rate for each parameter based on its gradient history\n",
    "2. **Momentum**: Uses moving averages of gradients to smooth out updates\n",
    "3. **Bias correction**: Adjusts for initialization bias in early iterations\n",
    "\n",
    "This is why modern language model training is more stable and efficient than simple gradient descent!\n",
    "\n",
    "You can refer to other external resources to go deeper into the details of the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress Demo: Watching the Model Learn\n",
    "\n",
    "Now that we understand all the components (forward pass, loss calculation, backpropagation, and gradient descent), let's put it all together and simulate what actually happens during training. We'll watch our model progressively get better at predicting \"we love deep learning\" over multiple training iterations.\n",
    "\n",
    "**The Training Loop:**\n",
    "\n",
    "```python\n",
    "for iteration in range(num_iterations):\n",
    "    # 1. Forward pass: Generate predictions\n",
    "    logits = model(context)\n",
    "    probabilities = softmax(logits)\n",
    "\n",
    "    # 2. Calculate loss: Measure prediction quality\n",
    "    loss = cross_entropy_loss(probabilities, target)\n",
    "\n",
    "    # 3. Backpropagation: Calculate gradients\n",
    "    gradients = backward_pass(loss)\n",
    "\n",
    "    # 4. Gradient descent: Update parameters\n",
    "    parameters = parameters - learning_rate * gradients\n",
    "```\n",
    "\n",
    "This process repeats thousands or millions of times until the model learns to predict our target sequence accurately.\n",
    "\n",
    "**What We'll Observe:**\n",
    "\n",
    "- How probabilities for target words increase over time\n",
    "- How loss decreases as the model learns\n",
    "- How early words (like \"we\") improve faster than later words\n",
    "- How the model learns from random weights to confident predictions\n",
    "\n",
    "Let's simulate this training process step by step!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SIMULATION: Watching the Model Learn\n",
      "==================================================\n",
      "Training Progress: Probabilities and Loss Over Time\n",
      "============================================================\n",
      "  Iter |  P(we) |  P(love) |  P(deep) | P(learning) | Total Loss\n",
      "----------------------------------------------------------------------\n",
      "     0 |  0.149 |    0.571 |    0.623 |     0.994 |      2.941\n",
      "    50 |  0.267 |    0.702 |    0.731 |     0.997 |      1.992\n",
      "   200 |  0.456 |    0.823 |    0.847 |     0.999 |      1.146\n",
      "   500 |  0.634 |    0.901 |    0.923 |     0.999 |      0.640\n",
      "  1000 |  0.789 |    0.945 |    0.967 |     1.000 |      0.327\n",
      "  2000 |  0.892 |    0.976 |    0.989 |     1.000 |      0.150\n",
      "  5000 |  0.956 |    0.993 |    0.998 |     1.000 |      0.054\n",
      "\n",
      "Key Observations:\n",
      "- All target word probabilities increase over training\n",
      "- Total loss decreases dramatically (2.94 → 0.05)\n",
      "- Later context words (like 'learning') learn faster\n",
      "- Early context words (like 'we') are harder to learn\n"
     ]
    }
   ],
   "source": [
    "def simulate_training_progress():\n",
    "    print(\"TRAINING SIMULATION: Watching the Model Learn\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Simulate training progress over multiple iterations\n",
    "    # Starting probabilities (from our earlier calculations)\n",
    "    training_iterations = [0, 50, 200, 500, 1000, 2000, 5000]\n",
    "\n",
    "    # Simulated training data - probabilities improve over time\n",
    "    training_data = {\n",
    "        0: {  # Initial (random weights)\n",
    "            \"step_1\": {\n",
    "                \"context\": \"<BOS>\",\n",
    "                \"target\": \"we\",\n",
    "                \"prob\": 0.149,\n",
    "                \"loss\": 1.901,\n",
    "            },\n",
    "            \"step_2\": {\"context\": \"we\", \"target\": \"love\", \"prob\": 0.571, \"loss\": 0.561},\n",
    "            \"step_3\": {\n",
    "                \"context\": \"we love\",\n",
    "                \"target\": \"deep\",\n",
    "                \"prob\": 0.623,\n",
    "                \"loss\": 0.473,\n",
    "            },\n",
    "            \"step_4\": {\n",
    "                \"context\": \"we love deep\",\n",
    "                \"target\": \"learning\",\n",
    "                \"prob\": 0.994,\n",
    "                \"loss\": 0.006,\n",
    "            },\n",
    "        },\n",
    "        50: {  # Early training - some improvement\n",
    "            \"step_1\": {\n",
    "                \"context\": \"<BOS>\",\n",
    "                \"target\": \"we\",\n",
    "                \"prob\": 0.267,\n",
    "                \"loss\": 1.321,\n",
    "            },\n",
    "            \"step_2\": {\"context\": \"we\", \"target\": \"love\", \"prob\": 0.702, \"loss\": 0.355},\n",
    "            \"step_3\": {\n",
    "                \"context\": \"we love\",\n",
    "                \"target\": \"deep\",\n",
    "                \"prob\": 0.731,\n",
    "                \"loss\": 0.313,\n",
    "            },\n",
    "            \"step_4\": {\n",
    "                \"context\": \"we love deep\",\n",
    "                \"target\": \"learning\",\n",
    "                \"prob\": 0.997,\n",
    "                \"loss\": 0.003,\n",
    "            },\n",
    "        },\n",
    "        200: {  # Significant learning\n",
    "            \"step_1\": {\n",
    "                \"context\": \"<BOS>\",\n",
    "                \"target\": \"we\",\n",
    "                \"prob\": 0.456,\n",
    "                \"loss\": 0.785,\n",
    "            },\n",
    "            \"step_2\": {\"context\": \"we\", \"target\": \"love\", \"prob\": 0.823, \"loss\": 0.194},\n",
    "            \"step_3\": {\n",
    "                \"context\": \"we love\",\n",
    "                \"target\": \"deep\",\n",
    "                \"prob\": 0.847,\n",
    "                \"loss\": 0.166,\n",
    "            },\n",
    "            \"step_4\": {\n",
    "                \"context\": \"we love deep\",\n",
    "                \"target\": \"learning\",\n",
    "                \"prob\": 0.999,\n",
    "                \"loss\": 0.001,\n",
    "            },\n",
    "        },\n",
    "        500: {  # Good progress\n",
    "            \"step_1\": {\n",
    "                \"context\": \"<BOS>\",\n",
    "                \"target\": \"we\",\n",
    "                \"prob\": 0.634,\n",
    "                \"loss\": 0.455,\n",
    "            },\n",
    "            \"step_2\": {\"context\": \"we\", \"target\": \"love\", \"prob\": 0.901, \"loss\": 0.104},\n",
    "            \"step_3\": {\n",
    "                \"context\": \"we love\",\n",
    "                \"target\": \"deep\",\n",
    "                \"prob\": 0.923,\n",
    "                \"loss\": 0.080,\n",
    "            },\n",
    "            \"step_4\": {\n",
    "                \"context\": \"we love deep\",\n",
    "                \"target\": \"learning\",\n",
    "                \"prob\": 0.999,\n",
    "                \"loss\": 0.001,\n",
    "            },\n",
    "        },\n",
    "        1000: {  # Very good\n",
    "            \"step_1\": {\n",
    "                \"context\": \"<BOS>\",\n",
    "                \"target\": \"we\",\n",
    "                \"prob\": 0.789,\n",
    "                \"loss\": 0.237,\n",
    "            },\n",
    "            \"step_2\": {\"context\": \"we\", \"target\": \"love\", \"prob\": 0.945, \"loss\": 0.057},\n",
    "            \"step_3\": {\n",
    "                \"context\": \"we love\",\n",
    "                \"target\": \"deep\",\n",
    "                \"prob\": 0.967,\n",
    "                \"loss\": 0.033,\n",
    "            },\n",
    "            \"step_4\": {\n",
    "                \"context\": \"we love deep\",\n",
    "                \"target\": \"learning\",\n",
    "                \"prob\": 1.000,\n",
    "                \"loss\": 0.000,\n",
    "            },\n",
    "        },\n",
    "        2000: {  # Excellent\n",
    "            \"step_1\": {\n",
    "                \"context\": \"<BOS>\",\n",
    "                \"target\": \"we\",\n",
    "                \"prob\": 0.892,\n",
    "                \"loss\": 0.115,\n",
    "            },\n",
    "            \"step_2\": {\"context\": \"we\", \"target\": \"love\", \"prob\": 0.976, \"loss\": 0.024},\n",
    "            \"step_3\": {\n",
    "                \"context\": \"we love\",\n",
    "                \"target\": \"deep\",\n",
    "                \"prob\": 0.989,\n",
    "                \"loss\": 0.011,\n",
    "            },\n",
    "            \"step_4\": {\n",
    "                \"context\": \"we love deep\",\n",
    "                \"target\": \"learning\",\n",
    "                \"prob\": 1.000,\n",
    "                \"loss\": 0.000,\n",
    "            },\n",
    "        },\n",
    "        5000: {  # Near perfect\n",
    "            \"step_1\": {\n",
    "                \"context\": \"<BOS>\",\n",
    "                \"target\": \"we\",\n",
    "                \"prob\": 0.956,\n",
    "                \"loss\": 0.045,\n",
    "            },\n",
    "            \"step_2\": {\"context\": \"we\", \"target\": \"love\", \"prob\": 0.993, \"loss\": 0.007},\n",
    "            \"step_3\": {\n",
    "                \"context\": \"we love\",\n",
    "                \"target\": \"deep\",\n",
    "                \"prob\": 0.998,\n",
    "                \"loss\": 0.002,\n",
    "            },\n",
    "            \"step_4\": {\n",
    "                \"context\": \"we love deep\",\n",
    "                \"target\": \"learning\",\n",
    "                \"prob\": 1.000,\n",
    "                \"loss\": 0.000,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"Training Progress: Probabilities and Loss Over Time\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Header\n",
    "    print(\n",
    "        f\"{'Iter':>6} | {'P(we)':>6} | {'P(love)':>8} | {'P(deep)':>8} | {'P(learning)':>9} | {'Total Loss':>10}\"\n",
    "    )\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for iteration in training_iterations:\n",
    "        data = training_data[iteration]\n",
    "\n",
    "        prob_we = data[\"step_1\"][\"prob\"]\n",
    "        prob_love = data[\"step_2\"][\"prob\"]\n",
    "        prob_deep = data[\"step_3\"][\"prob\"]\n",
    "        prob_learning = data[\"step_4\"][\"prob\"]\n",
    "\n",
    "        total_loss = sum(step[\"loss\"] for step in data.values())\n",
    "\n",
    "        print(\n",
    "            f\"{iteration:6d} | {prob_we:6.3f} | {prob_love:8.3f} | {prob_deep:8.3f} | {prob_learning:9.3f} | {total_loss:10.3f}\"\n",
    "        )\n",
    "\n",
    "    print()\n",
    "    print(\"Key Observations:\")\n",
    "    print(\"- All target word probabilities increase over training\")\n",
    "    print(\"- Total loss decreases dramatically (2.94 → 0.05)\")\n",
    "    print(\"- Later context words (like 'learning') learn faster\")\n",
    "    print(\"- Early context words (like 'we') are harder to learn\")\n",
    "\n",
    "    return training_data\n",
    "\n",
    "training_data = simulate_training_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training iterations and corresponding losses\n",
    "iterations = [0, 50, 200, 500, 1000, 2000, 5000, 10000]\n",
    "losses = [2.940, 1.992, 1.145, 0.686, 0.327, 0.150, 0.054, 0.025]\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(iterations, losses, marker='o', linestyle='-', color='royalblue')\n",
    "plt.title(\"Learning Curve: NLL Loss vs. Training Iterations\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Average NLL Loss\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "for x, y in zip(iterations, losses):\n",
    "    plt.text(x, y + 0.05, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_prediction_improvement():\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"PREDICTION QUALITY: From Random to Confident\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    print(\"Let's see how the model's actual predictions change during training:\")\n",
    "    print()\n",
    "\n",
    "    # Show prediction evolution for the first word\n",
    "    prediction_snapshots = [\n",
    "        {\n",
    "            \"iteration\": 0,\n",
    "            \"description\": \"Random initialization\",\n",
    "            \"top_predictions\": [\n",
    "                (\"the\", 0.449),\n",
    "                (\"hello\", 0.301),\n",
    "                (\"we\", 0.149),\n",
    "                (\"is\", 0.082),\n",
    "                (\"love\", 0.011),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"iteration\": 200,\n",
    "            \"description\": \"Early learning\",\n",
    "            \"top_predictions\": [\n",
    "                (\"we\", 0.456),\n",
    "                (\"the\", 0.312),\n",
    "                (\"hello\", 0.156),\n",
    "                (\"is\", 0.048),\n",
    "                (\"love\", 0.028),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"iteration\": 1000,\n",
    "            \"description\": \"Good progress\",\n",
    "            \"top_predictions\": [\n",
    "                (\"we\", 0.789),\n",
    "                (\"the\", 0.134),\n",
    "                (\"hello\", 0.045),\n",
    "                (\"is\", 0.021),\n",
    "                (\"love\", 0.011),\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"iteration\": 5000,\n",
    "            \"description\": \"Near perfect\",\n",
    "            \"top_predictions\": [\n",
    "                (\"we\", 0.956),\n",
    "                (\"the\", 0.032),\n",
    "                (\"hello\", 0.008),\n",
    "                (\"is\", 0.003),\n",
    "                (\"love\", 0.001),\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for snapshot in prediction_snapshots:\n",
    "        print(f\"ITERATION {snapshot['iteration']} ({snapshot['description']}):\")\n",
    "        print(\"Context: '<BOS>' -> Predicting next word\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for i, (word, prob) in enumerate(snapshot[\"top_predictions\"]):\n",
    "            rank = i + 1\n",
    "            marker = \" <- TARGET!\" if word == \"we\" else \"\"\n",
    "            print(f\"  {rank}. {word:8}: {prob:.3f}{marker}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(\"Key Insight: The model gradually learns to assign higher probability\")\n",
    "    print(\"to the target word 'we' and lower probability to other words.\")\n",
    "    print()\n",
    "    print(\"This same process happens for every word in our sequence!\")\n",
    "\n",
    "    return prediction_snapshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "PREDICTION QUALITY: From Random to Confident\n",
      "=================================================================\n",
      "Let's see how the model's actual predictions change during training:\n",
      "\n",
      "ITERATION 0 (Random initialization):\n",
      "Context: '<BOS>' -> Predicting next word\n",
      "----------------------------------------\n",
      "  1. the     : 0.449\n",
      "  2. hello   : 0.301\n",
      "  3. we      : 0.149 <- TARGET!\n",
      "  4. is      : 0.082\n",
      "  5. love    : 0.011\n",
      "\n",
      "ITERATION 200 (Early learning):\n",
      "Context: '<BOS>' -> Predicting next word\n",
      "----------------------------------------\n",
      "  1. we      : 0.456 <- TARGET!\n",
      "  2. the     : 0.312\n",
      "  3. hello   : 0.156\n",
      "  4. is      : 0.048\n",
      "  5. love    : 0.028\n",
      "\n",
      "ITERATION 1000 (Good progress):\n",
      "Context: '<BOS>' -> Predicting next word\n",
      "----------------------------------------\n",
      "  1. we      : 0.789 <- TARGET!\n",
      "  2. the     : 0.134\n",
      "  3. hello   : 0.045\n",
      "  4. is      : 0.021\n",
      "  5. love    : 0.011\n",
      "\n",
      "ITERATION 5000 (Near perfect):\n",
      "Context: '<BOS>' -> Predicting next word\n",
      "----------------------------------------\n",
      "  1. we      : 0.956 <- TARGET!\n",
      "  2. the     : 0.032\n",
      "  3. hello   : 0.008\n",
      "  4. is      : 0.003\n",
      "  5. love    : 0.001\n",
      "\n",
      "Key Insight: The model gradually learns to assign higher probability\n",
      "to the target word 'we' and lower probability to other words.\n",
      "\n",
      "This same process happens for every word in our sequence!\n"
     ]
    }
   ],
   "source": [
    "prediction_evolution = demonstrate_prediction_improvement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Training Works: The Complete Picture\n",
    "\n",
    "We've now covered all the fundamental components that make language models work. Let's step back and see how everything fits together in the complete training process. This section synthesizes all the concepts we've learned into a unified understanding of how language models learn to generate human-like text.\n",
    "\n",
    "**The Four Pillars We've Covered:**\n",
    "\n",
    "1. **Forward Pass** (Sections 2-4): How the model generates predictions from input text\n",
    "2. **Loss Calculation** (Section 5): How we measure prediction quality using cross-entropy loss\n",
    "3. **Backpropagation** (Section 6): How we calculate gradients to identify needed changes\n",
    "4. **Gradient Descent** (Section 7): How we update parameters to improve predictions\n",
    "\n",
    "**The Complete Training Cycle:**\n",
    "\n",
    "```\n",
    "Training Data → Forward Pass → Loss → Backprop → Gradient Descent → Better Model\n",
    "       ↑                                                                    ↓\n",
    "       ←←←←←←←←←←←← Repeat Millions of Times ←←←←←←←←←←←←←←←←←←←←←←←←←←←←←←←\n",
    "```\n",
    "\n",
    "### From Single Examples to Massive Scale\n",
    "\n",
    "Our example with \"we love deep learning\" represents just one tiny training example. Real language models are trained on:\n",
    "\n",
    "- **Billions of text examples** from books, websites, articles, and other sources\n",
    "- **Trillions of individual word predictions** across all these examples\n",
    "- **Billions or even trillions of parameters** that need to be optimized\n",
    "- **Weeks or months of training time** on powerful GPU clusters\n",
    "\n",
    "### The Emergence of Language Understanding\n",
    "\n",
    "What's remarkable is that through this simple process of next-token prediction, language models develop sophisticated capabilities:\n",
    "\n",
    "- **Grammar and syntax**: Learning correct sentence structure\n",
    "- **Factual knowledge**: Absorbing information from training data\n",
    "- **Reasoning patterns**: Developing logical thinking from text examples\n",
    "- **Context awareness**: Understanding how meaning depends on surrounding text\n",
    "- **Style and tone**: Adapting writing style based on context\n",
    "\n",
    "This is why the training objective is so powerful - by learning to predict text accurately, models implicitly learn the patterns that make language meaningful and useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE COMPLETE TRAINING PROCESS: Putting It All Together\n",
      "============================================================\n",
      "Let's trace through one complete training iteration with all components:\n",
      "\n",
      "STEP 1: TRAINING DATA\n",
      "-------------------------\n",
      "Input: A piece of text from the training dataset\n",
      "Example: 'we love deep learning'\n",
      "Purpose: Provides the ground truth for what the model should predict\n",
      "\n",
      "STEP 2: FORWARD PASS\n",
      "------------------------\n",
      "Process:\n",
      "  1. Convert text to tokens: ['<BOS>', 'we', 'love', 'deep', 'learning']\n",
      "  2. For each position, compute logits using neural network layers:\n",
      "     - Input embeddings\n",
      "     - Hidden layer transformations (our simplified layers)\n",
      "     - Output layer producing vocabulary-sized logit vector\n",
      "  3. Convert logits to probabilities using softmax\n",
      "\n",
      "Result: Model produces probability distributions for each next-token prediction\n",
      "\n",
      "STEP 3: LOSS CALCULATION\n",
      "----------------------------\n",
      "Process:\n",
      "  1. For each prediction, compute cross-entropy loss:\n",
      "     Loss = -log(P(correct_word))\n",
      "  2. Sum losses across all predictions in the sequence\n",
      "  3. Our example: 1.901 + 0.561 + 0.473 + 0.006 = 2.941\n",
      "\n",
      "Result: Single number measuring how well model predicted this sequence\n",
      "\n",
      "STEP 4: BACKPROPAGATION\n",
      "---------------------------\n",
      "Process:\n",
      "  1. Calculate ∂Loss/∂logits using softmax gradient formula:\n",
      "     ∂Loss/∂logit_i = P(word_i) - target_i\n",
      "  2. Use chain rule to compute gradients for all parameters:\n",
      "     ∂Loss/∂output_weights, ∂Loss/∂layer2_weights, ∂Loss/∂layer1_weights\n",
      "  3. Each gradient tells us direction and magnitude of needed change\n",
      "\n",
      "Result: Gradient vector for each parameter in the model\n",
      "\n",
      "STEP 5: PARAMETER UPDATE (Gradient Descent)\n",
      "----------------------------------------------\n",
      "Process:\n",
      "  1. Apply update rule: new_param = old_param - learning_rate × gradient\n",
      "  2. Update all parameters simultaneously\n",
      "  3. Model is now slightly better at predicting this type of sequence\n",
      "\n",
      "Result: Updated model parameters that should give lower loss next time\n",
      "\n",
      "THE CYCLE CONTINUES...\n",
      "-------------------------\n",
      "This entire process repeats for:\n",
      "- Every training example in the dataset\n",
      "- Multiple epochs (full passes through the dataset)\n",
      "- Until loss converges or training time limit is reached\n",
      "\n",
      "\n",
      "=================================================================\n",
      "SCALING TO REAL LANGUAGE MODELS\n",
      "=================================================================\n",
      "Comparison: Our Educational Example vs. Production Models\n",
      "------------------------------------------------------------\n",
      "\n",
      "Vocabulary Size:\n",
      "  Our example: 12 words\n",
      "  Real models: 50,000-100,000+ tokens\n",
      "  Impact: Much more complex predictions with longer sequences\n",
      "\n",
      "Model Parameters:\n",
      "  Our example: 36 parameters\n",
      "  Real models: 7B-175B+ parameters\n",
      "  Impact: Vastly more capacity to learn complex patterns\n",
      "\n",
      "Training Examples:\n",
      "  Our example: 1 sequence\n",
      "  Real models: Billions of sequences\n",
      "  Impact: Model learns from diverse, comprehensive text data\n",
      "\n",
      "Context Length:\n",
      "  Our example: 4 tokens\n",
      "  Real models: 2K-100K+ tokens\n",
      "  Impact: Can understand and generate much longer coherent text\n",
      "\n",
      "Training Time:\n",
      "  Our example: Seconds\n",
      "  Real models: Weeks to months\n",
      "  Impact: Allows convergence on massive datasets\n",
      "\n",
      "Hardware:\n",
      "  Our example: CPU demonstration\n",
      "  Real models: Thousands of GPUs/TPUs\n",
      "  Impact: Parallel processing enables feasible training\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHT: Same Fundamental Process, Different Scale\n",
      "============================================================\n",
      "Everything we've learned applies to GPT-4, ChatGPT, and other LLMs:\n",
      "- Same forward pass: tokens → embeddings → layers → logits → probabilities\n",
      "- Same loss function: cross-entropy loss on next-token prediction\n",
      "- Same backpropagation: chain rule to compute parameter gradients\n",
      "- Same optimization: gradient descent (usually Adam) to update parameters\n",
      "\n",
      "The only differences are scale and architectural sophistication.\n",
      "The core learning process remains identical!\n",
      "\n",
      "======================================================================\n",
      "THE MAGIC: How Simple Training Creates Complex Capabilities\n",
      "======================================================================\n",
      "Through next-token prediction training, models spontaneously develop:\n",
      "\n",
      "1. Grammar and Syntax\n",
      "   How it emerges: Learning Statistical Patterns\n",
      "   Example: Model learns 'The cat' is more likely than 'Cat the' from training examples\n",
      "\n",
      "2. Factual Knowledge\n",
      "   How it emerges: Memorizing Training Data Patterns\n",
      "   Example: Seeing 'Paris is the capital of' → 'France' millions of times\n",
      "\n",
      "3. Reasoning Chains\n",
      "   How it emerges: Pattern Matching on Logic\n",
      "   Example: Training on 'If X then Y. X is true. Therefore Y' teaches logical structure\n",
      "\n",
      "4. Context Awareness\n",
      "   How it emerges: Long-range Dependencies\n",
      "   Example: Understanding pronouns refer to earlier nouns in the conversation\n",
      "\n",
      "5. Style Adaptation\n",
      "   How it emerges: Conditional Probability Learning\n",
      "   Example: Different word choices after 'Dear Sir' vs 'Hey dude'\n",
      "\n",
      "6. In-Context Learning\n",
      "   How it emerges: Pattern Recognition\n",
      "   Example: Seeing examples in prompt teaches model new tasks without parameter updates\n",
      "\n",
      "PHILOSOPHICAL INSIGHT:\n",
      "-------------------------\n",
      "These capabilities aren't explicitly programmed - they emerge naturally\n",
      "from the optimization process trying to minimize next-token prediction loss.\n",
      "This is why language modeling is such a powerful learning objective!\n",
      "\n",
      "LIMITATION:\n",
      "---------------\n",
      "Models learn statistical patterns, not true understanding.\n",
      "They excel at pattern matching but may lack genuine comprehension.\n",
      "This leads to both impressive capabilities and notable limitations.\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_complete_training_cycle():\n",
    "    print(\"THE COMPLETE TRAINING PROCESS: Putting It All Together\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"Let's trace through one complete training iteration with all components:\")\n",
    "    print()\n",
    "\n",
    "    # Step 1: Training Data\n",
    "    print(\"STEP 1: TRAINING DATA\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"Input: A piece of text from the training dataset\")\n",
    "    print(\"Example: 'we love deep learning'\")\n",
    "    print(\"Purpose: Provides the ground truth for what the model should predict\")\n",
    "    print()\n",
    "\n",
    "    # Step 2: Forward Pass\n",
    "    print(\"STEP 2: FORWARD PASS\")\n",
    "    print(\"-\" * 24)\n",
    "    print(\"Process:\")\n",
    "    print(\"  1. Convert text to tokens: ['<BOS>', 'we', 'love', 'deep', 'learning']\")\n",
    "    print(\"  2. For each position, compute logits using neural network layers:\")\n",
    "    print(\"     - Input embeddings\")\n",
    "    print(\"     - Hidden layer transformations (our simplified layers)\")\n",
    "    print(\"     - Output layer producing vocabulary-sized logit vector\")\n",
    "    print(\"  3. Convert logits to probabilities using softmax\")\n",
    "    print()\n",
    "    print(\n",
    "        \"Result: Model produces probability distributions for each next-token prediction\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Step 3: Loss Calculation\n",
    "    print(\"STEP 3: LOSS CALCULATION\")\n",
    "    print(\"-\" * 28)\n",
    "    print(\"Process:\")\n",
    "    print(\"  1. For each prediction, compute cross-entropy loss:\")\n",
    "    print(\"     Loss = -log(P(correct_word))\")\n",
    "    print(\"  2. Sum losses across all predictions in the sequence\")\n",
    "    print(\"  3. Our example: 1.901 + 0.561 + 0.473 + 0.006 = 2.941\")\n",
    "    print()\n",
    "    print(\"Result: Single number measuring how well model predicted this sequence\")\n",
    "    print()\n",
    "\n",
    "    # Step 4: Backpropagation\n",
    "    print(\"STEP 4: BACKPROPAGATION\")\n",
    "    print(\"-\" * 27)\n",
    "    print(\"Process:\")\n",
    "    print(\"  1. Calculate ∂Loss/∂logits using softmax gradient formula:\")\n",
    "    print(\"     ∂Loss/∂logit_i = P(word_i) - target_i\")\n",
    "    print(\"  2. Use chain rule to compute gradients for all parameters:\")\n",
    "    print(\"     ∂Loss/∂output_weights, ∂Loss/∂layer2_weights, ∂Loss/∂layer1_weights\")\n",
    "    print(\"  3. Each gradient tells us direction and magnitude of needed change\")\n",
    "    print()\n",
    "    print(\"Result: Gradient vector for each parameter in the model\")\n",
    "    print()\n",
    "\n",
    "    # Step 5: Parameter Update\n",
    "    print(\"STEP 5: PARAMETER UPDATE (Gradient Descent)\")\n",
    "    print(\"-\" * 46)\n",
    "    print(\"Process:\")\n",
    "    print(\"  1. Apply update rule: new_param = old_param - learning_rate × gradient\")\n",
    "    print(\"  2. Update all parameters simultaneously\")\n",
    "    print(\"  3. Model is now slightly better at predicting this type of sequence\")\n",
    "    print()\n",
    "    print(\"Result: Updated model parameters that should give lower loss next time\")\n",
    "    print()\n",
    "\n",
    "    # The cycle continues\n",
    "    print(\"THE CYCLE CONTINUES...\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"This entire process repeats for:\")\n",
    "    print(\"- Every training example in the dataset\")\n",
    "    print(\"- Multiple epochs (full passes through the dataset)\")\n",
    "    print(\"- Until loss converges or training time limit is reached\")\n",
    "    print()\n",
    "\n",
    "    return \"Training cycle complete\"\n",
    "\n",
    "\n",
    "def demonstrate_scaling_to_real_models():\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"SCALING TO REAL LANGUAGE MODELS\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    # Compare our toy example to real models\n",
    "    comparisons = [\n",
    "        {\n",
    "            \"aspect\": \"Vocabulary Size\",\n",
    "            \"our_example\": \"12 words\",\n",
    "            \"real_model\": \"50,000-100,000+ tokens\",\n",
    "            \"impact\": \"Much more complex predictions with longer sequences\",\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Model Parameters\",\n",
    "            \"our_example\": \"36 parameters\",\n",
    "            \"real_model\": \"7B-175B+ parameters\",\n",
    "            \"impact\": \"Vastly more capacity to learn complex patterns\",\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Training Examples\",\n",
    "            \"our_example\": \"1 sequence\",\n",
    "            \"real_model\": \"Billions of sequences\",\n",
    "            \"impact\": \"Model learns from diverse, comprehensive text data\",\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Context Length\",\n",
    "            \"our_example\": \"4 tokens\",\n",
    "            \"real_model\": \"2K-100K+ tokens\",\n",
    "            \"impact\": \"Can understand and generate much longer coherent text\",\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Training Time\",\n",
    "            \"our_example\": \"Seconds\",\n",
    "            \"real_model\": \"Weeks to months\",\n",
    "            \"impact\": \"Allows convergence on massive datasets\",\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Hardware\",\n",
    "            \"our_example\": \"CPU demonstration\",\n",
    "            \"real_model\": \"Thousands of GPUs/TPUs\",\n",
    "            \"impact\": \"Parallel processing enables feasible training\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    print(\"Comparison: Our Educational Example vs. Production Models\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for comp in comparisons:\n",
    "        print(f\"\\n{comp['aspect']}:\")\n",
    "        print(f\"  Our example: {comp['our_example']}\")\n",
    "        print(f\"  Real models: {comp['real_model']}\")\n",
    "        print(f\"  Impact: {comp['impact']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"KEY INSIGHT: Same Fundamental Process, Different Scale\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Everything we've learned applies to GPT-4, ChatGPT, and other LLMs:\")\n",
    "    print(\"- Same forward pass: tokens → embeddings → layers → logits → probabilities\")\n",
    "    print(\"- Same loss function: cross-entropy loss on next-token prediction\")\n",
    "    print(\"- Same backpropagation: chain rule to compute parameter gradients\")\n",
    "    print(\"- Same optimization: gradient descent (usually Adam) to update parameters\")\n",
    "    print()\n",
    "    print(\"The only differences are scale and architectural sophistication.\")\n",
    "    print(\"The core learning process remains identical!\")\n",
    "\n",
    "    return comparisons\n",
    "\n",
    "\n",
    "def demonstrate_emergent_capabilities():\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"THE MAGIC: How Simple Training Creates Complex Capabilities\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"Through next-token prediction training, models spontaneously develop:\")\n",
    "    print()\n",
    "\n",
    "    capabilities = [\n",
    "        {\n",
    "            \"capability\": \"Grammar and Syntax\",\n",
    "            \"mechanism\": \"Learning Statistical Patterns\",\n",
    "            \"example\": \"Model learns 'The cat' is more likely than 'Cat the' from training examples\",\n",
    "        },\n",
    "        {\n",
    "            \"capability\": \"Factual Knowledge\",\n",
    "            \"mechanism\": \"Memorizing Training Data Patterns\",\n",
    "            \"example\": \"Seeing 'Paris is the capital of' → 'France' millions of times\",\n",
    "        },\n",
    "        {\n",
    "            \"capability\": \"Reasoning Chains\",\n",
    "            \"mechanism\": \"Pattern Matching on Logic\",\n",
    "            \"example\": \"Training on 'If X then Y. X is true. Therefore Y' teaches logical structure\",\n",
    "        },\n",
    "        {\n",
    "            \"capability\": \"Context Awareness\",\n",
    "            \"mechanism\": \"Long-range Dependencies\",\n",
    "            \"example\": \"Understanding pronouns refer to earlier nouns in the conversation\",\n",
    "        },\n",
    "        {\n",
    "            \"capability\": \"Style Adaptation\",\n",
    "            \"mechanism\": \"Conditional Probability Learning\",\n",
    "            \"example\": \"Different word choices after 'Dear Sir' vs 'Hey dude'\",\n",
    "        },\n",
    "        {\n",
    "            \"capability\": \"In-Context Learning\",\n",
    "            \"mechanism\": \"Pattern Recognition\",\n",
    "            \"example\": \"Seeing examples in prompt teaches model new tasks without parameter updates\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for i, cap in enumerate(capabilities, 1):\n",
    "        print(f\"{i}. {cap['capability']}\")\n",
    "        print(f\"   How it emerges: {cap['mechanism']}\")\n",
    "        print(f\"   Example: {cap['example']}\")\n",
    "        print()\n",
    "\n",
    "    print(\"PHILOSOPHICAL INSIGHT:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"These capabilities aren't explicitly programmed - they emerge naturally\")\n",
    "    print(\n",
    "        \"from the optimization process trying to minimize next-token prediction loss.\"\n",
    "    )\n",
    "    print(\"This is why language modeling is such a powerful learning objective!\")\n",
    "    print()\n",
    "    print(\"LIMITATION:\")\n",
    "    print(\"-\" * 15)\n",
    "    print(\"Models learn statistical patterns, not true understanding.\")\n",
    "    print(\"They excel at pattern matching but may lack genuine comprehension.\")\n",
    "    print(\"This leads to both impressive capabilities and notable limitations.\")\n",
    "\n",
    "    return capabilities\n",
    "\n",
    "\n",
    "# Run all demonstrations\n",
    "training_cycle = demonstrate_complete_training_cycle()\n",
    "scaling_comparison = demonstrate_scaling_to_real_models()\n",
    "emergent_caps = demonstrate_emergent_capabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Insights: Why Some Predictions Learn Faster\n",
    "\n",
    "You may have noticed in our training simulation that some words learned faster than others:\n",
    "\n",
    "- **\"learning\"** reached high probability very quickly (0.994 → 1.000)\n",
    "- **\"we\"** took much longer to improve (0.149 → 0.956)\n",
    "\n",
    "This pattern reflects real training dynamics in language models:\n",
    "\n",
    "**Later words in sequences often learn faster because:**\n",
    "\n",
    "1. **More context available**: By the time we predict \"learning\", we have \"we love deep\" as context\n",
    "2. **Stronger constraints**: \"deep **_\" has fewer plausible completions than \"< BOS > _**\"\n",
    "3. **Clearer gradients**: Specific contexts lead to more focused gradient signals\n",
    "\n",
    "**Earlier words are harder because:**\n",
    "\n",
    "1. **Less context**: Starting tokens have minimal preceding information\n",
    "2. **Many valid options**: Sentences can begin with countless different words\n",
    "3. **Competing patterns**: Model must learn to distinguish when \"we\" vs \"the\" vs other starters are appropriate\n",
    "\n",
    "This explains why language models often struggle with generating good opening sentences but excel at coherent continuations once they get started.\n",
    "\n",
    "### Real-World Training Implications\n",
    "\n",
    "Understanding these dynamics helps explain many behaviors we see in production language models:\n",
    "\n",
    "- **Context dependency**: Models perform better with more context\n",
    "- **Prompt engineering**: Providing good starting context improves generation quality\n",
    "- **Few-shot learning**: Examples in prompts help establish the right prediction patterns\n",
    "- **Training data balance**: Ensuring diverse starting patterns improves model robustness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Summary and Real-World Connections\n",
    "\n",
    "We've built a complete understanding of how Large Language Models learn to predict text. Let's summarize what we've discovered and connect it to real-world LLM applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY CONCEPTS: What We've Learned About LLMs\n",
      "============================================================\n",
      "\n",
      "1. Next Token Prediction\n",
      "   What: LLMs learn by predicting the next word in a sequence\n",
      "   Example: \"we love deep\" → predict \"learning\"\n",
      "\n",
      "2. Logits and Probabilities\n",
      "   What: Models output raw scores (logits) converted to probabilities\n",
      "   Example: logit 2.1 → probability 0.891 for \"learning\"\n",
      "\n",
      "3. Cross-Entropy Loss\n",
      "   What: Measures surprise - how unexpected the correct answer was\n",
      "   Example: High probability (0.9) → Low loss (0.105)\n",
      "\n",
      "4. Backpropagation\n",
      "   What: Calculates gradients to find which parameters need adjustment\n",
      "   Example: Chain rule: ∂Loss/∂weight = ∂Loss/∂prob × ∂prob/∂logit × ∂logit/∂weight\n",
      "\n",
      "5. Gradient Descent\n",
      "   What: Updates parameters in direction that reduces loss\n",
      "   Example: new_weight = old_weight - learning_rate × gradient\n",
      "\n",
      "6. Emergent Capabilities\n",
      "   What: Complex behaviors emerge from simple next-token training\n",
      "   Example: Reasoning, coding, math from predicting text sequences\n",
      "\n",
      "============================================================\n",
      "INSIGHT: All of language understanding emerges from learning\n",
      "to predict the next token in text sequences!\n",
      "\n",
      "REAL-WORLD CONNECTIONS: From 'we love deep learning' to ChatGPT\n",
      "======================================================================\n",
      "\n",
      "Vocabulary Size:\n",
      "  Our Example: 6 words\n",
      "  Real World: 100,000+ tokens\n",
      "  Impact: Can understand and generate any concept\n",
      "\n",
      "Model Parameters:\n",
      "  Our Example: ~18 parameters\n",
      "  Real World: 175 billion (GPT-3)\n",
      "  Impact: Captures nuanced patterns in language\n",
      "\n",
      "Training Data:\n",
      "  Our Example: 1 sentence\n",
      "  Real World: Trillions of tokens\n",
      "  Impact: Learns from human knowledge and culture\n",
      "\n",
      "Training Time:\n",
      "  Our Example: Seconds\n",
      "  Real World: Months on thousands of GPUs\n",
      "  Impact: Develops sophisticated reasoning abilities\n",
      "\n",
      "======================================================================\n",
      "The same principles we learned apply to all modern LLMs:\n",
      "- GPT-4, Claude, Gemini all use next-token prediction\n",
      "- They all use backpropagation and gradient descent\n",
      "- They all learn from cross-entropy loss\n",
      "- Scale is the main difference!\n",
      "\n",
      "REAL APPLICATIONS: How Next-Token Prediction Powers Everything\n",
      "======================================================================\n",
      "\n",
      "Text Completion:\n",
      "  How: Direct next-token prediction\n",
      "  Examples: Code completion in IDEs, Email suggestions, Search autocomplete\n",
      "\n",
      "Question Answering:\n",
      "  How: Predict tokens that form an answer\n",
      "  Examples: ChatGPT conversations, Technical documentation, Educational tutoring\n",
      "\n",
      "Code Generation:\n",
      "  How: Predict code tokens following patterns\n",
      "  Examples: GitHub Copilot, Code explanation, Bug fixing suggestions\n",
      "\n",
      "Translation:\n",
      "  How: Predict tokens in target language\n",
      "  Examples: Google Translate, Document translation, Cross-lingual communication\n",
      "\n",
      "Creative Writing:\n",
      "  How: Predict tokens following creative patterns\n",
      "  Examples: Story generation, Poetry creation, Marketing copy\n",
      "\n",
      "Reasoning:\n",
      "  How: Predict logical next steps in reasoning\n",
      "  Examples: Math problem solving, Scientific analysis, Strategic planning\n",
      "\n",
      "======================================================================\n",
      "REMARKABLE INSIGHT: All these capabilities emerge from the same\n",
      "simple training objective - predict the next token!\n",
      "\n",
      "TRAINING INNOVATIONS: What Makes Modern LLMs Possible\n",
      "============================================================\n",
      "\n",
      "Transformer Architecture:\n",
      "  What: Attention mechanism processes entire sequences efficiently\n",
      "  Why Important: Enables understanding of long-range dependencies\n",
      "\n",
      "Massive Scale:\n",
      "  What: Billions of parameters trained on internet-scale data\n",
      "  Why Important: Captures nuanced patterns in human language\n",
      "\n",
      "Reinforcement Learning from Human Feedback (RLHF):\n",
      "  What: Fine-tuning using human preferences\n",
      "  Why Important: Makes models helpful, harmless, and honest\n",
      "\n",
      "Instruction Tuning:\n",
      "  What: Training on diverse task instructions\n",
      "  Why Important: Enables following complex human instructions\n",
      "\n",
      "Chain-of-Thought Prompting:\n",
      "  What: Training models to show reasoning steps\n",
      "  Why Important: Improves complex reasoning capabilities\n",
      "\n",
      "============================================================\n",
      "All these innovations build on the foundation we learned:\n",
      "next-token prediction with backpropagation!\n"
     ]
    }
   ],
   "source": [
    "def summarize_key_concepts():\n",
    "    \"\"\"\n",
    "    Summarize the key concepts we've learned about how LLMs work.\n",
    "    \"\"\"\n",
    "    print(\"KEY CONCEPTS: What We've Learned About LLMs\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    concepts = [\n",
    "        {\n",
    "            \"concept\": \"Next Token Prediction\",\n",
    "            \"explanation\": \"LLMs learn by predicting the next word in a sequence\",\n",
    "            \"example\": '\"we love deep\" → predict \"learning\"',\n",
    "        },\n",
    "        {\n",
    "            \"concept\": \"Logits and Probabilities\",\n",
    "            \"explanation\": \"Models output raw scores (logits) converted to probabilities\",\n",
    "            \"example\": 'logit 2.1 → probability 0.891 for \"learning\"',\n",
    "        },\n",
    "        {\n",
    "            \"concept\": \"Cross-Entropy Loss\",\n",
    "            \"explanation\": \"Measures surprise - how unexpected the correct answer was\",\n",
    "            \"example\": \"High probability (0.9) → Low loss (0.105)\",\n",
    "        },\n",
    "        {\n",
    "            \"concept\": \"Backpropagation\",\n",
    "            \"explanation\": \"Calculates gradients to find which parameters need adjustment\",\n",
    "            \"example\": \"Chain rule: ∂Loss/∂weight = ∂Loss/∂prob × ∂prob/∂logit × ∂logit/∂weight\",\n",
    "        },\n",
    "        {\n",
    "            \"concept\": \"Gradient Descent\",\n",
    "            \"explanation\": \"Updates parameters in direction that reduces loss\",\n",
    "            \"example\": \"new_weight = old_weight - learning_rate × gradient\",\n",
    "        },\n",
    "        {\n",
    "            \"concept\": \"Emergent Capabilities\",\n",
    "            \"explanation\": \"Complex behaviors emerge from simple next-token training\",\n",
    "            \"example\": \"Reasoning, coding, math from predicting text sequences\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for i, concept_data in enumerate(concepts, 1):\n",
    "        print(f\"\\n{i}. {concept_data['concept']}\")\n",
    "        print(f\"   What: {concept_data['explanation']}\")\n",
    "        print(f\"   Example: {concept_data['example']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INSIGHT: All of language understanding emerges from learning\")\n",
    "    print(\"to predict the next token in text sequences!\")\n",
    "\n",
    "\n",
    "def connect_to_real_world():\n",
    "    \"\"\"\n",
    "    Connect our simple example to real-world LLM applications.\n",
    "    \"\"\"\n",
    "    print(\"\\nREAL-WORLD CONNECTIONS: From 'we love deep learning' to ChatGPT\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    scaling_factors = [\n",
    "        {\n",
    "            \"aspect\": \"Vocabulary Size\",\n",
    "            \"our_example\": \"6 words\",\n",
    "            \"real_world\": \"100,000+ tokens\",\n",
    "            \"impact\": \"Can understand and generate any concept\",\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Model Parameters\",\n",
    "            \"our_example\": \"~18 parameters\",\n",
    "            \"real_world\": \"175 billion (GPT-3)\",\n",
    "            \"impact\": \"Captures nuanced patterns in language\",\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Training Data\",\n",
    "            \"our_example\": \"1 sentence\",\n",
    "            \"real_world\": \"Trillions of tokens\",\n",
    "            \"impact\": \"Learns from human knowledge and culture\",\n",
    "        },\n",
    "        {\n",
    "            \"aspect\": \"Training Time\",\n",
    "            \"our_example\": \"Seconds\",\n",
    "            \"real_world\": \"Months on thousands of GPUs\",\n",
    "            \"impact\": \"Develops sophisticated reasoning abilities\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for factor in scaling_factors:\n",
    "        print(f\"\\n{factor['aspect']}:\")\n",
    "        print(f\"  Our Example: {factor['our_example']}\")\n",
    "        print(f\"  Real World: {factor['real_world']}\")\n",
    "        print(f\"  Impact: {factor['impact']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"The same principles we learned apply to all modern LLMs:\")\n",
    "    print(\"- GPT-4, Claude, Gemini all use next-token prediction\")\n",
    "    print(\"- They all use backpropagation and gradient descent\")\n",
    "    print(\"- They all learn from cross-entropy loss\")\n",
    "    print(\"- Scale is the main difference!\")\n",
    "\n",
    "\n",
    "def demonstrate_real_applications():\n",
    "    \"\"\"\n",
    "    Show how next-token prediction enables various real-world applications.\n",
    "    \"\"\"\n",
    "    print(\"\\nREAL APPLICATIONS: How Next-Token Prediction Powers Everything\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    applications = [\n",
    "        {\n",
    "            \"application\": \"Text Completion\",\n",
    "            \"mechanism\": \"Direct next-token prediction\",\n",
    "            \"examples\": [\n",
    "                \"Code completion in IDEs\",\n",
    "                \"Email suggestions\",\n",
    "                \"Search autocomplete\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"application\": \"Question Answering\",\n",
    "            \"mechanism\": \"Predict tokens that form an answer\",\n",
    "            \"examples\": [\n",
    "                \"ChatGPT conversations\",\n",
    "                \"Technical documentation\",\n",
    "                \"Educational tutoring\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"application\": \"Code Generation\",\n",
    "            \"mechanism\": \"Predict code tokens following patterns\",\n",
    "            \"examples\": [\n",
    "                \"GitHub Copilot\",\n",
    "                \"Code explanation\",\n",
    "                \"Bug fixing suggestions\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"application\": \"Translation\",\n",
    "            \"mechanism\": \"Predict tokens in target language\",\n",
    "            \"examples\": [\n",
    "                \"Google Translate\",\n",
    "                \"Document translation\",\n",
    "                \"Cross-lingual communication\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"application\": \"Creative Writing\",\n",
    "            \"mechanism\": \"Predict tokens following creative patterns\",\n",
    "            \"examples\": [\"Story generation\", \"Poetry creation\", \"Marketing copy\"],\n",
    "        },\n",
    "        {\n",
    "            \"application\": \"Reasoning\",\n",
    "            \"mechanism\": \"Predict logical next steps in reasoning\",\n",
    "            \"examples\": [\n",
    "                \"Math problem solving\",\n",
    "                \"Scientific analysis\",\n",
    "                \"Strategic planning\",\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for app in applications:\n",
    "        print(f\"\\n{app['application']}:\")\n",
    "        print(f\"  How: {app['mechanism']}\")\n",
    "        print(f\"  Examples: {', '.join(app['examples'])}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REMARKABLE INSIGHT: All these capabilities emerge from the same\")\n",
    "    print(\"simple training objective - predict the next token!\")\n",
    "\n",
    "\n",
    "def explain_training_innovations():\n",
    "    \"\"\"\n",
    "    Explain key innovations that make modern LLM training possible.\n",
    "    \"\"\"\n",
    "    print(\"\\nTRAINING INNOVATIONS: What Makes Modern LLMs Possible\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    innovations = [\n",
    "        {\n",
    "            \"innovation\": \"Transformer Architecture\",\n",
    "            \"what\": \"Attention mechanism processes entire sequences efficiently\",\n",
    "            \"why_important\": \"Enables understanding of long-range dependencies\",\n",
    "        },\n",
    "        {\n",
    "            \"innovation\": \"Massive Scale\",\n",
    "            \"what\": \"Billions of parameters trained on internet-scale data\",\n",
    "            \"why_important\": \"Captures nuanced patterns in human language\",\n",
    "        },\n",
    "        {\n",
    "            \"innovation\": \"Reinforcement Learning from Human Feedback (RLHF)\",\n",
    "            \"what\": \"Fine-tuning using human preferences\",\n",
    "            \"why_important\": \"Makes models helpful, harmless, and honest\",\n",
    "        },\n",
    "        {\n",
    "            \"innovation\": \"Instruction Tuning\",\n",
    "            \"what\": \"Training on diverse task instructions\",\n",
    "            \"why_important\": \"Enables following complex human instructions\",\n",
    "        },\n",
    "        {\n",
    "            \"innovation\": \"Chain-of-Thought Prompting\",\n",
    "            \"what\": \"Training models to show reasoning steps\",\n",
    "            \"why_important\": \"Improves complex reasoning capabilities\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for innovation in innovations:\n",
    "        print(f\"\\n{innovation['innovation']}:\")\n",
    "        print(f\"  What: {innovation['what']}\")\n",
    "        print(f\"  Why Important: {innovation['why_important']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"All these innovations build on the foundation we learned:\")\n",
    "    print(\"next-token prediction with backpropagation!\")\n",
    "\n",
    "\n",
    "# Run all summary functions\n",
    "summarize_key_concepts()\n",
    "connect_to_real_world()\n",
    "demonstrate_real_applications()\n",
    "explain_training_innovations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts: The Beauty of Statistical Learning\n",
    "\n",
    "What makes Large Language Models remarkable is not just their scale, but the elegant simplicity of their core learning principle. By learning to predict the next token in text sequences, they discover:\n",
    "\n",
    "- **Grammar and syntax** from the statistical patterns of language\n",
    "- **Facts and knowledge** from the information patterns in their training data\n",
    "- **Reasoning abilities** from the logical patterns in human discourse\n",
    "- **Cultural understanding** from the social patterns in human communication\n",
    "\n",
    "This demonstrates a profound truth about intelligence: complex, seemingly magical capabilities can emerge from simple statistical learning processes when applied at sufficient scale with appropriate data.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you understand how LLMs learn, you're ready to dive deeper into:\n",
    "\n",
    "1. **Transformer Architecture** - The specific neural network design that makes modern LLMs possible\n",
    "2. **Training at Scale** - How to actually train models with billions of parameters\n",
    "3. **Fine-tuning and Alignment** - How to adapt pre-trained models for specific tasks\n",
    "4. **Inference Optimization** - How to make trained models fast and efficient\n",
    "\n",
    "The foundation you've built here - understanding next-token prediction, loss functions, backpropagation, and gradient descent - will serve you throughout your journey in understanding and building with Large Language Models.\n",
    "\n",
    "Remember: every time you interact with ChatGPT, Claude, or any other LLM, you're witnessing the same fundamental process we explored - predicting the next token in a sequence, one token at a time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
