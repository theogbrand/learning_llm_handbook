{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2.2: Introduction to Attention Mechanisms\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll explore the fundamental concept of *attention* in Transformer models. Attention mechanisms are the core innovation that allowed Transformers to revolutionize NLP and eventually lead to modern Large Language Models (LLMs).\n",
    "\n",
    "We'll break down the mathematics behind self-attention, implement it step by step, and understand how it enables models to capture relationships between words regardless of their distance in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: Context in Language Modeling\n",
    "\n",
    "Let's start by understanding the fundamental challenge that attention solves. From analysing simple character level n-gram models, we observed how incorporating longer context (from single characters to n-characters to words) resulted in \"sentence completers\" that produced more \"interesting\" generations.\n",
    "\n",
    "Before the Transformer architecture, other neural network architectures (like RNNs and LSTMs) attempted to lean into the idea of capturing long-range dependencies, but faced many constraints. These models processed words sequentially, and information from early words would get diluted as the sequence grew longer.\n",
    "\n",
    "A simple theoretical example of a long-range dependency:\n",
    "- \"The cat, which was sitting on the mat, **purrs**.\"\n",
    "- \"The cats, which were sitting on the mat, **purr**.\"\n",
    "\n",
    "The verb at the end needs to agree with the subject at the beginning, even though they're separated by several words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Key Insight: Direct Access Through Attention\n",
    "\n",
    "The breakthrough idea in attention is surprisingly simple: **let each word directly access every other word in the sequence**.\n",
    "\n",
    "Instead of forcing information to flow through a chain of sequential operations, attention creates direct pathways between any two positions in a sequence.\n",
    "\n",
    "This simple idea has profound implications, as it allows the model to:\n",
    "- Capture long-range dependencies with ease\n",
    "- Process sequences in parallel rather than sequentially\n",
    "- Learn which connections are important for a given task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Attention: Mathematical Intuition\n",
    "\n",
    "At its core, self-attention is about computing weighted averages of a sequence. The weights are determined by how relevant each token is to the current token being processed.\n",
    "\n",
    "Let's break down the mathematics step by step:\n",
    "\n",
    "1. **Queries, Keys, and Values**:\n",
    "   For each token in our sequence, we compute three vectors:\n",
    "   - Query (Q): What information is this token looking for?\n",
    "   - Key (K): What information does this token contain?\n",
    "   - Value (V): The actual content/information of this token\n",
    "\n",
    "2. **Attention Scores**:\n",
    "   We compute how much each token should attend to every other token by taking the dot product of queries and keys:\n",
    "   - Score = Q · K^T\n",
    "\n",
    "3. **Scaling and Softmax**:\n",
    "   - Scale the scores by 1/√(dimension of keys) to prevent vanishing gradients\n",
    "   - Apply softmax to get normalized attention weights\n",
    "\n",
    "4. **Weighted Sum**:\n",
    "   - Multiply attention weights by value vectors and sum them up\n",
    "   - This gives us the output of the attention mechanism\n",
    "\n",
    "Mathematically, the self-attention operation is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- Q, K, V are matrices containing the queries, keys, and values for all tokens\n",
    "- d_k is the dimension of the keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Self-Attention\n",
    "\n",
    "Let's visualize self-attention for a simple example to build intuition. We'll create a toy sequence and see how attention works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings:\n",
      "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047],\n",
      "        [-0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624],\n",
      "        [ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,  1.6806],\n",
      "        [ 1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,  0.8599]])\n"
     ]
    }
   ],
   "source": [
    "# Define a simple sequence\n",
    "sequence_length = 4\n",
    "d_model = 8  # Embedding dimension\n",
    "\n",
    "# Random embedding vectors for 4 tokens\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "embeddings = torch.randn(sequence_length, d_model)\n",
    "print(\"Token embeddings:\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([4, 8])\n",
      "Keys shape: torch.Size([4, 8])\n",
      "Values shape: torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Create random projection matrices for Q, K, V\n",
    "d_k = d_model  # Dimension of queries and keys\n",
    "d_v = d_model  # Dimension of values\n",
    "\n",
    "W_q = torch.randn(d_model, d_k)\n",
    "W_k = torch.randn(d_model, d_k)\n",
    "W_v = torch.randn(d_model, d_v)\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = embeddings @ W_q  # Shape: [sequence_length, d_k]\n",
    "K = embeddings @ W_k  # Shape: [sequence_length, d_k]\n",
    "V = embeddings @ W_v  # Shape: [sequence_length, d_v]\n",
    "\n",
    "print(\"Queries shape:\", Q.shape)\n",
    "print(\"Keys shape:\", K.shape)\n",
    "print(\"Values shape:\", V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores (before scaling):\n",
      "tensor([[ 48.3592,  -1.4314,   7.0602,  16.1742],\n",
      "        [  1.8840,  14.5877, -10.8521, -11.8844],\n",
      "        [-20.8970,  -3.9824,  16.8504,   5.9619],\n",
      "        [  7.2162,   3.6702,  49.6052,  35.6275]])\n",
      "\n",
      "Attention scores (after scaling):\n",
      "tensor([[17.0975, -0.5061,  2.4962,  5.7184],\n",
      "        [ 0.6661,  5.1575, -3.8368, -4.2018],\n",
      "        [-7.3882, -1.4080,  5.9575,  2.1079],\n",
      "        [ 2.5513,  1.2976, 17.5381, 12.5962]])\n"
     ]
    }
   ],
   "source": [
    "# Compute attention scores\n",
    "scores = Q @ K.transpose(0, 1)  # Shape: [sequence_length, sequence_length]\n",
    "print(\"Attention scores (before scaling):\")\n",
    "print(scores)\n",
    "\n",
    "# Scale scores\n",
    "scores = scores / (d_k ** 0.5)\n",
    "print(\"\\nAttention scores (after scaling):\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      "tensor([[9.9999e-01, 2.2638e-08, 4.5571e-07, 1.1432e-05],\n",
      "        [1.1078e-02, 9.8871e-01, 1.2271e-04, 8.5188e-05],\n",
      "        [1.5654e-06, 6.1913e-04, 9.7855e-01, 2.0830e-02],\n",
      "        [3.0778e-07, 8.7853e-08, 9.9291e-01, 7.0906e-03]])\n",
      "\n",
      "Row sums (should all be 1.0):\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to get attention weights\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "print(\"Attention weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Verify that weights sum to 1 for each token\n",
    "print(\"\\nRow sums (should all be 1.0):\")\n",
    "print(weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention output:\n",
      "tensor([[-3.6925,  0.7997,  9.4704, -2.5174, -6.2702, -0.8382, -3.9583, -3.3199],\n",
      "        [-1.7831,  5.1673,  3.8020,  2.5630, -3.0025,  1.5952,  0.3802,  5.1056],\n",
      "        [-5.2188,  3.3771, -5.2385,  0.9001,  3.2845, -0.4186,  3.6742, -0.9937],\n",
      "        [-5.2142,  3.4009, -5.2754,  0.8955,  3.3392, -0.3942,  3.6949, -1.0621]])\n"
     ]
    }
   ],
   "source": [
    "# Compute weighted sum of values\n",
    "output = weights @ V  # Shape: [sequence_length, d_v]\n",
    "print(\"Self-attention output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(weights.detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.xlabel('Key positions')\n",
    "plt.ylabel('Query positions')\n",
    "plt.xticks(np.arange(sequence_length))\n",
    "plt.yticks(np.arange(sequence_length))\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing Self-Attention in PyTorch\n",
    "\n",
    "Now, let's implement self-attention as a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k=None, d_v=None):\n",
    "        super().__init__()\n",
    "        d_k = d_k or d_model\n",
    "        d_v = d_v or d_model\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_v, bias=False)\n",
    "        self.W_o = nn.Linear(d_v, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: [batch_size, sequence_length, d_model]\n",
    "        batch_size, sequence_length, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.W_q(x)  # [batch_size, sequence_length, d_k]\n",
    "        K = self.W_k(x)  # [batch_size, sequence_length, d_k]\n",
    "        V = self.W_v(x)  # [batch_size, sequence_length, d_v]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)  # [batch_size, sequence_length, sequence_length]\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        weights = F.softmax(scores, dim=-1)  # [batch_size, sequence_length, sequence_length]\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        output = torch.matmul(weights, V)  # [batch_size, sequence_length, d_v]\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(output)  # [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 8])\n",
      "Output shape: torch.Size([2, 4, 8])\n",
      "Attention weights shape: torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test our implementation\n",
    "batch_size = 2\n",
    "sequence_length = 4\n",
    "d_model = 8\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "# Initialize self-attention module\n",
    "self_attention = SelfAttention(d_model)\n",
    "\n",
    "# Forward pass\n",
    "output, weights = self_attention(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Masked Self-Attention for Language Modeling\n",
    "\n",
    "In language modeling, we want to predict the next token based only on previous tokens. To prevent the model from \"cheating\" by looking at future tokens, we use a mask that hides future positions.\n",
    "\n",
    "Let's implement masked self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask for sequence_length=5:\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def create_causal_mask(sequence_length):\n",
    "    \"\"\"Create a causal mask for masked self-attention.\"\"\"\n",
    "    # Lower triangular matrix of ones\n",
    "    mask = torch.tril(torch.ones(sequence_length, sequence_length))\n",
    "    return mask\n",
    "\n",
    "# Example\n",
    "mask = create_causal_mask(5)\n",
    "print(\"Causal mask for sequence_length=5:\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention weights (each row shows which positions a token can attend to):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2605, 0.7395, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2025, 0.1859, 0.6116, 0.0000, 0.0000],\n",
      "        [0.2658, 0.2540, 0.0900, 0.3901, 0.0000],\n",
      "        [0.1623, 0.2064, 0.1695, 0.2603, 0.2015]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test masked self-attention\n",
    "batch_size = 1\n",
    "sequence_length = 5\n",
    "d_model = 8\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = create_causal_mask(sequence_length).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Initialize self-attention module\n",
    "self_attention = SelfAttention(d_model)\n",
    "\n",
    "# Forward pass with mask\n",
    "output, weights = self_attention(x, mask)\n",
    "\n",
    "print(\"Masked attention weights (each row shows which positions a token can attend to):\")\n",
    "print(weights[0])  # First batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize masked attention\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(weights[0].detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Masked Self-Attention Weights')\n",
    "plt.xlabel('Key positions')\n",
    "plt.ylabel('Query positions')\n",
    "plt.xticks(np.arange(sequence_length))\n",
    "plt.yticks(np.arange(sequence_length))\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Notice how each token (row) can only attend to itself and previous tokens (columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Head Attention\n",
    "\n",
    "In practice, Transformer models use multi-head attention, which allows the model to jointly attend to information from different representation subspaces.\n",
    "\n",
    "Let's implement multi-head attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension of each head\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, _ = x.shape\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        Q = self.W_q(x).view(batch_size, sequence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, sequence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, sequence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Shape after transpose: [batch_size, num_heads, sequence_length, d_k]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        # scores shape: [batch_size, num_heads, sequence_length, sequence_length]\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            # Add dimensions for num_heads\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # weights shape: [batch_size, num_heads, sequence_length, sequence_length]\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        output = torch.matmul(weights, V)\n",
    "        # output shape: [batch_size, num_heads, sequence_length, d_k]\n",
    "        \n",
    "        # Reshape and concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, sequence_length, self.d_model)\n",
    "        # output shape: [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 64])\n",
      "Output shape: torch.Size([2, 5, 64])\n",
      "Attention weights shape: torch.Size([2, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test multi-head attention\n",
    "batch_size = 2\n",
    "sequence_length = 5\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = create_causal_mask(sequence_length).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Initialize multi-head attention module\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Forward pass with mask\n",
    "output, weights = multi_head_attention(x, mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building a Simple Language Model with Self-Attention\n",
    "\n",
    "Now, let's integrate our self-attention module into a simple language model. We'll build a minimal model that uses self-attention to predict the next token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length]\n",
    "        batch_size, sequence_length = x.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(sequence_length, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Get token and position embeddings\n",
    "        token_emb = self.token_embedding(x)  # [batch_size, sequence_length, d_model]\n",
    "        pos_emb = self.position_embedding(positions)  # [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = create_causal_mask(sequence_length).to(x.device)\n",
    "        \n",
    "        # Self-attention block with residual connection and layer normalization\n",
    "        attn_output, _ = self.attention(x, mask)\n",
    "        x = self.norm1(x + attn_output)  # Residual connection and normalization\n",
    "        \n",
    "        # Feed-forward block with residual connection and layer normalization\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_output)  # Residual connection and normalization\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        logits = self.output_projection(x)  # [batch_size, sequence_length, vocab_size]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m x = torch.randint(\u001b[32m0\u001b[39m, vocab_size, (batch_size, sequence_length))\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput logits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mSimpleLanguageModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     38\u001b[39m mask = create_causal_mask(sequence_length).to(x.device)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Self-attention block with residual connection and layer normalization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m attn_output, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(x + attn_output)  \u001b[38;5;66;03m# Residual connection and normalization\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Feed-forward block with residual connection and layer normalization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Add dimensions for num_heads\u001b[39;00m\n\u001b[32m     32\u001b[39m     mask = mask.unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     scores = \u001b[43mscores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1e9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Apply softmax to get attention weights\u001b[39;00m\n\u001b[32m     36\u001b[39m weights = F.softmax(scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "vocab_size = 1000\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "d_ff = 256\n",
    "max_seq_length = 128\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleLanguageModel(vocab_size, d_model, num_heads, d_ff, max_seq_length)\n",
    "\n",
    "# Create a random batch of token sequences\n",
    "batch_size = 3\n",
    "sequence_length = 10\n",
    "x = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Forward pass\n",
    "logits = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. The Attention Is All You Need Connection\n",
    "\n",
    "Our implementation follows the core principles introduced in the landmark paper \"Attention Is All You Need\" by Vaswani et al. (2017), which introduced the Transformer architecture.\n",
    "\n",
    "The key components of the Transformer that we've implemented are:\n",
    "\n",
    "1. **Self-Attention**: Allows tokens to directly interact with each other\n",
    "2. **Multi-Head Attention**: Enables the model to attend to different information subspaces\n",
    "3. **Masked Self-Attention**: Ensures that prediction for a token only depends on previous tokens\n",
    "4. **Positional Encoding**: Provides sequence order information (since self-attention has no inherent notion of position)\n",
    "5. **Feed-Forward Networks**: Process the attended information\n",
    "6. **Residual Connections**: Help with gradient flow during training\n",
    "7. **Layer Normalization**: Stabilizes the model's hidden state\n",
    "\n",
    "A full Transformer would typically have multiple identical layers stacked on top of each other, but our simplified model captures the essence of how attention-based language models work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps: Training on Real Data\n",
    "\n",
    "To complete this implementation, we would need to:\n",
    "\n",
    "1. Prepare a dataset (like Shakespeare used in Karpathy's example)\n",
    "2. Create a training loop\n",
    "3. Implement loss calculation and optimization\n",
    "4. Add text generation functionality\n",
    "\n",
    "The full implementation can be found in [Karpathy's repository](https://github.com/karpathy/ng-video-lecture).\n",
    "\n",
    "In the next Week 2 notebooks, we will explore additional components of the Transformer architecture and see how they come together to form the foundation of modern Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the fundamental concept of self-attention, which is the core innovation behind Transformer models. We've:\n",
    "\n",
    "1. Understood the mathematical intuition behind self-attention\n",
    "2. Implemented single-head and multi-head attention mechanisms\n",
    "3. Built a simple language model using self-attention\n",
    "4. Connected these concepts to the broader Transformer architecture\n",
    "\n",
    "This forms the foundation for understanding how modern Large Language Models work and why they've been so successful at a wide range of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
