{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2.2: Introduction to Attention Mechanisms\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll explore the fundamental concept of *attention* in Transformer models. Attention mechanisms are the core innovation that allowed Transformers to revolutionize NLP and eventually lead to modern Large Language Models (LLMs).\n",
    "\n",
    "We'll break down the mathematics behind self-attention, implement it step by step, and understand how it enables models to capture relationships between words regardless of their distance in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Basic understanding of neural networks\n",
    "- Familiarity with backpropagation (covered in Week 2.1)\n",
    "- Understanding of tokenization (from Week 1.3)\n",
    "- PyTorch basics (from Week 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: Context in Language Modeling\n",
    "\n",
    "Let's start by understanding the fundamental challenge that attention solves.\n",
    "\n",
    "In previous language models (like RNNs and LSTMs), capturing long-range dependencies in text was difficult. These models processed words sequentially, and information from early words would get diluted as the sequence grew longer.\n",
    "\n",
    "Consider sentences like:\n",
    "- \"The cat, which was sitting on the mat, **purrs**.\"\n",
    "- \"The cats, which were sitting on the mat, **purr**.\"\n",
    "\n",
    "The verb at the end needs to agree with the subject at the beginning, even though they're separated by several words. Traditional models struggled with this kind of dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Key Insight: Direct Access Through Attention\n",
    "\n",
    "The breakthrough idea in attention is surprisingly simple: **let each word directly access every other word in the sequence**.\n",
    "\n",
    "Instead of forcing information to flow through a chain of sequential operations, attention creates direct pathways between any two positions in a sequence.\n",
    "\n",
    "This simple idea has profound implications, as it allows the model to:\n",
    "- Capture long-range dependencies with ease\n",
    "- Process sequences in parallel rather than sequentially\n",
    "- Learn which connections are important for a given task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Attention: Mathematical Intuition\n",
    "\n",
    "At its core, self-attention is about computing weighted averages of a sequence. The weights are determined by how relevant each token is to the current token being processed.\n",
    "\n",
    "Let's break down the mathematics step by step:\n",
    "\n",
    "1. **Queries, Keys, and Values**:\n",
    "   For each token in our sequence, we compute three vectors:\n",
    "   - Query (Q): What information is this token looking for?\n",
    "   - Key (K): What information does this token contain?\n",
    "   - Value (V): The actual content/information of this token\n",
    "\n",
    "2. **Attention Scores**:\n",
    "   We compute how much each token should attend to every other token by taking the dot product of queries and keys:\n",
    "   - Score = Q · K^T\n",
    "\n",
    "3. **Scaling and Softmax**:\n",
    "   - Scale the scores by 1/√(dimension of keys) to prevent vanishing gradients\n",
    "   - Apply softmax to get normalized attention weights\n",
    "\n",
    "4. **Weighted Sum**:\n",
    "   - Multiply attention weights by value vectors and sum them up\n",
    "   - This gives us the output of the attention mechanism\n",
    "\n",
    "Mathematically, the self-attention operation is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- Q, K, V are matrices containing the queries, keys, and values for all tokens\n",
    "- d_k is the dimension of the keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Self-Attention\n",
    "\n",
    "Let's visualize self-attention for a simple example to build intuition. We'll create a toy sequence and see how attention works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a simple sequence\n",
    "sequence_length = 4\n",
    "d_model = 8  # Embedding dimension\n",
    "\n",
    "# Random embedding vectors for 4 tokens\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "embeddings = torch.randn(sequence_length, d_model)\n",
    "print(\"Token embeddings:\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create random projection matrices for Q, K, V\n",
    "d_k = d_model  # Dimension of queries and keys\n",
    "d_v = d_model  # Dimension of values\n",
    "\n",
    "W_q = torch.randn(d_model, d_k)\n",
    "W_k = torch.randn(d_model, d_k)\n",
    "W_v = torch.randn(d_model, d_v)\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = embeddings @ W_q  # Shape: [sequence_length, d_k]\n",
    "K = embeddings @ W_k  # Shape: [sequence_length, d_k]\n",
    "V = embeddings @ W_v  # Shape: [sequence_length, d_v]\n",
    "\n",
    "print(\"Queries shape:\", Q.shape)\n",
    "print(\"Keys shape:\", K.shape)\n",
    "print(\"Values shape:\", V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compute attention scores\n",
    "scores = Q @ K.transpose(0, 1)  # Shape: [sequence_length, sequence_length]\n",
    "print(\"Attention scores (before scaling):\")\n",
    "print(scores)\n",
    "\n",
    "# Scale scores\n",
    "scores = scores / (d_k ** 0.5)\n",
    "print(\"\\nAttention scores (after scaling):\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply softmax to get attention weights\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "print(\"Attention weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Verify that weights sum to 1 for each token\n",
    "print(\"\\nRow sums (should all be 1.0):\")\n",
    "print(weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compute weighted sum of values\n",
    "output = weights @ V  # Shape: [sequence_length, d_v]\n",
    "print(\"Self-attention output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(weights.detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.xlabel('Key positions')\n",
    "plt.ylabel('Query positions')\n",
    "plt.xticks(np.arange(sequence_length))\n",
    "plt.yticks(np.arange(sequence_length))\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing Self-Attention in PyTorch\n",
    "\n",
    "Now, let's implement self-attention as a PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k=None, d_v=None):\n",
    "        super().__init__()\n",
    "        d_k = d_k or d_model\n",
    "        d_v = d_v or d_model\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_v, bias=False)\n",
    "        self.W_o = nn.Linear(d_v, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: [batch_size, sequence_length, d_model]\n",
    "        batch_size, sequence_length, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.W_q(x)  # [batch_size, sequence_length, d_k]\n",
    "        K = self.W_k(x)  # [batch_size, sequence_length, d_k]\n",
    "        V = self.W_v(x)  # [batch_size, sequence_length, d_v]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)  # [batch_size, sequence_length, sequence_length]\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        weights = F.softmax(scores, dim=-1)  # [batch_size, sequence_length, sequence_length]\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        output = torch.matmul(weights, V)  # [batch_size, sequence_length, d_v]\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(output)  # [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test our implementation\n",
    "batch_size = 2\n",
    "sequence_length = 4\n",
    "d_model = 8\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "# Initialize self-attention module\n",
    "self_attention = SelfAttention(d_model)\n",
    "\n",
    "# Forward pass\n",
    "output, weights = self_attention(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Masked Self-Attention for Language Modeling\n",
    "\n",
    "In language modeling, we want to predict the next token based only on previous tokens. To prevent the model from \"cheating\" by looking at future tokens, we use a mask that hides future positions.\n",
    "\n",
    "Let's implement masked self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_causal_mask(sequence_length):\n",
    "    \"\"\"Create a causal mask for masked self-attention.\"\"\"\n",
    "    # Lower triangular matrix of ones\n",
    "    mask = torch.tril(torch.ones(sequence_length, sequence_length))\n",
    "    return mask\n",
    "\n",
    "# Example\n",
    "mask = create_causal_mask(5)\n",
    "print(\"Causal mask for sequence_length=5:\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test masked self-attention\n",
    "batch_size = 1\n",
    "sequence_length = 5\n",
    "d_model = 8\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = create_causal_mask(sequence_length).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Initialize self-attention module\n",
    "self_attention = SelfAttention(d_model)\n",
    "\n",
    "# Forward pass with mask\n",
    "output, weights = self_attention(x, mask)\n",
    "\n",
    "print(\"Masked attention weights (each row shows which positions a token can attend to):\")\n",
    "print(weights[0])  # First batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize masked attention\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(weights[0].detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Masked Self-Attention Weights')\n",
    "plt.xlabel('Key positions')\n",
    "plt.ylabel('Query positions')\n",
    "plt.xticks(np.arange(sequence_length))\n",
    "plt.yticks(np.arange(sequence_length))\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Notice how each token (row) can only attend to itself and previous tokens (columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Head Attention\n",
    "\n",
    "In practice, Transformer models use multi-head attention, which allows the model to jointly attend to information from different representation subspaces.\n",
    "\n",
    "Let's implement multi-head attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension of each head\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, _ = x.shape\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        Q = self.W_q(x).view(batch_size, sequence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, sequence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, sequence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Shape after transpose: [batch_size, num_heads, sequence_length, d_k]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        # scores shape: [batch_size, num_heads, sequence_length, sequence_length]\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            # Add dimensions for num_heads\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # weights shape: [batch_size, num_heads, sequence_length, sequence_length]\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        output = torch.matmul(weights, V)\n",
    "        # output shape: [batch_size, num_heads, sequence_length, d_k]\n",
    "        \n",
    "        # Reshape and concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, sequence_length, self.d_model)\n",
    "        # output shape: [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test multi-head attention\n",
    "batch_size = 2\n",
    "sequence_length = 5\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = create_causal_mask(sequence_length).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Initialize multi-head attention module\n",
    "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Forward pass with mask\n",
    "output, weights = multi_head_attention(x, mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building a Simple Language Model with Self-Attention\n",
    "\n",
    "Now, let's integrate our self-attention module into a simple language model inspired by Karpathy's approach. We'll build a minimal model that uses self-attention to predict the next token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length]\n",
    "        batch_size, sequence_length = x.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(sequence_length, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Get token and position embeddings\n",
    "        token_emb = self.token_embedding(x)  # [batch_size, sequence_length, d_model]\n",
    "        pos_emb = self.position_embedding(positions)  # [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = create_causal_mask(sequence_length).to(x.device)\n",
    "        \n",
    "        # Self-attention block with residual connection and layer normalization\n",
    "        attn_output, _ = self.attention(x, mask)\n",
    "        x = self.norm1(x + attn_output)  # Residual connection and normalization\n",
    "        \n",
    "        # Feed-forward block with residual connection and layer normalization\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_output)  # Residual connection and normalization\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        logits = self.output_projection(x)  # [batch_size, sequence_length, vocab_size]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example usage\n",
    "vocab_size = 1000\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "d_ff = 256\n",
    "max_seq_length = 128\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleLanguageModel(vocab_size, d_model, num_heads, d_ff, max_seq_length)\n",
    "\n",
    "# Create a random batch of token sequences\n",
    "batch_size = 3\n",
    "sequence_length = 10\n",
    "x = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Forward pass\n",
    "logits = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. The Attention Is All You Need Connection\n",
    "\n",
    "Our implementation follows the core principles introduced in the landmark paper \"Attention Is All You Need\" by Vaswani et al. (2017), which introduced the Transformer architecture.\n",
    "\n",
    "The key components of the Transformer that we've implemented are:\n",
    "\n",
    "1. **Self-Attention**: Allows tokens to directly interact with each other\n",
    "2. **Multi-Head Attention**: Enables the model to attend to different information subspaces\n",
    "3. **Masked Self-Attention**: Ensures that prediction for a token only depends on previous tokens\n",
    "4. **Positional Encoding**: Provides sequence order information (since self-attention has no inherent notion of position)\n",
    "5. **Feed-Forward Networks**: Process the attended information\n",
    "6. **Residual Connections**: Help with gradient flow during training\n",
    "7. **Layer Normalization**: Stabilizes the model's hidden state\n",
    "\n",
    "A full Transformer would typically have multiple identical layers stacked on top of each other, but our simplified model captures the essence of how attention-based language models work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps: Training on Real Data\n",
    "\n",
    "To complete this implementation, we would need to:\n",
    "\n",
    "1. Prepare a dataset (like Shakespeare used in Karpathy's example)\n",
    "2. Create a training loop\n",
    "3. Implement loss calculation and optimization\n",
    "4. Add text generation functionality\n",
    "\n",
    "The full implementation can be found in [Karpathy's repository](https://github.com/karpathy/ng-video-lecture).\n",
    "\n",
    "In the next Week 2 notebooks, we will explore additional components of the Transformer architecture and see how they come together to form the foundation of modern Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the fundamental concept of self-attention, which is the core innovation behind Transformer models. We've:\n",
    "\n",
    "1. Understood the mathematical intuition behind self-attention\n",
    "2. Implemented single-head and multi-head attention mechanisms\n",
    "3. Built a simple language model using self-attention\n",
    "4. Connected these concepts to the broader Transformer architecture\n",
    "\n",
    "This forms the foundation for understanding how modern Large Language Models work and why they've been so successful at a wide range of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}