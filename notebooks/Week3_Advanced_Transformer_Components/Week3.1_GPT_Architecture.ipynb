{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3.1: Introducing GPT Architecture\n",
    "\n",
    "In this notebook, we'll explore the GPT (Generative Pre-trained Transformer) architecture, building on the transformer concepts we learned in Week 2. We'll examine how GPT2 specifically works, why its design choices are effective, and implement key components of the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to GPT Architecture\n",
    "\n",
    "GPT models are a family of transformer-based language models that use a decoder-only architecture. Unlike the original transformer that had both encoder and decoder components, GPT models consist only of transformer decoder blocks stacked on top of each other.\n",
    "\n",
    "Key characteristics of GPT architecture:\n",
    "- **Decoder-only**: Uses only the decoder part of the transformer\n",
    "- **Causal attention**: Each token can only attend to itself and previous tokens (not future tokens)\n",
    "- **Pre-training and fine-tuning**: Trained on large text corpora and then fine-tuned for specific tasks\n",
    "- **Autoregressive generation**: Generates text by predicting one token at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPT2 Configuration\n",
    "\n",
    "GPT2 comes in different sizes, with different hyperparameters. Here's a class to encapsulate these configuration options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config: {'vocab_size': 50257, 'n_positions': 1024, 'n_embd': 768, 'n_layer': 12, 'n_head': 12, 'dropout': 0.1, 'activation_function': 'gelu'}\n"
     ]
    }
   ],
   "source": [
    "class GPT2Config:\n",
    "    \"\"\"Configuration class for GPT2 model variants\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=50257, n_positions=1024, n_embd=768, n_layer=12, n_head=12, \n",
    "                 dropout=0.1, activation_function=\"gelu\"):\n",
    "        self.vocab_size = vocab_size  # Size of the vocabulary\n",
    "        self.n_positions = n_positions  # Maximum sequence length\n",
    "        self.n_embd = n_embd  # Embedding dimension\n",
    "        self.n_layer = n_layer  # Number of layers\n",
    "        self.n_head = n_head  # Number of attention heads\n",
    "        self.dropout = dropout  # Dropout probability\n",
    "        self.activation_function = activation_function  # Activation function (\"gelu\" or \"relu\")\n",
    "\n",
    "# Define different GPT2 model sizes\n",
    "gpt2_small_config = GPT2Config()  # 124M parameters\n",
    "gpt2_medium_config = GPT2Config(n_embd=1024, n_layer=24, n_head=16)  # 355M parameters\n",
    "gpt2_large_config = GPT2Config(n_embd=1280, n_layer=36, n_head=20)  # 774M parameters\n",
    "gpt2_xl_config = GPT2Config(n_embd=1600, n_layer=48, n_head=25)  # 1.5B parameters\n",
    "\n",
    "# We'll use the small config for our implementation\n",
    "config = gpt2_small_config\n",
    "print(f\"Model config: {config.__dict__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building Blocks of GPT2\n",
    "\n",
    "Now, let's implement the key components of the GPT2 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Layer Normalization\n",
    "\n",
    "GPT2 uses Layer Normalization to stabilize the activations in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization module with optional bias\"\"\"\n",
    "    \n",
    "    def __init__(self, ndim, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate the mean and variance along the last dimension\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # Normalize\n",
    "        x = (x - mean) / torch.sqrt(var + 1e-5)\n",
    "        \n",
    "        # Scale and shift\n",
    "        if self.bias is not None:\n",
    "            x = self.weight * x + self.bias\n",
    "        else:\n",
    "            x = self.weight * x\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multi-Head Causal Self-Attention\n",
    "\n",
    "One of the key innovations in GPT2 is the causal self-attention mechanism. Unlike the original transformer, GPT models ensure that each token can only attend to itself and previous tokens, not future tokens. This is essential for autoregressive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention module\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # Key, query, value projections\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=True)\n",
    "        \n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=True)\n",
    "        \n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "        \n",
    "        # Register a buffer for the causal mask to avoid future tokens\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(config.n_positions, config.n_positions))\n",
    "            .view(1, 1, config.n_positions, config.n_positions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, sequence_length, embedding_dimension)\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        # (batch, sequence_length, 3*n_embd)\n",
    "        qkv = self.c_attn(x)\n",
    "        \n",
    "        # Split into query, key, value and heads\n",
    "        # Each has shape (batch, n_head, sequence_length, head_size)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        q = q.view(batch_size, sequence_length, self.n_head, self.head_size).transpose(1, 2)\n",
    "        k = k.view(batch_size, sequence_length, self.n_head, self.head_size).transpose(1, 2)\n",
    "        v = v.view(batch_size, sequence_length, self.n_head, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # (batch, n_head, sequence_length, sequence_length)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # Apply causal mask - sets attention scores for future tokens to -inf\n",
    "        att = att.masked_fill(self.mask[:, :, :sequence_length, :sequence_length] == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax and dropout\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # (batch, n_head, sequence_length, head_size)\n",
    "        y = att @ v\n",
    "        \n",
    "        # Restore original shape and project back\n",
    "        # (batch, sequence_length, n_embd)\n",
    "        y = y.transpose(1, 2).contiguous().view(batch_size, sequence_length, self.n_embd)\n",
    "        \n",
    "        # Output projection and dropout\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 MLP Block\n",
    "\n",
    "The MLP (Multi-Layer Perceptron) block in GPT2 consists of two linear layers with a GELU activation function in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP module with GELU activation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=True)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=True)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Choose activation function\n",
    "        if config.activation_function == \"gelu\":\n",
    "            self.activation = F.gelu\n",
    "        elif config.activation_function == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {config.activation_function}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First linear layer and activation\n",
    "        x = self.c_fc(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Second linear layer and dropout\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Transformer Block\n",
    "\n",
    "The transformer block combines the attention and MLP blocks along with layer normalization and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        \n",
    "        # MLP with residual connection\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete GPT2 Model\n",
    "\n",
    "Now let's put everything together to create the complete GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    \"\"\"GPT-2 Language Model\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        \n",
    "        # Position embedding\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        \n",
    "        # Dropout\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = LayerNorm(config.n_embd)\n",
    "        \n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: the token embedding matrix is tied to the LM head weight matrix\n",
    "        self.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.config = config\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: (batch, sequence_length) of token indices\n",
    "        device = idx.device\n",
    "        batch_size, sequence_length = idx.size()\n",
    "        assert sequence_length <= self.config.n_positions, f\"Sequence length ({sequence_length}) exceeds model's maximum length ({self.config.n_positions})\"\n",
    "        \n",
    "        # Get token embeddings\n",
    "        # (batch, sequence_length, n_embd)\n",
    "        token_embeddings = self.wte(idx)\n",
    "        \n",
    "        # Get position embeddings\n",
    "        # Create position ids tensor: 0, 1, 2, ..., sequence_length-1\n",
    "        position_ids = torch.arange(0, sequence_length, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # (batch, sequence_length, n_embd)\n",
    "        position_embeddings = self.wpe(position_ids)\n",
    "        \n",
    "        # Add token and position embeddings\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Language modeling logits\n",
    "        # (batch, sequence_length, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # Calculate loss if targets are provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Reshape logits to (batch*sequence_length, vocab_size)\n",
    "            logits_view = logits.view(-1, logits.size(-1))\n",
    "            # Reshape targets to (batch*sequence_length,)\n",
    "            targets_view = targets.view(-1)\n",
    "            # Cross entropy loss\n",
    "            loss = F.cross_entropy(logits_view, targets_view)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens beyond the context provided in idx\"\"\"\n",
    "        # idx: (batch, sequence_length) of token indices\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop the sequence to the maximum allowed length\n",
    "            idx_cond = idx if idx.size(1) <= self.config.n_positions else idx[:, -self.config.n_positions:]\n",
    "            \n",
    "            # Get model predictions\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            \n",
    "            # Focus on the last token's prediction\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Optional: top-k sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to the existing sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating a Model Instance\n",
    "\n",
    "Let's create an instance of our GPT2 model with the small configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "model = GPT2(config).to(device)\n",
    "\n",
    "# Calculate number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Basic Training Loop\n",
    "\n",
    "Let's implement a basic training loop function for our GPT2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, max_epochs):\n",
    "    \"\"\"Basic training loop for GPT2\"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_losses = []\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            # Move data to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            _, loss = model(inputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record loss\n",
    "            loss_value = loss.item()\n",
    "            epoch_losses.append(loss_value)\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch: {epoch+1}/{max_epochs}, Batch: {batch_idx}, Loss: {loss_value:.4f}\")\n",
    "        \n",
    "        # Record average epoch loss\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Generation with GPT2\n",
    "\n",
    "We've already implemented a generation method in our GPT2 class. Let's create a helper function to actually generate text using a trained model and a tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, top_k=40):\n",
    "    \"\"\"Generate text using the GPT2 model\"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor([prompt_tokens], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate new tokens\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(output_ids[0].tolist())\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Architectural Innovations in GPT2\n",
    "\n",
    "GPT2 introduced several important innovations to the transformer architecture:\n",
    "\n",
    "1. **Layer Normalization Position**: GPT2 uses a modified pre-normalization scheme, where layer normalization is applied before the self-attention and feed-forward blocks, rather than after as in the original transformer.\n",
    "\n",
    "2. **Scale Initialization**: Weight initialization was modified to account for the increased depth of the model.\n",
    "\n",
    "3. **Increased Context Length**: GPT2 can handle sequences of up to 1024 tokens, allowing it to maintain longer context.\n",
    "\n",
    "4. **Larger Vocabulary**: The vocabulary size was increased to 50,257 tokens.\n",
    "\n",
    "5. **Byte Pair Encoding (BPE)**: GPT2 uses BPE tokenization with byte-level operations, allowing it to encode any text without out-of-vocabulary tokens.\n",
    "\n",
    "6. **Sparse Attention Patterns**: Some variants of GPT use sparse attention patterns to reduce computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Why GPT2 Architecture Works Well\n",
    "\n",
    "The GPT2 architecture is effective for several reasons:\n",
    "\n",
    "1. **Pretraining and Transfer Learning**: By learning from a vast corpus of text, the model acquires general language understanding that can be adapted to various tasks.\n",
    "\n",
    "2. **Causal Attention Mechanism**: The causal attention mechanism is perfectly suited for language modeling, as it explicitly enforces the autoregressive property of text generation.\n",
    "\n",
    "3. **Layer Normalization and Residual Connections**: These help stabilize training and allow for building deeper networks.\n",
    "\n",
    "4. **Scaled Dot-Product Attention**: This efficiently captures relationships between tokens in a sequence.\n",
    "\n",
    "5. **Parameter Sharing**: The same transformer blocks are reused throughout the network, allowing the model to learn hierarchical representations.\n",
    "\n",
    "6. **Weight Tying**: Sharing weights between the input embedding and output layers reduces parameters and improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've explored the GPT2 architecture, implemented its key components, and discussed why it's effective for language modeling and generation tasks. The GPT architecture has been foundational for modern language models and continues to evolve in models like GPT-3, GPT-4, and beyond.\n",
    "\n",
    "In the next notebooks, we'll dive deeper into advanced transformer components and techniques for fine-tuning these models for specific tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
