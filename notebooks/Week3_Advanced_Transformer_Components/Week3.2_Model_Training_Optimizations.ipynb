{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3.2: Model Training Optimizations\n",
    "\n",
    "In this notebook, we'll introduce optimization techniques used to make training large language models faster and more efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Training Optimizations\n",
    "\n",
    "1. **Starting point**: Unoptimized GPT-2 training (over 1000ms per iteration)\n",
    "2. **GPUs, mixed precision**: First speedup (to ~1000ms)\n",
    "3. **Tensor Cores, TF32 precision**: Second speedup (to ~333ms) \n",
    "4. **float16, gradient scalers, bfloat16**: Third speedup (to ~300ms)\n",
    "5. **torch.compile, Python overhead, kernel fusion**: Fourth speedup (to ~130ms)\n",
    "6. **Flash attention**: Fifth speedup (to ~96ms)\n",
    "7. **Vocabulary size optimization** (from 50257 â†’ 50304): Final speedup (to ~93ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 1: GPUs and Mixed Precision\n",
    "\n",
    "The first optimization involves moving computation to GPUs and leveraging mixed precision training.\n",
    "\n",
    "### Key changes:\n",
    "- Moving model and data to GPU with `model.to(device)` and `batch.to(device)`\n",
    "- Setting up device detection:\n",
    "```python\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- GPUs provide massive parallelism for matrix operations\n",
    "- Initial speedup bringing iteration time to approximately 1000ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 2: Tensor Cores and TF32 Precision\n",
    "\n",
    "NVIDIA GPUs starting from the Ampere architecture (e.g., A100, RTX 3090) have Tensor Cores that can use TensorFloat32 (TF32) for faster matrix multiplications.\n",
    "\n",
    "### Relevant code ([commit 5265b20](https://github.com/karpathy/build-nanogpt/commit/5265b20)):\n",
    "```python\n",
    "# enable TensorFloat32 (TF32) for matrix multiplications\n",
    "torch.set_float32_matmul_precision('high')\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- TF32 uses truncated 19-bit mantissa instead of 23-bit in standard float32\n",
    "- Provides nearly float32 precision with performance close to float16\n",
    "- Reduced iteration time to approximately 333ms (a 3x improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 3: float16, Gradient Scalers, and bfloat16\n",
    "\n",
    "Further precision optimizations use half-precision floating point formats with gradient scaling to prevent underflow.\n",
    "\n",
    "### Relevant code ([commit 177e4cd](https://github.com/karpathy/build-nanogpt/commit/177e4cd)):\n",
    "```python\n",
    "# use bfloat16 precision\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "context_manager = torch.amp.autocast(device_type=device_type, dtype=getattr(torch, dtype))\n",
    "\n",
    "# gradient scaler for mixed precision training stability\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# in training loop:\n",
    "with context_manager:\n",
    "    logits, loss = model(X, Y)\n",
    "    \n",
    "# scale gradients to prevent underflow\n",
    "scaler.scale(loss).backward()\n",
    "scaler.unscale_(optimizer)\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- float16 uses 16 bits instead of 32, halving memory usage and speeding up computation\n",
    "- bfloat16 (brain floating point) has better numerical range than float16\n",
    "- Gradient scaling prevents numerical underflow in gradients\n",
    "- Reduced iteration time to approximately 300ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 4: torch.compile and Kernel Fusion\n",
    "\n",
    "PyTorch 2.0 introduced `torch.compile()` which dynamically optimizes code execution through kernel fusion and other techniques.\n",
    "\n",
    "### Relevant code ([commit fb8bd6e](https://github.com/karpathy/build-nanogpt/commit/fb8bd6e)):\n",
    "```python\n",
    "# apply torch.compile for PyTorch 2.0+ optimization\n",
    "model = torch.compile(model)\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Fuses multiple operations into optimized CUDA kernels\n",
    "- Reduces Python overhead by generating optimized code\n",
    "- Dynamic graph capture and optimization\n",
    "- Drastically reduced iteration time to approximately 130ms (a major improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 5: Flash Attention\n",
    "\n",
    "Flash Attention is an optimized attention implementation that reduces memory usage and increases computation speed.\n",
    "\n",
    "### Relevant code ([commit 7ee630c](https://github.com/karpathy/build-nanogpt/commit/7ee630c)):\n",
    "```python\n",
    "# In CausalSelfAttention forward method\n",
    "y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # flash attention\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Optimized memory access patterns for attention computation\n",
    "- Computes attention in small blocks that fit in fast SRAM\n",
    "- Reduced memory bandwidth requirements\n",
    "- Reduced iteration time to approximately 96ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 6: Vocabulary Size Optimization\n",
    "\n",
    "The final optimization adjusts the vocabulary size to a nice round number that's a multiple of 64, making better use of hardware.\n",
    "\n",
    "### Relevant code ([commit 7230096](https://github.com/karpathy/build-nanogpt/commit/7230096129a52ae763cd810f2bb9e61f67ec9ab7)):\n",
    "```python\n",
    "# Change from\n",
    "model = GPT(GPTConfig())\n",
    "# to\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Changed vocabulary size from 50257 to 50304 (a multiple of 64)\n",
    "- Better alignment with hardware vector units\n",
    "- More efficient memory access and computation\n",
    "- Final reduction in iteration time to approximately 93ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Example\n",
    "\n",
    "Below is a simplified implementation that incorporates these optimizations for a training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set up device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "\n",
    "# Enable TF32 for faster matrix multiplications on Ampere+ GPUs\n",
    "if device_type == 'cuda':\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Set up model (assuming you have a model class)\n",
    "model = YourTransformerModel(vocab_size=50304)  # Use hardware-friendly size\n",
    "model.to(device)\n",
    "\n",
    "# Apply torch.compile (PyTorch 2.0+)\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set up mixed precision training\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "context_manager = torch.amp.autocast(device_type=device_type, dtype=getattr(torch, dtype))\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# Training loop\n",
    "def train_batch(x, y):\n",
    "    # Move data to device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    # Forward pass with mixed precision\n",
    "    with context_manager:\n",
    "        logits, loss = model(x, y)\n",
    "    \n",
    "    # Backward pass with gradient scaling\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "By implementing these optimizations, Karpathy was able to reduce the per-iteration training time from over 1000ms to just 93ms - more than a 10x speedup. The progression of optimizations demonstrates different levels of complexity:\n",
    "\n",
    "1. **Basic**: Moving to GPU\n",
    "2. **Intermediate**: Using mixed precision (TF32, float16, bfloat16) with gradient scaling\n",
    "3. **Advanced**: Leveraging torch.compile and Flash Attention\n",
    "4. **Fine-tuning**: Optimizing vocabulary size for hardware alignment\n",
    "\n",
    "These optimizations compound, with each building on the previous ones, and can be applied to many deep learning models, not just transformers.\n",
    "\n",
    "When training your own models, always consider starting with these optimizations to dramatically reduce training time and resource usage. The order presented here is also a good guideline for implementation, as it progresses from simpler to more complex changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
