{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3.3: Model Training Hyperparameters\n",
    "\n",
    "In this notebook, we'll explore key hyperparameters and optimization techniques for training large language models effectively. These techniques build upon the performance optimizations covered in the previous lesson, now focusing on training effectiveness rather than just speed.\n",
    "\n",
    "We'll continue examining Andrej Karpathy's [build-nanogpt](https://github.com/karpathy/build-nanogpt) repository, focusing on the commits that implement these hyperparameter optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Hyperparameter Optimizations\n",
    "\n",
    "Based on the timestamps in the image, we can see the following progression of optimizations:\n",
    "\n",
    "1. **Hyperparameters, AdamW, gradient clipping** (02:14:55)\n",
    "2. **Learning rate scheduler: warmup + cosine decay** (02:21:06)\n",
    "3. **Batch size schedule, weight decay, FusedAdamW** (02:26:21 - 90ms)\n",
    "4. **Gradient accumulation** (02:34:09)\n",
    "5. **Distributed Data Parallel (DDP)** (02:46:52)\n",
    "6. **Datasets used in GPT-2, GPT-3, FineWeb (EDU)** (03:10:21)\n",
    "7. **Validation data split, validation loss, sampling revive** (03:23:10)\n",
    "8. **Evaluation: HellaSwag, starting the run** (03:28:23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hyperparameters, AdamW, and Gradient Clipping\n",
    "\n",
    "The first optimization focuses on properly configuring the optimizer and implementing gradient clipping to stabilize training.\n",
    "\n",
    "### AdamW Optimizer\n",
    "\n",
    "AdamW is a variant of the Adam optimizer that implements weight decay correctly, separating it from the adaptive learning rate mechanism.\n",
    "\n",
    "```python\n",
    "# Configure AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.95),  # Default is (0.9, 0.999)\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.1  # Will be applied separately from learning rate adjustments\n",
    ")\n",
    "```\n",
    "\n",
    "### Gradient Clipping\n",
    "\n",
    "Gradient clipping prevents exploding gradients by scaling them when their norm exceeds a threshold.\n",
    "\n",
    "```python\n",
    "# Apply gradient clipping\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- AdamW properly decouples weight decay from adaptive momentum\n",
    "- Gradient clipping helps stabilize training, especially in early stages\n",
    "- Beta parameters tuned for transformer models (β₂ = 0.95 instead of 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Rate Scheduler: Warmup + Cosine Decay\n",
    "\n",
    "Learning rate scheduling is crucial for efficient transformer training, typically using a warmup period followed by cosine decay.\n",
    "\n",
    "```python\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# Update learning rate during training\n",
    "lr = get_lr(iteration)\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Warmup period helps stabilize early training when gradients might be erratic\n",
    "- Cosine decay provides a smooth learning rate reduction\n",
    "- Prevents the model from getting stuck in local minima\n",
    "- Improves final model quality and training stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Size Schedule, Weight Decay, and FusedAdamW\n",
    "\n",
    "These optimizations focus on efficient batch processing and further optimizer improvements.\n",
    "\n",
    "### Batch Size Schedule\n",
    "\n",
    "Gradually increasing batch size during training can improve both efficiency and final model performance.\n",
    "\n",
    "```python\n",
    "# Example of batch size scheduling\n",
    "batch_size = min(max_batch_size, initial_batch_size * (iteration // batch_size_schedule_interval + 1))\n",
    "```\n",
    "\n",
    "### Weight Decay with Parameter Filtering\n",
    "\n",
    "Applying weight decay only to weight matrices (not biases, normalization params):\n",
    "\n",
    "```python\n",
    "# Only apply weight decay to 2D parameters (weights, not biases or LN params)\n",
    "decay_params = []\n",
    "nodecay_params = []\n",
    "for pname, p in model.named_parameters():\n",
    "    if p.dim() >= 2:\n",
    "        decay_params.append(p)\n",
    "    else:\n",
    "        nodecay_params.append(p)\n",
    "        \n",
    "optim_groups = [\n",
    "    {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "    {\"params\": nodecay_params, \"weight_decay\": 0.0}\n",
    "]\n",
    "```\n",
    "\n",
    "### FusedAdamW\n",
    "\n",
    "Using NVIDIA Apex's fused implementation of AdamW for faster optimization steps:\n",
    "\n",
    "```python\n",
    "# Using apex for faster optimizer implementation when available\n",
    "try:\n",
    "    from apex.optimizers import FusedAdam\n",
    "    optimizer = FusedAdam(optim_groups, lr=learning_rate, betas=(0.9, 0.95))\n",
    "except ImportError:\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95))\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Scheduled batch sizes improve memory usage and convergence\n",
    "- Selective weight decay improves model generalization\n",
    "- FusedAdamW improves optimizer step performance\n",
    "- Combined, these optimizations reduce training iteration time to ~90ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Accumulation\n",
    "\n",
    "Gradient accumulation allows training with effectively larger batch sizes by accumulating gradients across multiple forward-backward passes before updating the model.\n",
    "\n",
    "```python\n",
    "# Training loop with gradient accumulation\n",
    "accum_iter = 4  # Accumulate gradients over 4 batches\n",
    "model.zero_grad()\n",
    "\n",
    "for micro_step in range(accum_iter):\n",
    "    with torch.amp.autocast(device_type=device_type, dtype=dtype):\n",
    "        logits, loss = model(X, Y)\n",
    "        # Scale the loss to account for gradient accumulation\n",
    "        loss = loss / accum_iter\n",
    "    \n",
    "    # Backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "# Apply optimizer step after accumulation\n",
    "if (iter_num + 1) % accum_iter == 0:\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Enables training with larger effective batch sizes without increasing memory usage\n",
    "- Improves gradient signal quality by averaging over more examples\n",
    "- Allows using larger models on the same hardware\n",
    "- Useful for scenarios with limited GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distributed Data Parallel (DDP)\n",
    "\n",
    "DDP enables training across multiple GPUs or nodes, significantly accelerating training for large models.\n",
    "\n",
    "```python\n",
    "# Initialize process group\n",
    "torch.distributed.init_process_group(backend='nccl')\n",
    "local_rank = int(os.environ['LOCAL_RANK'])\n",
    "torch.cuda.set_device(local_rank)\n",
    "device = f'cuda:{local_rank}'\n",
    "\n",
    "# Wrap model in DDP\n",
    "model = torch.nn.parallel.DistributedDataParallel(\n",
    "    model, \n",
    "    device_ids=[local_rank], \n",
    "    output_device=local_rank\n",
    ")\n",
    "\n",
    "# Use distributed sampler for dataloader\n",
    "sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Linear scaling of training speed with the number of GPUs\n",
    "- Automatic gradient synchronization across all processes\n",
    "- Efficient communication using NCCL backend\n",
    "- Enables training models that wouldn't fit on a single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Datasets Used in GPT-2, GPT-3, FineWeb (EDU)\n",
    "\n",
    "The quality and composition of training data significantly impact model performance. This section explores datasets used for training large language models.\n",
    "\n",
    "### OpenWebText (for GPT-2)\n",
    "- Recreation of WebText dataset used to train GPT-2\n",
    "- Crawled from Reddit links with at least 3 upvotes\n",
    "- ~8 million documents with ~40GB of text\n",
    "\n",
    "### The Pile and C4 (Common Crawl used for GPT-3)\n",
    "- The Pile: 825GB of diverse English text from 22 sources\n",
    "- C4 (Colossal Clean Crawled Corpus): 156GB filtered web text\n",
    "\n",
    "### FineWeb EDU\n",
    "- Educational content filtered from Common Crawl\n",
    "- Higher quality and more reliable information than general web text\n",
    "- Used for specialized models requiring factual accuracy\n",
    "\n",
    "```python\n",
    "# Example dataset loading code\n",
    "train_data = np.memmap(f'{data_dir}/train.bin', dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(f'{data_dir}/val.bin', dtype=np.uint16, mode='r')\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy(data[i:i+block_size].astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy(data[i+1:i+1+block_size].astype(np.int64)) for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation Data Split, Validation Loss, Sampling Revive\n",
    "\n",
    "Proper validation procedures are essential for monitoring training and avoiding overfitting.\n",
    "\n",
    "### Validation Data Split\n",
    "\n",
    "```python\n",
    "# Creating validation split\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]  # 90% for training\n",
    "val_data = data[int(n*0.9):]    # 10% for validation\n",
    "```\n",
    "\n",
    "### Validation Loss Monitoring\n",
    "\n",
    "```python\n",
    "# Evaluate on validation set periodically\n",
    "if iter_num % eval_interval == 0:\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(eval_iters):\n",
    "        with torch.no_grad():\n",
    "            X, Y = get_batch('val')\n",
    "            logits, loss = model(X, Y)\n",
    "            losses.append(loss.item())\n",
    "    val_loss = torch.tensor(losses).mean()\n",
    "    model.train()\n",
    "    \n",
    "    # Log metrics\n",
    "    print(f\"Iteration {iter_num}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping logic or learning rate adjustment based on val_loss\n",
    "```\n",
    "\n",
    "### Sampling During Training (Model Output Preview)\n",
    "\n",
    "```python\n",
    "# Generate sample outputs during training to monitor quality\n",
    "if iter_num % sample_interval == 0:\n",
    "    model.eval()\n",
    "    context = \"Once upon a time\"\n",
    "    encoded = tokenizer.encode(context)\n",
    "    x = torch.tensor([encoded], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate sample text\n",
    "    with torch.no_grad():\n",
    "        y = model.generate(x, max_new_tokens=100, temperature=0.8)[0]\n",
    "        \n",
    "    decoded = tokenizer.decode(y.tolist())\n",
    "    print(f\"\\nSample at iteration {iter_num}:\\n{decoded}\\n\")\n",
    "    model.train()\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Validation loss provides a signal for overfitting\n",
    "- Sample generation reveals model capabilities during training\n",
    "- Helps in early stopping or hyperparameter adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation: HellaSwag\n",
    "\n",
    "Beyond validation loss, evaluating models on standardized benchmarks provides a more comprehensive understanding of capabilities.\n",
    "\n",
    "### HellaSwag Benchmark\n",
    "\n",
    "HellaSwag is a challenging commonsense NLI dataset for evaluating language models' ability to complete scenarios with commonsense reasoning.\n",
    "\n",
    "```python\n",
    "# Example HellaSwag evaluation code\n",
    "def evaluate_hellaswag(model, tokenizer):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for item in hellaswag_dataset:\n",
    "        # Get context and candidate answers\n",
    "        context = item['context']\n",
    "        candidates = item['candidates']\n",
    "        label = item['label']\n",
    "        \n",
    "        # Score each candidate\n",
    "        scores = []\n",
    "        for candidate in candidates:\n",
    "            # Calculate log probability of candidate given context\n",
    "            score = score_text(model, tokenizer, context, candidate)\n",
    "            scores.append(score)\n",
    "            \n",
    "        # Select highest scoring candidate\n",
    "        pred = scores.index(max(scores))\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    return correct / total\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- Provides standardized, task-specific evaluation\n",
    "- Allows comparison with other models in the field\n",
    "- Tests specific reasoning capabilities beyond simple perplexity\n",
    "- Helps identify model strengths and weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Complete Training Pipeline\n",
    "\n",
    "Below is a simplified implementation that incorporates these hyperparameter optimizations into a complete training pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 6e-4\n",
    "min_lr = 6e-5\n",
    "weight_decay = 0.1\n",
    "max_iters = 600000\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = 500000\n",
    "eval_interval = 1000\n",
    "eval_iters = 200\n",
    "grad_clip = 1.0\n",
    "grad_accum_steps = 8\n",
    "batch_size = 12\n",
    "\n",
    "# Set up distributed training if available\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "else:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    master_process = True\n",
    "\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "\n",
    "# Create model (simplified example)\n",
    "model = TransformerModel(vocab_size=50304).to(device)\n",
    "\n",
    "# Configure optimizer\n",
    "# Separate weight decay parameters\n",
    "decay_params = [p for n, p in model.named_parameters() if p.dim() >= 2]\n",
    "nodecay_params = [p for n, p in model.named_parameters() if p.dim() < 2]\n",
    "optim_groups = [\n",
    "    {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "    {\"params\": nodecay_params, \"weight_decay\": 0.0}\n",
    "]\n",
    "\n",
    "# Use FUSED AdamW if available\n",
    "try:\n",
    "    from apex.optimizers import FusedAdam\n",
    "    optimizer = FusedAdam(optim_groups, lr=learning_rate, betas=(0.9, 0.95))\n",
    "    print(\"Using FusedAdam\")\n",
    "except ImportError:\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95))\n",
    "    print(\"Using torch.optim.AdamW\")\n",
    "\n",
    "# Mixed precision setup\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ctx = torch.amp.autocast(device_type=device_type, dtype=getattr(torch, dtype))\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# Wrap model for DDP\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    \n",
    "# Torch compile if available (PyTorch 2.0+)\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Set up learning rate scheduler function\n",
    "def get_lr(it):\n",
    "    # Linear warmup followed by cosine decay\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# Training loop\n",
    "iter_num = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Main loop\n",
    "model.train()\n",
    "while iter_num < max_iters:\n",
    "    \n",
    "    # Update learning rate according to schedule\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    # Gradient accumulation loop\n",
    "    micro_step = 0\n",
    "    while micro_step < grad_accum_steps:\n",
    "        # Get batch data\n",
    "        X, Y = get_batch('train', batch_size=batch_size)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / grad_accum_steps  # Scale for accumulation\n",
    "            \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        micro_step += 1\n",
    "        \n",
    "    # Update weights after accumulation\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Evaluation\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        model.eval()\n",
    "        val_loss = estimate_loss(model, 'val', eval_iters)\n",
    "        print(f\"Step {iter_num}: val loss {val_loss:.4f}, lr {lr:.6f}\")\n",
    "        \n",
    "        # Generate sample\n",
    "        if iter_num > 0:\n",
    "            sample_text = generate_sample(model, tokenizer, \"Once upon a time\")\n",
    "            print(f\"\\nSample:\\n{sample_text}\\n\")\n",
    "            \n",
    "        # Save checkpoint if best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                }\n",
    "                torch.save(checkpoint, \"best_model.pt\")\n",
    "                print(f\"Saved new best model with val_loss {best_val_loss:.4f}\")\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    iter_num += 1\n",
    "\n",
    "# Evaluate on HellaSwag benchmark after training\n",
    "if master_process:\n",
    "    hellaswag_score = evaluate_hellaswag(model, tokenizer)\n",
    "    print(f\"HellaSwag score: {hellaswag_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "This notebook covered essential hyperparameter optimizations for training large language models effectively. Key takeaways include:\n",
    "\n",
    "1. **Optimizer configuration matters**:\n",
    "   - AdamW with proper weight decay separation\n",
    "   - Gradient clipping to prevent instability\n",
    "   - FusedAdamW for performance optimization\n",
    "\n",
    "2. **Learning rate scheduling is crucial**:\n",
    "   - Warmup period followed by cosine decay\n",
    "   - Finding the right minimum and maximum learning rates\n",
    "\n",
    "3. **Efficient training strategies**:\n",
    "   - Gradient accumulation for larger effective batch sizes\n",
    "   - Distributed Data Parallel for multi-GPU training\n",
    "   - Selective weight decay for better generalization\n",
    "\n",
    "4. **Data and evaluation**:\n",
    "   - High-quality datasets are essential (FineWeb EDU vs standard web crawls)\n",
    "   - Proper validation splits and monitoring\n",
    "   - Task-specific evaluation with benchmarks like HellaSwag\n",
    "\n",
    "These optimizations go beyond simply making training faster—they significantly improve model quality, stability, and final performance on downstream tasks. By implementing these techniques, you can train more effective language models with the same compute budget."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}