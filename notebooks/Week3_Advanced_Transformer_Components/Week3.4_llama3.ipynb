{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3.4: From GPT-2 to LLaMA3: Understanding Modern LLM Architectures\n",
    "\n",
    "In this notebook, we'll explore how language model architectures have evolved from GPT-2 to the modern LLaMA family of models. We'll implement key components of the LLaMA architecture and highlight the architectural innovations that have improved performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. Understand the key architectural differences between GPT-2 and LLaMA models\n",
    "2. Implement and explain Rotary Positional Embeddings (RoPE)\n",
    "3. Implement and explain RMSNorm as an alternative to LayerNorm\n",
    "4. Understand Group Query Attention (GQA) and its advantages\n",
    "5. Implement SwiGLU activation for feed-forward networks\n",
    "6. Integrate these components into a coherent model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architectural Overview: GPT-2 vs. LLaMA\n",
    "\n",
    "Before diving into the implementation details, let's compare the high-level architectures of GPT-2 and LLaMA models:\n",
    "\n",
    "| Component | GPT-2 | LLaMA |\n",
    "|-----------|-------|-------|\n",
    "| Normalization | LayerNorm | RMSNorm |\n",
    "| Normalization Placement | Post-attention & Post-FFN | Pre-attention & Pre-FFN (\"RMSNorm-only\") |\n",
    "| Positional Encoding | Learned Positional Embeddings | Rotary Positional Embeddings (RoPE) |\n",
    "| Attention Mechanism | Multi-head Attention (equal Q/K/V heads) | Group Query Attention (more Q than K/V heads) |\n",
    "| Feed-Forward Network | GELU Activation | SwiGLU Activation |\n",
    "| Parameter Count | 124M - 1.5B | 7B - 70B+ |\n",
    "\n",
    "Both models follow the general Transformer architecture with a stack of self-attention and feed-forward layers, but LLaMA introduces several key innovations that improve efficiency and performance at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Let's define configurations for both GPT-2 (for reference) and a small LLaMA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Config:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 50257\n",
    "        self.n_positions = 1024\n",
    "        self.n_embd = 768\n",
    "        self.n_layer = 12\n",
    "        self.n_head = 12\n",
    "        self.dropout = 0.1\n",
    "\n",
    "class LLaMAConfig:\n",
    "    def __init__(self):\n",
    "        # Model dimensions\n",
    "        self.vocab_size = 32000\n",
    "        self.hidden_size = 768  # Dimension of embeddings\n",
    "        self.intermediate_size = 2048  # Dimension of feed-forward layer\n",
    "        self.n_layers = 12\n",
    "        \n",
    "        # Attention-specific parameters\n",
    "        self.n_heads = 32  # Number of query heads\n",
    "        self.n_kv_heads = 8  # Number of key/value heads (for GQA)\n",
    "        self.head_dim = 64   # Dimension of each attention head\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.max_position_embeddings = 2048\n",
    "        self.rope_theta = 10000.0  # Base for RoPE calculations\n",
    "        \n",
    "        # Other parameters\n",
    "        self.norm_eps = 1e-5  # Epsilon for normalization stability\n",
    "        self.dropout = 0.0  # Modern LLMs often use no dropout\n",
    "\n",
    "# We'll use these configs later when implementing our models\n",
    "gpt2_config = GPT2Config()\n",
    "llama_config = LLaMAConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization: LayerNorm vs. RMSNorm\n",
    "\n",
    "GPT-2 uses standard LayerNorm, which normalizes inputs by both mean and variance. LLaMA uses RMSNorm (Root Mean Square Normalization), which only normalizes by the root mean square of activations, making it more computationally efficient.\n",
    "\n",
    "Let's implement both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm normalization layer, as used in GPT-2\"\"\"\n",
    "    def __init__(self, ndim, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Normalize by mean and variance\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(var + 1e-5)  # Add epsilon for numerical stability\n",
    "        \n",
    "        # Apply weights and optional bias\n",
    "        if self.bias is not None:\n",
    "            x = self.weight * x + self.bias\n",
    "        else:\n",
    "            x = self.weight * x\n",
    "        return x\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"RMSNorm normalization layer, as used in LLaMA\"\"\"\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Normalize by Root Mean Square (RMS) only\n",
    "        # No centering step (no mean subtraction)\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        x = x / rms * self.weight\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the behavior of these two normalization methods on a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "  Mean: 2.1427, Std: 2.9749\n",
      "\n",
      "After LayerNorm:\n",
      "  Mean: -0.0000, Std: 1.0102\n",
      "  Mean per sample: tensor([-5.9605e-09, -2.9802e-09, -4.1723e-08,  2.3842e-08, -5.9605e-09],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "\n",
      "After RMSNorm:\n",
      "  Mean: 0.6163, Std: 0.7955\n",
      "  Mean per sample: tensor([0.6000, 0.6447, 0.7569, 0.5719, 0.5080], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with specific mean and variance\n",
    "x = torch.randn(5, 10) * 3 + 2  # mean ≈ 2, std ≈ 3\n",
    "\n",
    "# Apply both normalization methods\n",
    "layer_norm = LayerNorm(10)\n",
    "rms_norm = RMSNorm(10)\n",
    "\n",
    "x_ln = layer_norm(x)\n",
    "x_rms = rms_norm(x)\n",
    "\n",
    "# Compare statistics\n",
    "print(\"Original data:\")\n",
    "print(f\"  Mean: {x.mean().item():.4f}, Std: {x.std().item():.4f}\")\n",
    "\n",
    "print(\"\\nAfter LayerNorm:\")\n",
    "print(f\"  Mean: {x_ln.mean().item():.4f}, Std: {x_ln.std().item():.4f}\")\n",
    "print(f\"  Mean per sample: {x_ln.mean(dim=1)}\")\n",
    "\n",
    "print(\"\\nAfter RMSNorm:\")\n",
    "print(f\"  Mean: {x_rms.mean().item():.4f}, Std: {x_rms.std().item():.4f}\")\n",
    "print(f\"  Mean per sample: {x_rms.mean(dim=1)}\")\n",
    "\n",
    "# Key difference: LayerNorm forces means to 0, while RMSNorm preserves direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences:\n",
    "\n",
    "1. **Computation**: LayerNorm subtracts the mean and divides by standard deviation, while RMSNorm only divides by the root mean square.\n",
    "2. **Parameter Efficiency**: RMSNorm has fewer parameters (no bias term).\n",
    "3. **Performance**: RMSNorm is faster to compute.\n",
    "4. **Direction Preservation**: RMSNorm preserves the direction of the input vectors, only scaling their magnitude, which can better preserve the information in representations.\n",
    "\n",
    "RMSNorm has proven to be particularly effective for larger language models, helping them train more stably with improved gradient flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding: Learned Embeddings vs. RoPE\n",
    "\n",
    "GPT-2 uses learned positional embeddings, which are added to the token embeddings at the input layer. LLaMA uses Rotary Positional Embeddings (RoPE), which apply a rotation to the query and key vectors in the attention mechanism.\n",
    "\n",
    "Let's implement both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbeddings(nn.Module):\n",
    "    \"\"\"Learned positional embeddings, as used in GPT-2\"\"\"\n",
    "    def __init__(self, max_seq_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, embed_dim)\n",
    "        \n",
    "    def forward(self, token_embeddings):\n",
    "        seq_len = token_embeddings.size(1)\n",
    "        positions = torch.arange(0, seq_len, device=token_embeddings.device).unsqueeze(0)\n",
    "        position_embeddings = self.position_embeddings(positions)\n",
    "        \n",
    "        # Add positional embeddings to token embeddings\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement Rotary Positional Embeddings (RoPE). RoPE uses complex number rotations to encode positions, which provides better relative position modeling and helps extend context length beyond the training sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"Rotary Positional Embeddings (RoPE), as used in LLaMA\"\"\"\n",
    "    def __init__(self, dim, max_position=2048, base=10000.0):\n",
    "        super().__init__()\n",
    "        # For each dimension, we'll have a different frequency\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.max_position = max_position\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x, seq_len=None):\n",
    "        if seq_len is None:\n",
    "            seq_len = x.shape[1]\n",
    "        \n",
    "        # Create position indices\n",
    "        position = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "        \n",
    "        # Outer product to get a frequency for each position and dimension\n",
    "        freqs = torch.outer(position, self.inv_freq)\n",
    "        \n",
    "        # Create complex numbers from frequencies (cos and sin values)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        \n",
    "        # Reshape to match expected dimensions [seq_len, dim]\n",
    "        return emb.cos(), emb.sin()\n",
    "    \n",
    "def apply_rotary_embeddings(q, k, cos, sin):\n",
    "    \"\"\"Apply rotary embeddings to query and key tensors\"\"\"\n",
    "    # Reshape inputs for efficient broadcasting\n",
    "    q_embed = torch.cat([q[..., ::2], q[..., 1::2]], dim=-1)\n",
    "    k_embed = torch.cat([k[..., ::2], k[..., 1::2]], dim=-1)\n",
    "    \n",
    "    # Create rotated version by applying trig functions\n",
    "    q_cos = cos * q_embed[..., :q.shape[-1]//2]\n",
    "    q_sin = sin * q_embed[..., q.shape[-1]//2:]\n",
    "    k_cos = cos * k_embed[..., :k.shape[-1]//2]\n",
    "    k_sin = sin * k_embed[..., k.shape[-1]//2:]\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    q_rotated = torch.cat(\n",
    "        [q_cos - q_sin, q_cos + q_sin], \n",
    "        dim=-1\n",
    "    ).reshape(q.shape)\n",
    "    \n",
    "    k_rotated = torch.cat(\n",
    "        [k_cos - k_sin, k_cos + k_sin], \n",
    "        dim=-1\n",
    "    ).reshape(k.shape)\n",
    "    \n",
    "    return q_rotated, k_rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand RoPE, let's visualize how it embeds positional information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m     plt.axis(\u001b[33m'\u001b[39m\u001b[33mequal\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     44\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mvisualize_rope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mvisualize_rope\u001b[39m\u001b[34m(dim, seq_len, base)\u001b[39m\n\u001b[32m     11\u001b[39m cos, sin = rope(q)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Apply rotary embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m q_rotated, _ = \u001b[43mapply_rotary_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Visualize the original and rotated vectors for the first dimension pair\u001b[39;00m\n\u001b[32m     17\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mapply_rotary_embeddings\u001b[39m\u001b[34m(q, k, cos, sin)\u001b[39m\n\u001b[32m     31\u001b[39m k_embed = torch.cat([k[..., ::\u001b[32m2\u001b[39m], k[..., \u001b[32m1\u001b[39m::\u001b[32m2\u001b[39m]], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Create rotated version by applying trig functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m q_cos = \u001b[43mcos\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_embed\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     35\u001b[39m q_sin = sin * q_embed[..., q.shape[-\u001b[32m1\u001b[39m]//\u001b[32m2\u001b[39m:]\n\u001b[32m     36\u001b[39m k_cos = cos * k_embed[..., :k.shape[-\u001b[32m1\u001b[39m]//\u001b[32m2\u001b[39m]\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Visualize rotary embeddings\n",
    "def visualize_rope(dim=64, seq_len=10, base=10000.0):\n",
    "    # Create a rotation embedding instance\n",
    "    rope = RotaryEmbedding(dim=dim, base=base)\n",
    "    \n",
    "    # Create a simple query vector filled with ones\n",
    "    # [batch_size=1, seq_len, dim]\n",
    "    q = torch.ones(1, seq_len, dim)\n",
    "    \n",
    "    # Get the cos and sin components\n",
    "    cos, sin = rope(q)\n",
    "    \n",
    "    # Apply rotary embeddings\n",
    "    q_rotated, _ = apply_rotary_embeddings(q, q, cos, sin)\n",
    "    \n",
    "    # Visualize the original and rotated vectors for the first dimension pair\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Select a pair of dimensions to visualize (each position has a rotation in 2D space)\n",
    "    dim_1, dim_2 = 0, 1\n",
    "    \n",
    "    # Plot original points\n",
    "    x_orig = q[0, :, dim_1].detach().numpy()\n",
    "    y_orig = q[0, :, dim_2].detach().numpy()\n",
    "    plt.scatter(x_orig, y_orig, label='Original', color='blue')\n",
    "    \n",
    "    # Plot rotated points\n",
    "    x_rot = q_rotated[0, :, dim_1].detach().numpy()\n",
    "    y_rot = q_rotated[0, :, dim_2].detach().numpy()\n",
    "    plt.scatter(x_rot, y_rot, label='After RoPE', color='red')\n",
    "    \n",
    "    # Add arrows to show the rotation for each position\n",
    "    for i in range(seq_len):\n",
    "        plt.annotate(f\"Pos {i}\", (x_rot[i], y_rot[i]), fontsize=9)\n",
    "        plt.arrow(x_orig[i], y_orig[i], x_rot[i]-x_orig[i], y_rot[i]-y_orig[i], \n",
    "                 width=0.02, head_width=0.05, color='gray', alpha=0.5)\n",
    "    \n",
    "    plt.title(f\"Rotary Position Embeddings Effect on Vector Dimensions {dim_1} and {dim_2}\")\n",
    "    plt.xlabel(f\"Dimension {dim_1}\")\n",
    "    plt.ylabel(f\"Dimension {dim_2}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "visualize_rope(dim=64, seq_len=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences:\n",
    "\n",
    "1. **Integration**: Learned embeddings are added to token embeddings at the input, while RoPE is applied to query and key vectors in each attention layer.\n",
    "2. **Inductive Bias**: RoPE explicitly encodes relative positions, while learned embeddings must learn position relationships implicitly.\n",
    "3. **Extrapolation**: RoPE allows models to generalize to longer sequences than seen during training.\n",
    "4. **Parameter Efficiency**: RoPE doesn't add trainable parameters, while learned embeddings do.\n",
    "\n",
    "RoPE's ability to model relative positions and extrapolate to longer sequences has made it particularly valuable for modern LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Mechanism: Standard MHA vs. Group Query Attention\n",
    "\n",
    "GPT-2's architecture uses standard Multi-Head Attention (MHA) where the number of query, key, and value heads are equal. LLaMA introduces Group Query Attention (GQA), where multiple query heads share the same key and value heads, reducing computational cost.\n",
    "\n",
    "Let's implement both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    \"\"\"Standard multi-head attention as used in GPT-2\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # Single projection for Q, K, V (as in GPT-2)\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.size()  # batch, sequence length, embedding dimensionality\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        # Reshape to [B, nh, T, hs]\n",
    "        head_size = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # [B, nh, T, T]\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # Apply attention mask (for causal attention)\n",
    "        if attention_mask is None:\n",
    "            # Causal self-attention mask\n",
    "            mask = torch.tril(torch.ones(T, T, device=x.device)).view(1, 1, T, T)\n",
    "            att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        else:\n",
    "            att = att + attention_mask\n",
    "        \n",
    "        # Apply softmax\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        y = att @ v  # [B, nh, T, hs]\n",
    "        \n",
    "        # Reshape back to [B, T, C]\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMAGroupQueryAttention(nn.Module):\n",
    "    \"\"\"Group Query Attention as used in LLaMA\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads  # Number of query heads\n",
    "        self.n_kv_heads = config.n_kv_heads  # Number of key/value heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        # Compute mapping from query heads to key/value heads\n",
    "        # Each key/value head serves multiple query heads\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        \n",
    "        # Separate projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.n_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.n_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        \n",
    "        # Rotary embeddings\n",
    "        self.rope = RotaryEmbedding(self.head_dim, max_position=config.max_position_embeddings, \n",
    "                                   base=config.rope_theta)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.size()  # batch, sequence length, embedding dimensionality\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)  # [B, T, n_q, head_dim]\n",
    "        k = self.k_proj(x).view(B, T, self.n_kv_heads, self.head_dim)  # [B, T, n_kv, head_dim]\n",
    "        v = self.v_proj(x).view(B, T, self.n_kv_heads, self.head_dim)  # [B, T, n_kv, head_dim]\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        cos, sin = self.rope(x, seq_len=T)\n",
    "        q = q.transpose(1, 2)  # [B, n_q, T, head_dim]\n",
    "        k = k.transpose(1, 2)  # [B, n_kv, T, head_dim]\n",
    "        v = v.transpose(1, 2)  # [B, n_kv, T, head_dim]\n",
    "        \n",
    "        # Apply RoPE to q and k\n",
    "        for i in range(self.n_heads):\n",
    "            # Find corresponding kv head - integer division maps query head to kv head\n",
    "            kv_idx = i // self.n_rep\n",
    "            q_i = q[:, i]\n",
    "            k_i = k[:, kv_idx]\n",
    "            \n",
    "            # Apply rotary embeddings\n",
    "            q_rotated, k_rotated = apply_rotary_embeddings(q_i.unsqueeze(1), k_i.unsqueeze(1), cos, sin)\n",
    "            \n",
    "            # Store rotated values back\n",
    "            q[:, i] = q_rotated.squeeze(1)\n",
    "            k[:, kv_idx] = k_rotated.squeeze(1)\n",
    "            \n",
    "        # Compute attention scores\n",
    "        attn_output = torch.zeros(B, self.n_heads, T, self.head_dim, device=x.device)\n",
    "        \n",
    "        # Process each query head\n",
    "        for i in range(self.n_heads):\n",
    "            # Find corresponding kv head\n",
    "            kv_idx = i // self.n_rep\n",
    "            q_i = q[:, i]  # [B, T, head_dim]\n",
    "            k_i = k[:, kv_idx]  # [B, T, head_dim]\n",
    "            v_i = v[:, kv_idx]  # [B, T, head_dim]\n",
    "            \n",
    "            # Compute attention scores\n",
    "            att = (q_i @ k_i.transpose(-2, -1)) * (1.0 / math.sqrt(k_i.size(-1)))  # [B, T, T]\n",
    "            \n",
    "            # Apply causal mask\n",
    "            mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0)\n",
    "            att = att.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "            # Apply softmax\n",
    "            att = F.softmax(att, dim=-1)  # [B, T, T]\n",
    "            \n",
    "            # Apply attention to values\n",
    "            attn_output[:, i] = att @ v_i  # [B, T, head_dim]\n",
    "        \n",
    "        # Reshape and project outputs\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, self.n_heads * self.head_dim)\n",
    "        output = self.o_proj(attn_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the key difference in head structure between standard attention and Group Query Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_head_structure():\n",
    "    \"\"\"Visualize the head structure difference between MHA and GQA\"\"\"\n",
    "    # Define parameters\n",
    "    n_mha_heads = 8\n",
    "    n_gqa_q_heads = 32\n",
    "    n_gqa_kv_heads = 8\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # MHA visualization\n",
    "    mha_q = np.ones((n_mha_heads, 1))\n",
    "    mha_k = np.ones((n_mha_heads, 1))\n",
    "    mha_v = np.ones((n_mha_heads, 1))\n",
    "    \n",
    "    ax1.imshow(np.hstack([mha_q, mha_k, mha_v]), cmap='Blues', aspect='auto')\n",
    "    \n",
    "    # Add labels\n",
    "    for i in range(n_mha_heads):\n",
    "        ax1.text(-0.5, i, f\"H{i}\", ha=\"center\", va=\"center\")\n",
    "        ax1.text(0, i, f\"Q{i}\", ha=\"center\", va=\"center\", color='white')\n",
    "        ax1.text(1, i, f\"K{i}\", ha=\"center\", va=\"center\", color='white')\n",
    "        ax1.text(2, i, f\"V{i}\", ha=\"center\", va=\"center\", color='white')\n",
    "    \n",
    "    ax1.set_title(f\"Multi-Head Attention (MHA)\\n{n_mha_heads} query, key, and value heads\")\n",
    "    ax1.set_xticks([0, 1, 2])\n",
    "    ax1.set_xticklabels(['Query', 'Key', 'Value'])\n",
    "    ax1.set_yticks([])\n",
    "    \n",
    "    # GQA visualization\n",
    "    gqa_heads = np.ones((n_gqa_q_heads, 3))\n",
    "    \n",
    "    # Show which KV heads are shared\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_gqa_kv_heads))\n",
    "    gqa_colors = np.zeros((n_gqa_q_heads, 3, 3))\n",
    "    \n",
    "    # Assign colors to show grouping\n",
    "    for i in range(n_gqa_q_heads):\n",
    "        kv_idx = i // (n_gqa_q_heads // n_gqa_kv_heads)\n",
    "        gqa_colors[i, 0] = [1, 1, 1]  # White for query\n",
    "        gqa_colors[i, 1] = colors[kv_idx, :3]  # Colored for key\n",
    "        gqa_colors[i, 2] = colors[kv_idx, :3]  # Same color for value\n",
    "    \n",
    "    # Plot rectangles manually\n",
    "    for i in range(n_gqa_q_heads):\n",
    "        kv_idx = i // (n_gqa_q_heads // n_gqa_kv_heads)\n",
    "        ax2.add_patch(plt.Rectangle((0, i), 1, 1, color='royalblue'))\n",
    "        ax2.add_patch(plt.Rectangle((1, i), 1, 1, color=colors[kv_idx, :3]))\n",
    "        ax2.add_patch(plt.Rectangle((2, i), 1, 1, color=colors[kv_idx, :3]))\n",
    "        \n",
    "        # Add text labels\n",
    "        ax2.text(-0.5, i, f\"H{i}\", ha=\"center\", va=\"center\")\n",
    "        ax2.text(0, i, f\"Q{i}\", ha=\"center\", va=\"center\", color='white')\n",
    "        ax2.text(1, i, f\"K{kv_idx}\", ha=\"center\", va=\"center\", color='white')\n",
    "        ax2.text(2, i, f\"V{kv_idx}\", ha=\"center\", va=\"center\", color='white')\n",
    "    \n",
    "    ax2.set_xlim(-1, 3)\n",
    "    ax2.set_ylim(n_gqa_q_heads, -1)  # Reverse y axis\n",
    "    ax2.set_title(f\"Group Query Attention (GQA)\\n{n_gqa_q_heads} query heads, {n_gqa_kv_heads} key-value heads\")\n",
    "    ax2.set_xticks([0, 1, 2])\n",
    "    ax2.set_xticklabels(['Query', 'Key', 'Value'])\n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_head_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences:\n",
    "\n",
    "1. **Parameter Efficiency**: GQA reduces the number of parameters by sharing key and value projections across multiple query heads.\n",
    "2. **Computation Cost**: GQA significantly reduces FLOPs in large models by computing fewer key and value projections.\n",
    "3. **Representation Power**: GQA maintains high representation power with more query heads, while reducing memory and computation costs.\n",
    "4. **Memory Bandwidth**: GQA reduces KV cache memory requirements during inference by a factor proportional to the ratio of query to key-value heads.\n",
    "\n",
    "GQA has been particularly important for scaling LLMs efficiently, reducing memory bottlenecks during both training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feed-Forward Network: GELU vs. SwiGLU\n",
    "\n",
    "GPT-2 uses a standard feed-forward network with GELU activation, while LLaMA uses SwiGLU, which is a variant of the GLU (Gated Linear Unit) activation.\n",
    "\n",
    "Let's implement both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    \"\"\"Standard MLP with GELU activation, as used in GPT-2\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x)  # GELU activation\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class LLaMASwiGLU(nn.Module):\n",
    "    \"\"\"SwiGLU feed-forward network, as used in LLaMA\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.w3 = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.w2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # SwiGLU activation: (x * W1) * SiLU((x * W3))\n",
    "        # First compute the gating function SiLU(x * W3)\n",
    "        gate = F.silu(self.w3(x))\n",
    "        # Then compute the linear transform x * W1\n",
    "        x = self.w1(x)\n",
    "        # Multiply the gate with the linear transform\n",
    "        x = x * gate\n",
    "        # Apply the final linear projection\n",
    "        x = self.w2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize and compare the GELU and SiLU (used in SwiGLU) activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations():\n",
    "    \"\"\"Visualize GELU and SiLU activation functions\"\"\"\n",
    "    x = torch.linspace(-5, 5, 1000)\n",
    "    \n",
    "    gelu = F.gelu(x)\n",
    "    silu = F.silu(x)  # Same as Swish activation\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x.numpy(), gelu.numpy(), label='GELU (GPT-2)', linewidth=2)\n",
    "    plt.plot(x.numpy(), silu.numpy(), label='SiLU/Swish (LLaMA)', linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Add baseline ReLU for comparison\n",
    "    plt.plot(x.numpy(), F.relu(x).numpy(), label='ReLU (reference)', color='gray', alpha=0.5)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.title('Comparison of Activation Functions')\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.2)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.2)\n",
    "    plt.show()\n",
    "    \n",
    "    # Explain the gating in SwiGLU\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Example where the gate varies\n",
    "    x1 = torch.ones(1000) * 2.0  # Constant input\n",
    "    x2 = torch.linspace(-5, 5, 1000)  # Varying input to gate\n",
    "    \n",
    "    plt.plot(x2.numpy(), (x1 * F.silu(x2)).numpy(), label='SwiGLU Gate Effect', linewidth=2)\n",
    "    plt.plot(x2.numpy(), F.silu(x2).numpy(), label='SiLU Gate', linewidth=2, linestyle='--')\n",
    "    plt.plot(x2.numpy(), x1.numpy(), label='Input Signal', color='gray', alpha=0.5)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.title('SwiGLU Gating Mechanism')\n",
    "    plt.xlabel('Gate Input')\n",
    "    plt.ylabel('Output')\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.2)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "visualize_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences:\n",
    "\n",
    "1. **Activation Function**: GPT-2 uses GELU activation while LLaMA uses the SiLU (Swish) activation in a gated architecture (SwiGLU).\n",
    "2. **Gating Mechanism**: SwiGLU employs a gating mechanism where one projection is multiplied by a non-linear transformation of another projection, allowing for more complex feature interactions.\n",
    "3. **Projections**: GPT-2 uses two projections (up and down), while LLaMA uses three (w1, w2, and w3).\n",
    "4. **Performance**: SwiGLU has been shown to perform better than GELU for large language models, offering improved convergence and better final performance.\n",
    "\n",
    "The SwiGLU activation provides a more expressive transformation in the feed-forward network, contributing to LLaMA's improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting It All Together: LLaMA Block and Model\n",
    "\n",
    "Now, let's implement a full LLaMA transformer block and the complete model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMABlock(nn.Module):\n",
    "    \"\"\"LLaMA Transformer block\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        # Pre-normalization for attention (different from GPT-2's post-normalization)\n",
    "        self.attention_norm = RMSNorm(config.hidden_size, eps=config.norm_eps)\n",
    "        self.attention = LLaMAGroupQueryAttention(config)\n",
    "        \n",
    "        # Pre-normalization for feed-forward\n",
    "        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.norm_eps)\n",
    "        self.feed_forward = LLaMASwiGLU(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pre-norm for attention (different from GPT-2)\n",
    "        h = x + self.attention(self.attention_norm(x))\n",
    "        \n",
    "        # Pre-norm for feed-forward\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        \n",
    "        return out\n",
    "\n",
    "class LLaMAModel(nn.Module):\n",
    "    \"\"\"LLaMA language model\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        \n",
    "        # No separate position embeddings like in GPT-2 (RoPE is applied inside attention)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.layers = nn.ModuleList([LLaMABlock(config) for _ in range(config.n_layers)])\n",
    "        \n",
    "        # Final normalization layer\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.norm_eps)\n",
    "        \n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights with standard approach\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        B, T = input_ids.size()  # batch size, sequence length\n",
    "        \n",
    "        # Get token embeddings\n",
    "        h = self.token_embeddings(input_ids)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        h = self.norm(h)\n",
    "        \n",
    "        # Language modeling head\n",
    "        logits = self.lm_head(h)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison: Key Differences Visualized\n",
    "\n",
    "Let's visualize the key architectural differences between GPT-2 and LLaMA with diagrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_architecture_comparison():\n",
    "    \"\"\"Visualize key architecture differences between GPT-2 and LLaMA\"\"\"\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    \n",
    "    # GPT-2 Architecture\n",
    "    gpt2_blocks = [\n",
    "        \"Input Embeddings\",\n",
    "        \"+ Positional Embeddings\",\n",
    "        \"↓\",\n",
    "        \"Self-Attention\",\n",
    "        \"+ Residual\",\n",
    "        \"LayerNorm\",\n",
    "        \"↓\",\n",
    "        \"MLP (GELU)\",\n",
    "        \"+ Residual\",\n",
    "        \"LayerNorm\",\n",
    "        \"↓\",\n",
    "        \"x N Layers\",\n",
    "        \"↓\",\n",
    "        \"LayerNorm\",\n",
    "        \"↓\",\n",
    "        \"Language Modeling Head\"\n",
    "    ]\n",
    "    \n",
    "    # LLaMA Architecture\n",
    "    llama_blocks = [\n",
    "        \"Input Embeddings\",\n",
    "        \"↓\",\n",
    "        \"RMSNorm\",\n",
    "        \"↓\",\n",
    "        \"Self-Attention (RoPE, GQA)\",\n",
    "        \"+ Residual\",\n",
    "        \"↓\",\n",
    "        \"RMSNorm\",\n",
    "        \"↓\",\n",
    "        \"MLP (SwiGLU)\",\n",
    "        \"+ Residual\",\n",
    "        \"↓\",\n",
    "        \"x N Layers\",\n",
    "        \"↓\",\n",
    "        \"RMSNorm\",\n",
    "        \"↓\",\n",
    "        \"Language Modeling Head\"\n",
    "    ]\n",
    "    \n",
    "    # Color maps for different components\n",
    "    gpt2_colors = [\n",
    "        'lightblue', 'lightblue',  # Embeddings\n",
    "        'white',  # Arrow\n",
    "        'lightgreen', 'lightgreen', 'lightgreen',  # Attention\n",
    "        'white',  # Arrow\n",
    "        'lightcoral', 'lightcoral', 'lightcoral',  # MLP\n",
    "        'white',  # Arrow\n",
    "        'lightyellow',  # Layers indicator\n",
    "        'white',  # Arrow\n",
    "        'lightgreen',  # Final LayerNorm\n",
    "        'white',  # Arrow\n",
    "        'lightgrey'  # LM Head\n",
    "    ]\n",
    "    \n",
    "    llama_colors = [\n",
    "        'lightblue',  # Embeddings\n",
    "        'white',  # Arrow\n",
    "        'lightgreen',  # RMSNorm\n",
    "        'white',  # Arrow\n",
    "        'lightgreen', 'lightgreen',  # Attention\n",
    "        'white',  # Arrow\n",
    "        'lightgreen',  # RMSNorm\n",
    "        'white',  # Arrow\n",
    "        'lightcoral', 'lightcoral',  # MLP\n",
    "        'white',  # Arrow\n",
    "        'lightyellow',  # Layers indicator\n",
    "        'white',  # Arrow\n",
    "        'lightgreen',  # Final RMSNorm\n",
    "        'white',  # Arrow\n",
    "        'lightgrey'  # LM Head\n",
    "    ]\n",
    "    \n",
    "    # Plot GPT-2 architecture\n",
    "    for i, (block, color) in enumerate(zip(gpt2_blocks, gpt2_colors)):\n",
    "        if block in [\"↓\"]:\n",
    "            # Draw arrow\n",
    "            ax[0].text(0.5, 1 - (i + 0.5) / len(gpt2_blocks), \"↓\", \n",
    "                     ha=\"center\", va=\"center\", fontsize=14)\n",
    "        else:\n",
    "            # Draw box\n",
    "            box = plt.Rectangle((0.1, 1 - (i + 1) / len(gpt2_blocks) * 0.9), \n",
    "                              0.8, 0.9 / len(gpt2_blocks), \n",
    "                              facecolor=color, alpha=0.8, edgecolor='black')\n",
    "            ax[0].add_patch(box)\n",
    "            # Add text\n",
    "            ax[0].text(0.5, 1 - (i + 0.5) / len(gpt2_blocks) * 0.9, block, \n",
    "                     ha=\"center\", va=\"center\", fontsize=10)\n",
    "    \n",
    "    # Plot LLaMA architecture\n",
    "    for i, (block, color) in enumerate(zip(llama_blocks, llama_colors)):\n",
    "        if block in [\"↓\"]:\n",
    "            # Draw arrow\n",
    "            ax[1].text(0.5, 1 - (i + 0.5) / len(llama_blocks), \"↓\", \n",
    "                     ha=\"center\", va=\"center\", fontsize=14)\n",
    "        else:\n",
    "            # Draw box\n",
    "            box = plt.Rectangle((0.1, 1 - (i + 1) / len(llama_blocks) * 0.9), \n",
    "                              0.8, 0.9 / len(llama_blocks), \n",
    "                              facecolor=color, alpha=0.8, edgecolor='black')\n",
    "            ax[1].add_patch(box)\n",
    "            # Add text\n",
    "            ax[1].text(0.5, 1 - (i + 0.5) / len(llama_blocks) * 0.9, block, \n",
    "                     ha=\"center\", va=\"center\", fontsize=10)\n",
    "    \n",
    "    # Set titles and adjust\n",
    "    ax[0].set_title(\"GPT-2 Architecture\", fontsize=16)\n",
    "    ax[1].set_title(\"LLaMA Architecture\", fontsize=16)\n",
    "    \n",
    "    for i in range(2):\n",
    "        ax[i].set_xlim(0, 1)\n",
    "        ax[i].set_ylim(0, 1)\n",
    "        ax[i].axis('off')\n",
    "        \n",
    "    # Annotations for key differences\n",
    "    plt.figtext(0.5, 0.02, \"Key Differences: LLaMA vs GPT-2\\n\" + \n",
    "                \"1. Pre-normalization (RMSNorm) vs Post-normalization (LayerNorm)\\n\" + \n",
    "                \"2. Rotary positional embeddings vs Learned positional embeddings\\n\" + \n",
    "                \"3. Group Query Attention vs standard Multi-head Attention\\n\" + \n",
    "                \"4. SwiGLU activation vs GELU activation\", \n",
    "                ha=\"center\", fontsize=12, bbox=dict(facecolor='lightyellow', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "visualize_architecture_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. A Simple Example: Testing the LLaMA Block\n",
    "\n",
    "Let's create a small example to test the LLaMA block and ensure that our implementation is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small test configuration\n",
    "test_config = LLaMAConfig()\n",
    "test_config.hidden_size = 64\n",
    "test_config.intermediate_size = 128\n",
    "test_config.n_heads = 4\n",
    "test_config.n_kv_heads = 2\n",
    "test_config.head_dim = 16\n",
    "test_config.n_layers = 2\n",
    "\n",
    "# Instantiate a LLaMA block\n",
    "block = LLaMABlock(test_config)\n",
    "\n",
    "# Create a random input tensor\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, test_config.hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "out = block(x)\n",
    "\n",
    "# Check output shape\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "\n",
    "# Check that the output is different from the input (transformation happened)\n",
    "print(f\"Input != Output: {not torch.allclose(x, out)}\")\n",
    "\n",
    "# Check that gradients can flow\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "print(f\"Block has parameters with gradients: {all(p.grad is not None for p in block.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Best Practices\n",
    "\n",
    "Let's summarize the key innovations in the LLaMA architecture and their impact on model performance:\n",
    "\n",
    "1. **RMSNorm**:\n",
    "   - More computationally efficient than LayerNorm\n",
    "   - Preserves vector directions, improving representation quality\n",
    "   - Combined with pre-normalization for better gradient flow\n",
    "\n",
    "2. **Rotary Positional Embeddings (RoPE)**:\n",
    "   - Encodes relative positions more effectively than absolute embeddings\n",
    "   - Allows models to generalize to longer sequences than seen during training\n",
    "   - Theoretically sound approach based on complex number rotations\n",
    "\n",
    "3. **Group Query Attention (GQA)**:\n",
    "   - Reduces memory and computation costs compared to standard multi-head attention\n",
    "   - Maintains representational power with more query heads\n",
    "   - Particularly beneficial for memory bandwidth during inference\n",
    "\n",
    "4. **SwiGLU Activation**:\n",
    "   - More expressive than GELU with gating mechanism\n",
    "   - Improves convergence and final model performance\n",
    "   - Worth the additional computational cost\n",
    "\n",
    "**Best Practices for Modern LLM Architecture**:\n",
    "\n",
    "- Use pre-normalization (norm before each sub-layer) instead of post-normalization\n",
    "- Prefer RMSNorm over LayerNorm for better scaling properties\n",
    "- Implement Rotary Positional Embeddings for better position modeling\n",
    "- Consider Group Query Attention for larger models where memory is a concern\n",
    "- Use SwiGLU or similar gated activations in feed-forward networks\n",
    "- Match the number of model dimensions to be divisible by the number of attention heads\n",
    "- For very large models, consider techniques like parameter sharing and factorized attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've explored the key architectural differences between GPT-2 and the modern LLaMA family of models. We've implemented and compared each key component:\n",
    "\n",
    "- Normalization: LayerNorm vs. RMSNorm\n",
    "- Positional encoding: Learned embeddings vs. RoPE\n",
    "- Attention: Standard MHA vs. Group Query Attention\n",
    "- Feed-forward networks: GELU vs. SwiGLU\n",
    "\n",
    "These architectural innovations have played a crucial role in scaling language models to unprecedented sizes while maintaining computational efficiency. Understanding these components provides insights into the principles behind modern LLM design and can guide future research and development in this rapidly evolving field.\n",
    "\n",
    "The LLaMA architecture represents a careful balance of efficiency and performance, incorporating the best techniques from recent research to create models that can be trained and deployed at scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
