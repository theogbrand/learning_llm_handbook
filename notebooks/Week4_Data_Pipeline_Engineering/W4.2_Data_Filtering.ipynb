{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filtering Techniques for Synthetic Data\n",
    "\n",
    "This notebook explores techniques for filtering and processing synthetic data generated by large language models. Based on the Cosmopedia project, we'll cover:\n",
    "\n",
    "1. Post-Generation Cleanup\n",
    "2. Deduplication Pipeline\n",
    "3. Decontamination Process\n",
    "4. Educational Value Classification\n",
    "5. Quality Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.8 environment at: /Users/ob1/Desktop/NVIDIA teaching kits/aisg_github_course/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.8 environment at: /Users/ob1/Desktop/NVIDIA teaching kits/aisg_github_course/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 46ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "!uv pip install datasets transformers numpy scikit-learn tf-keras evaluate\n",
    "\n",
    "# For deduplication\n",
    "!uv pip install \"datatrove[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Post-Generation Cleanup\n",
    "\n",
    "After generating synthetic data with an LLM, we often need to clean up boilerplate text such as introductions, greetings, and template artifacts that instruction-tuned models are trained to generate as chatbots. We can start with regular expressions to identify and remove these patterns, but more complex postprocessing can be applied when you have a better understanding of the data you are generating. This will differ based on the model you are using, and in general, you should iteratively edit this post-generation cleanup process based on new observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define patterns to remove common boilerplate text\n",
    "patterns = [\n",
    "    # Greeting patterns\n",
    "    r\"^Hello.*?[.!]\\s+\",\n",
    "    r\"^My name is.*?[.!]\\s+\",\n",
    "    r\"^You've just arrived.*?[.!]\\s+\",\n",
    "    \n",
    "    # Documentation patterns\n",
    "    r\"^\\*\\*Welcome, .*?[.!]\\*\\*\\s+\",\n",
    "    r\"^(\\*\\*)?Warning:.*?[.!]\\s+\",\n",
    "    r\"^We're thrilled.*?[.!]\\s+\",\n",
    "    \n",
    "    # General introductions\n",
    "    r\"^Welcome, .*?[.!]\\s+\",\n",
    "]\n",
    "\n",
    "# Compile the patterns\n",
    "compiled_patterns = [re.compile(p, flags=re.IGNORECASE|re.MULTILINE) for p in patterns]\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(sample):\n",
    "    # Keep the original text\n",
    "    sample['completion_unfiltered'] = sample['text']\n",
    "    \n",
    "    # Apply each pattern to remove boilerplate\n",
    "    for pattern in compiled_patterns:\n",
    "        sample['text'] = pattern.sub('', sample['text'].strip())\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Cleanup Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Hello there! I'm excited to share this information with you.\n",
      "\n",
      "In this article, we'll explore the fascinating world of synthetic data processing.\n",
      "Let's begin by understanding what synthetic data is and why it matters.\n",
      "\n",
      "\n",
      "Cleaned text:\n",
      " I'm excited to share this information with you.\n",
      "\n",
      "In this article, we'll explore the fascinating world of synthetic data processing.\n",
      "Let's begin by understanding what synthetic data is and why it matters.\n"
     ]
    }
   ],
   "source": [
    "# Example text with boilerplate\n",
    "example_text = \"\"\"Hello there! I'm excited to share this information with you.\n",
    "\n",
    "In this article, we'll explore the fascinating world of synthetic data processing.\n",
    "Let's begin by understanding what synthetic data is and why it matters.\n",
    "\"\"\"\n",
    "\n",
    "# Manual application of cleanup\n",
    "sample = {'text': example_text}\n",
    "cleaned = clean_text(sample)\n",
    "\n",
    "print(\"Original text:\\n\", sample['completion_unfiltered'])\n",
    "print(\"\\nCleaned text:\\n\", sample['text']) # as you can tell, the clean up process is not perfect, but it is a good start. You can likely also remove \"I'm excited to share this information with you\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a44ebf62b97432ca9374d9169ed7777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"HuggingFaceTB/cosmopedia\" # or replace with your dataset you uploaded from the previous notebook after generating data\n",
    "data = load_dataset(dataset_name, \"khanacademy\")\n",
    "cleaned_data = data.map(clean_text, num_proc=4)  # num_proc uses multiprocessing for larger datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deduplication Pipeline\n",
    "\n",
    "Deduplication is critical for removing redundant content from the dataset. We'll use the MinHash algorithm via the datatrove library, which efficiently identifies similar documents in large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatrove.pipeline.dedup import MinhashDedupSignature\n",
    "from datatrove.pipeline.dedup.minhash import (\n",
    "    MinhashConfig,\n",
    "    MinhashDedupBuckets,\n",
    "    MinhashDedupCluster,\n",
    "    MinhashDedupFilter,\n",
    ")\n",
    "from datatrove.pipeline.readers import HuggingFaceDatasetReader\n",
    "from datatrove.pipeline.tokens import TokensCounter\n",
    "from datatrove.pipeline.writers.jsonl import JsonlWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deduplication process consists of four stages:\n",
    "\n",
    "1. **Compute signatures**: Generate MinHash signatures for each document\n",
    "2. **Find matches**: Use LSH (Locality-Sensitive Hashing) to find potential matches\n",
    "3. **Create clusters**: Group similar documents into clusters\n",
    "4. **Filter duplicates**: Keep only one document per cluster\n",
    "\n",
    "Below is an example of how to set up this pipeline using datatrove:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datatrove.executor.local import LocalPipelineExecutor\n",
    "from datatrove.utils.hashing import HashConfig\n",
    "\n",
    "# Configuration for MinHash deduplication\n",
    "minhash_config = MinhashConfig(  hash_config=HashConfig(precision=64),\n",
    "    num_buckets=14,\n",
    "    hashes_per_bucket=8,)  # Default configuration\n",
    "\n",
    "# Define the output paths (adjust as needed)\n",
    "output_base_path = \"./dedup_output\"\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    " \n",
    "reader = HuggingFaceDatasetReader( # Define the reader for input data, here we are loading from HF so HuggingFaceDatasetReader reader is used. Refer to the datatrove documentation for more details on other readers. Refer to this example pipeline used for FineWeb: https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py\n",
    "    dataset=dataset_name,\n",
    "    dataset_options={\"split\": \"train\", \"name\": \"khanacademy\"}, # https://github.com/huggingface/datatrove/blob/main/tests/pipeline/test_hf_reader.py#L12\n",
    "    text_key=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Compute MinHash Signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:09:16.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36madd_task_logger\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mLaunching pipeline for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:09:16.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36mlog_pipeline\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1m\n",
      "--- ðŸ› ï¸ PIPELINE ðŸ› \n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 1\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0214d5e626f84339b90e87b3397fbd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:11:27.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.dedup.minhash\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m262\u001b[0m - \u001b[1mSorting buckets...\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:27.531\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m98\u001b[0m - \u001b[32m\u001b[1mProcessing done for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:27.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: Task 0 ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 2 minutes and 10 seconds\n",
      "\n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "    Runtime: (0.04%) 0 seconds [2.32 millisecondsÂ±0.61 milliseconds/batch]\n",
      "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 1\n",
      "    Runtime: (99.96%) 2 minutes and 10 seconds [2 minutes, 10 seconds and 656.96 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {total: 24123}\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:27.537\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: All 1 tasks ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 2 minutes and 10 seconds\n",
      "\n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "    Runtime: (0.04%) 0 seconds [2.32 millisecondsÂ±0.61 milliseconds/batch]\n",
      "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 1\n",
      "    Runtime: (99.96%) 2 minutes and 10 seconds [2 minutes, 10 seconds and 656.96 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {total: 24123}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ðŸ“‰ðŸ“‰ðŸ“‰ Stats ðŸ“‰ðŸ“‰ðŸ“‰\n",
       "\n",
       "Total Runtime: 2 minutes and 10 seconds\n",
       "\n",
       "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
       "    Runtime: (0.04%) 0 seconds [2.32 millisecondsÂ±0.61 milliseconds/batch]\n",
       "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
       "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 1\n",
       "    Runtime: (99.96%) 2 minutes and 10 seconds [2 minutes, 10 seconds and 656.96 millisecondsÂ±0 milliseconds/doc]\n",
       "    Stats: {total: 24123}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 1: Generate signatures\n",
    "stage1 = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        reader,\n",
    "        MinhashDedupSignature(output_folder=f\"{output_base_path}/signatures\", config=minhash_config),\n",
    "    ],\n",
    "    tasks=1,  # Increase for larger datasets\n",
    ")\n",
    "stage1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Find Matches Between Signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 09:11:29.537 | INFO     | datatrove.utils.logging:add_task_logger:58 - Launching pipeline for rank=0\n",
      "2025-05-14 09:11:29.539 | INFO     | datatrove.utils.logging:log_pipeline:90 - \n",
      "--- ðŸ› ï¸ PIPELINE ðŸ› \n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 2\n",
      "2025-05-14 09:11:29.541 | INFO     | datatrove.pipeline.dedup.minhash:run:359 - Running worker 1/1 on bucket 000. Hash range: [0, np.uint64(2305843009213693951)]\n",
      "2025-05-14 09:11:29.542 | INFO     | datatrove.pipeline.dedup.minhash:run:406 - Finished initializing signatures priority queue.\n",
      "2025-05-14 09:11:29.568 | SUCCESS  | datatrove.executor.base:_run_for_rank:98 - Processing done for rank=0\n",
      "2025-05-14 09:11:29.578 | INFO     | datatrove.executor.base:_run_for_rank:104 - \n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: Task 0 ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds\n",
      "\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 2\n",
      "    Runtime: (100.00%) 0 seconds [27.40 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {total_matches: 53}\n",
      "2025-05-14 09:11:29.584 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 1/14 tasks completed.\n",
      "2025-05-14 09:11:29.635 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 2/14 tasks completed.\n",
      "2025-05-14 09:11:29.725 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 3/14 tasks completed.\n",
      "2025-05-14 09:11:29.803 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 4/14 tasks completed.\n",
      "2025-05-14 09:11:29.903 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 5/14 tasks completed.\n",
      "2025-05-14 09:11:29.990 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 6/14 tasks completed.\n",
      "2025-05-14 09:11:30.105 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 7/14 tasks completed.\n",
      "2025-05-14 09:11:30.183 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 8/14 tasks completed.\n",
      "2025-05-14 09:11:30.295 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 9/14 tasks completed.\n",
      "2025-05-14 09:11:30.379 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 10/14 tasks completed.\n",
      "2025-05-14 09:11:30.479 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 11/14 tasks completed.\n",
      "2025-05-14 09:11:30.566 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 12/14 tasks completed.\n",
      "2025-05-14 09:11:30.645 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 13/14 tasks completed.\n",
      "2025-05-14 09:11:30.738 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 14/14 tasks completed.\n",
      "\u001b[32m2025-05-14 09:11:30.775\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: All 14 tasks ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds Â± 0 seconds/task\n",
      "\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 2\n",
      "    Runtime: (100.00%) 0 secondsÂ±0 seconds/task, min=0 seconds, max=0 seconds [24.64 millisecondsÂ±1.41 milliseconds/doc]\n",
      "    Stats: {total_matches: 726}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ðŸ“‰ðŸ“‰ðŸ“‰ Stats ðŸ“‰ðŸ“‰ðŸ“‰\n",
       "\n",
       "Total Runtime: 0 seconds Â± 0 seconds/task\n",
       "\n",
       "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 2\n",
       "    Runtime: (100.00%) 0 secondsÂ±0 seconds/task, min=0 seconds, max=0 seconds [24.64 millisecondsÂ±1.41 milliseconds/doc]\n",
       "    Stats: {total_matches: 726}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 2: Find matches\n",
    "!export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES # requierd for multiprocessing used in datatrove\n",
    "\n",
    "stage2 = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        MinhashDedupBuckets(\n",
    "            input_folder=f\"{output_base_path}/signatures\",\n",
    "            output_folder=f\"{output_base_path}/buckets\",\n",
    "            config=minhash_config,\n",
    "        ),\n",
    "    ],\n",
    "    depends=stage1,\n",
    "    tasks=minhash_config.num_buckets,\n",
    ")\n",
    "stage2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Create Clusters of Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:11:30.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36madd_task_logger\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mLaunching pipeline for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36mlog_pipeline\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1m\n",
      "--- ðŸ› ï¸ PIPELINE ðŸ› \n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 3\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.dedup.minhash\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m511\u001b[0m - \u001b[1mLoading dup files...\u001b[0m\n",
      "Reading dup files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 5647.26it/s]\n",
      "\u001b[32m2025-05-14 09:11:30.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.dedup.minhash\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m521\u001b[0m - \u001b[1mFinished reading dup files.\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.847\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m98\u001b[0m - \u001b[32m\u001b[1mProcessing done for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: Task 0 ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds\n",
      "\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 3\n",
      "    Runtime: (100.00%) 0 seconds [5.31 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {duplicates: 69, cluster_size: 69 [min=2, max=18, 5.75Â±5/cluster], to_remove: 57}\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.852\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: All 1 tasks ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds\n",
      "\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 3\n",
      "    Runtime: (100.00%) 0 seconds [5.31 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {duplicates: 69, cluster_size: 69 [min=2, max=18, 5.75Â±5/cluster], to_remove: 57}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ðŸ“‰ðŸ“‰ðŸ“‰ Stats ðŸ“‰ðŸ“‰ðŸ“‰\n",
       "\n",
       "Total Runtime: 0 seconds\n",
       "\n",
       "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 3\n",
       "    Runtime: (100.00%) 0 seconds [5.31 millisecondsÂ±0 milliseconds/doc]\n",
       "    Stats: {duplicates: 69, cluster_size: 69 [min=2, max=18, 5.75Â±5/cluster], to_remove: 57}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 3: Create clusters\n",
    "stage3 = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        MinhashDedupCluster(\n",
    "            input_folder=f\"{output_base_path}/buckets\",\n",
    "            output_folder=f\"{output_base_path}/remove_ids\",\n",
    "            config=minhash_config,\n",
    "        ),\n",
    "    ],\n",
    "    depends=stage2,\n",
    "    tasks=1,\n",
    ")\n",
    "stage3.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4: Filter Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:11:30.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36madd_task_logger\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mLaunching pipeline for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36mlog_pipeline\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1m\n",
      "--- ðŸ› ï¸ PIPELINE ðŸ› \n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "ðŸ”¢ - TOKENIZER: ðŸ“Š Counter\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 4\n",
      "ðŸ’½ - WRITER: ðŸ¿ Jsonl\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c45ed6fad2a48ff9603207d8e909d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:12:05.418\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m98\u001b[0m - \u001b[32m\u001b[1mProcessing done for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:12:05.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: Task 0 ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 30 seconds\n",
      "\n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "    Runtime: (0.20%) 0 seconds [2.47 millisecondsÂ±0.59 milliseconds/batch]\n",
      "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
      "ðŸ”¢ - TOKENIZER: ðŸ“Š Counter\n",
      "    Runtime: (88.77%) 27 seconds [9 seconds and 89.86 millisecondsÂ±3 seconds and 101.20 milliseconds/batch]\n",
      "    Stats: {tokens: 20953163 [min=2, max=4439, 868.60Â±388/doc]}\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 4\n",
      "    Runtime: (0.10%) 0 seconds [0.00 millisecondsÂ±0.01 milliseconds/doc]\n",
      "    Stats: {total: 24123, forwarded: 24066, dropped: 57}\n",
      "ðŸ’½ - WRITER: ðŸ¿ Jsonl\n",
      "    Runtime: (10.93%) 3 seconds [0.14 millisecondsÂ±0.28 milliseconds/doc]\n",
      "    Stats: {XXXXX.jsonl.gz: 24066, total: 24066, doc_len: 88402371 [min=5, max=18344, 3673.33Â±1446/doc], doc_len_tokens: 20952551 [min=2, max=4439, 870.63Â±386/doc]}\u001b[0m\n",
      "\u001b[32m2025-05-14 09:12:05.424\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: All 1 tasks ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 30 seconds\n",
      "\n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "    Runtime: (0.20%) 0 seconds [2.47 millisecondsÂ±0.59 milliseconds/batch]\n",
      "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
      "ðŸ”¢ - TOKENIZER: ðŸ“Š Counter\n",
      "    Runtime: (88.77%) 27 seconds [9 seconds and 89.86 millisecondsÂ±3 seconds and 101.20 milliseconds/batch]\n",
      "    Stats: {tokens: 20953163 [min=2, max=4439, 868.60Â±388/doc]}\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 4\n",
      "    Runtime: (0.10%) 0 seconds [0.00 millisecondsÂ±0.01 milliseconds/doc]\n",
      "    Stats: {total: 24123, forwarded: 24066, dropped: 57}\n",
      "ðŸ’½ - WRITER: ðŸ¿ Jsonl\n",
      "    Runtime: (10.93%) 3 seconds [0.14 millisecondsÂ±0.28 milliseconds/doc]\n",
      "    Stats: {XXXXX.jsonl.gz: 24066, total: 24066, doc_len: 88402371 [min=5, max=18344, 3673.33Â±1446/doc], doc_len_tokens: 20952551 [min=2, max=4439, 870.63Â±386/doc]}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ðŸ“‰ðŸ“‰ðŸ“‰ Stats ðŸ“‰ðŸ“‰ðŸ“‰\n",
       "\n",
       "Total Runtime: 30 seconds\n",
       "\n",
       "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
       "    Runtime: (0.20%) 0 seconds [2.47 millisecondsÂ±0.59 milliseconds/batch]\n",
       "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
       "ðŸ”¢ - TOKENIZER: ðŸ“Š Counter\n",
       "    Runtime: (88.77%) 27 seconds [9 seconds and 89.86 millisecondsÂ±3 seconds and 101.20 milliseconds/batch]\n",
       "    Stats: {tokens: 20953163 [min=2, max=4439, 868.60Â±388/doc]}\n",
       "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 4\n",
       "    Runtime: (0.10%) 0 seconds [0.00 millisecondsÂ±0.01 milliseconds/doc]\n",
       "    Stats: {total: 24123, forwarded: 24066, dropped: 57}\n",
       "ðŸ’½ - WRITER: ðŸ¿ Jsonl\n",
       "    Runtime: (10.93%) 3 seconds [0.14 millisecondsÂ±0.28 milliseconds/doc]\n",
       "    Stats: {XXXXX.jsonl.gz: 24066, total: 24066, doc_len: 88402371 [min=5, max=18344, 3673.33Â±1446/doc], doc_len_tokens: 20952551 [min=2, max=4439, 870.63Â±386/doc]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 4: Filter duplicates\n",
    "# Uncomment to run with a real dataset\n",
    "#\n",
    "stage4 = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        reader,\n",
    "        TokensCounter(),  # Count tokens before and after deduplication\n",
    "        MinhashDedupFilter(\n",
    "            input_folder=f\"{output_base_path}/remove_ids\",\n",
    "            exclusion_writer=JsonlWriter(f\"{output_base_path}/removed\"),\n",
    "        ),\n",
    "        JsonlWriter(output_folder=f\"{output_base_path}/deduplicated_output\"),\n",
    "    ],\n",
    "    depends=stage3,\n",
    "    tasks=1,  # Match the number of tasks from stage 1\n",
    ")\n",
    "stage4.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deduplicated samples: 24066\n",
      "Number of removed samples: 57\n",
      "\n",
      "Sample statistics:\n",
      "Deduplicated avg length: 3673.33\n",
      "Removed avg length: 48.54\n",
      "\n",
      "Example removed samples:\n",
      "\n",
      "Removed sample 1:\n",
      " Let's dive into our next lesson: adding 8 + ...\n",
      "\n",
      "Removed sample 2:\n",
      " Now that we've learned how to multiply 1-digit numbers by 10, 100, and 1...\n",
      "\n",
      "Removed sample 3:\n",
      " **Topic C: Lessons 26-2...\n",
      "\n",
      "Removed sample 4:\n",
      " Dividing Whole Numbers by 1...\n",
      "\n",
      "Removed sample 5:\n",
      " Welcome to Unit 11: Operations and Algebra < 1...\n",
      "\n",
      "Removed sample 6:\n",
      " Welcome to Unit 17: Operations and Algebra 218-2...\n",
      "\n",
      "Removed sample 7:\n",
      " Welcome to Unit 17: Operations and Algebra 218-2...\n",
      "\n",
      "Removed sample 8:\n",
      " Welcome to Unit 18: Operations and Algebra 222-2...\n",
      "\n",
      "Removed sample 9:\n",
      " When working with powers of 1...\n",
      "\n",
      "Removed sample 10:\n",
      " Welcome to Unit 20: Operations and Algebra ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "output_base_path = \"dedup_output\"\n",
    "# Load deduplicated samples created from pipeline above\n",
    "dedup_path = Path(f\"{output_base_path}/deduplicated_output\")\n",
    "dedup_files = list(dedup_path.glob(\"*.jsonl.gz\"))\n",
    "dedup_samples = []\n",
    "for file in dedup_files:\n",
    "    with gzip.open(file, 'rt') as f:\n",
    "        for line in f:\n",
    "            dedup_samples.append(json.loads(line))\n",
    "\n",
    "# Load removed samples\n",
    "removed_path = Path(f\"{output_base_path}/removed\")\n",
    "removed_files = list(removed_path.glob(\"*.jsonl.gz\"))\n",
    "removed_samples = []\n",
    "for file in removed_files:\n",
    "    with gzip.open(file, 'rt') as f:\n",
    "        for line in f:\n",
    "            removed_samples.append(json.loads(line))\n",
    "\n",
    "print(f\"Number of deduplicated samples: {len(dedup_samples)}\")\n",
    "print(f\"Number of removed samples: {len(removed_samples)}\")\n",
    "\n",
    "# Compare some statistics\n",
    "if dedup_samples and removed_samples:\n",
    "    print(\"\\nSample statistics:\")\n",
    "    print(f\"Deduplicated avg length: {sum(len(s['text']) for s in dedup_samples) / len(dedup_samples):.2f}\")\n",
    "    print(f\"Removed avg length: {sum(len(s['text']) for s in removed_samples) / len(removed_samples):.2f}\")\n",
    "    \n",
    "    # Show a few examples of removed samples\n",
    "    print(\"\\nExample removed samples:\")\n",
    "    for i, sample in enumerate(removed_samples[:10]):\n",
    "        print(f\"\\nRemoved sample {i+1}:\")\n",
    "        print(sample['text'][:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decontamination Process\n",
    "\n",
    "Decontamination prevents data leakage by removing content that overlaps with evaluation benchmarks. This is especially important for synthetically generated data to ensure fair evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization and N-gram Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): # find out what and how this function does!\n",
    "    \"\"\"Normalize text by removing diacritics and tokenize.\"\"\"\n",
    "    text = \"\".join(c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\")\n",
    "    tokens = re.findall(\"\\\\w+\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_ngrams(tokens, n): # analyze what this function does!\n",
    "    \"\"\"Generate n-grams from tokens.\"\"\"\n",
    "    return set(zip(*[tokens[i:] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding N-gram Matches with Benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_ngrams_batch(batch, eval_ngrams, eval_datasets, eval_texts, ngram_len):\n",
    "    \"\"\"Find contaminated samples based on n-grams.\"\"\"\n",
    "    new_batch = {\"completion\": [], \"ngram\": [], \"bench_name\": [], \"bench_text\": []}\n",
    "    for completion in batch[\"text\"]:\n",
    "        tokens = tokenize(completion)\n",
    "        ngrams = get_ngrams(tokens, ngram_len)\n",
    "        for ngram in ngrams:\n",
    "            if ngram in eval_ngrams:\n",
    "                idx = eval_ngrams[ngram]\n",
    "                new_batch[\"completion\"].append(completion)\n",
    "                new_batch[\"ngram\"].append(ngram)\n",
    "                new_batch[\"bench_name\"].append(eval_datasets[idx])\n",
    "                new_batch[\"bench_text\"].append(eval_texts[idx])\n",
    "                break\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Matching for Better Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_strings(string1, string2):\n",
    "    \"\"\"Find matching parts between two strings.\"\"\"\n",
    "    matcher = difflib.SequenceMatcher(None, string1.lower(), string2.lower(), autojunk=False)\n",
    "    matching_blocks = matcher.get_matching_blocks()\n",
    "    matches = []\n",
    "    for block in matching_blocks:\n",
    "        start_a, start_b, length = block\n",
    "        if length > 5:  # Ignore very short matches\n",
    "            match = string1[start_a:start_a + length]\n",
    "            matches.append(match)\n",
    "    return matches\n",
    "\n",
    "def add_match_stats(example):\n",
    "    \"\"\"Add match statistics to identify contamination severity.\"\"\"\n",
    "    gen_text = \" \".join(tokenize(example[\"completion\"]))\n",
    "    bench_text = \" \".join(tokenize(example[\"bench_text\"]))\n",
    "    matching_parts = diff_strings(gen_text, bench_text)\n",
    "    match = \" \".join(\"\".join(matching_parts).split())\n",
    "    example[\"diff\"] = matching_parts\n",
    "    example[\"diff_ratio\"] = len(match) / len(bench_text) if len(bench_text) > 0 else 0\n",
    "    example[\"diff_length\"] = len(match)\n",
    "    example[\"longest_diff_part\"] = max(matching_parts, key=len, default=\"\")\n",
    "    example[\"longest_diff_part_length\"] = len(example[\"longest_diff_part\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Decontamination Pipeline\n",
    "\n",
    "The function below demonstrates the complete decontamination process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontaminate_dataset(train_dataset, eval_dataset, ngram_length=10, diff_threshold=0.5, num_proc=4):\n",
    "    \"\"\"\n",
    "    Decontaminate a dataset by removing examples that have significant overlap with evaluation benchmarks.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: Dataset to decontaminate\n",
    "        eval_dataset: Evaluation benchmark dataset\n",
    "        ngram_length: Length of n-grams to use for initial matching\n",
    "        diff_threshold: Similarity threshold for filtering (0.5 = 50% match)\n",
    "        num_proc: Number of processes for parallel processing\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (decontaminated dataset, contamination report)\n",
    "    \"\"\"\n",
    "    # Build n-gram index from evaluation data\n",
    "    eval_ngrams, eval_datasets, eval_texts = {}, [], []\n",
    "    \n",
    "    print(\"Building n-gram index from evaluation data...\")\n",
    "    for example in tqdm(eval_dataset):\n",
    "        tokens = tokenize(example[\"text\"])\n",
    "        ngrams = get_ngrams(tokens, ngram_length)\n",
    "        if ngrams:\n",
    "            idx = len(eval_texts)\n",
    "            eval_ngrams.update(zip(ngrams, [idx] * len(ngrams)))\n",
    "            eval_datasets.append(example.get(\"task_name\", \"unknown\"))\n",
    "            eval_texts.append(example[\"text\"])\n",
    "            \n",
    "    print(f\"Created n-gram index with {len(eval_ngrams)} unique n-grams\")\n",
    "    \n",
    "    # Find contamination candidates\n",
    "    print(\"Finding contamination candidates...\")\n",
    "    contamination_report = train_dataset.map(\n",
    "        lambda batch: retrieve_ngrams_batch(batch, eval_ngrams, eval_datasets, eval_texts, ngram_length),\n",
    "        batched=True, batch_size=1000, num_proc=num_proc, remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Add detailed matching statistics\n",
    "    print(\"Analyzing match details...\")\n",
    "    contamination_report = contamination_report.map(\n",
    "        lambda example: add_match_stats(example), num_proc=num_proc\n",
    "    )\n",
    "    \n",
    "    # Filter based on similarity threshold\n",
    "    print(\"Filtering contaminated examples...\")\n",
    "    contamination_report_filtered = contamination_report.filter(lambda x: x[\"diff_ratio\"] > diff_threshold)\n",
    "    \n",
    "    # Create decontaminated dataset\n",
    "    contaminated_completions = set(contamination_report_filtered[\"completion\"])\n",
    "    filtered_data = train_dataset.filter(lambda x: x[\"text\"] not in contaminated_completions)\n",
    "    \n",
    "    print(f\"Found {len(contamination_report_filtered)} contaminated examples out of {len(train_dataset)}\")\n",
    "    print(f\"Decontaminated dataset has {len(filtered_data)} examples\")\n",
    "    \n",
    "    return filtered_data, contamination_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building n-gram index from evaluation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10badebbaa2346a1a50f523eb8a2b371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created n-gram index with 93444 unique n-grams\n",
      "Finding contamination candidates...\n",
      "Analyzing match details...\n",
      "Filtering contaminated examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef7150ac0d04a81b7c1b94cd5b42cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 contaminated examples out of 24066\n",
      "Decontaminated dataset has 24062 examples\n"
     ]
    }
   ],
   "source": [
    "# Example usage (uncomment to run with real datasets)\n",
    "# train_dataset_name = \"HuggingFaceTB/cosmopedia-sample\" # or replace with \n",
    "train_dataset_name = \"dedup_output/deduplicated_output\"\n",
    "# eval_dataset_name = \"VGraf/unseen_wildchat_eval_subset\"\n",
    "eval_dataset_name = \"davanstrien/cosmopedia_sample\" # has 2% of 200 samples from KhanAcademy, so should contain 4 overlapping examples\n",
    "# \n",
    "train_data = load_dataset(train_dataset_name, split=\"train\")\n",
    "eval_data = load_dataset(eval_dataset_name, split=\"train\")\n",
    "\n",
    "decontaminated_data, contamination_report = decontaminate_dataset(\n",
    "    train_data, \n",
    "    eval_data,\n",
    "    ngram_length=10,\n",
    "    diff_threshold=0.5,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Scoring for educational value\n",
    "\n",
    "Classifying educational value helps identify high-quality content for learning purposes. We will first show how we can use a trained educational classifier to filter out low-quality content. \n",
    "\n",
    "Since we have not yet covered how finetuning models work, we will provide sample code for how to fine-tune a BERT model to score educational value, and you can come back to this section later in the course when you have covered how to finetune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_educational_dataset(dataset_name, target_column=\"score\"):\n",
    "#     \"\"\"Prepare a dataset for educational value classification training.\"\"\"\n",
    "#     # Load dataset with annotations\n",
    "#     dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "#     # Normalize scores to 0-5 range\n",
    "#     dataset = dataset.map(\n",
    "#         lambda x: {target_column: np.clip(int(x[target_column]), 0, 5)},\n",
    "#         num_proc=4,\n",
    "#     )\n",
    "    \n",
    "#     # Convert to classification labels\n",
    "#     from datasets import ClassLabel\n",
    "#     dataset = dataset.cast_column(\n",
    "#         target_column, ClassLabel(names=[str(i) for i in range(6)])\n",
    "#     )\n",
    "    \n",
    "#     # Split into train and test sets\n",
    "#     dataset = dataset.train_test_split(\n",
    "#         train_size=0.9, seed=42, stratify_by_column=target_column\n",
    "#     )\n",
    "    \n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are predicting if a text is educational or not, so the evaluation metric we will use is precision, recall, f1, and accuracy. Possible output values range from 0 to 5, and we can compare outputs to the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     \"\"\"Compute evaluation metrics for educational value classification.\"\"\"\n",
    "#     precision_metric = evaluate.load(\"precision\")\n",
    "#     recall_metric = evaluate.load(\"recall\")\n",
    "#     f1_metric = evaluate.load(\"f1\")\n",
    "#     accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "#     logits, labels = eval_pred\n",
    "#     preds = np.round(logits.squeeze()).clip(0, 5).astype(int)\n",
    "#     labels = np.round(labels.squeeze()).astype(int)\n",
    "    \n",
    "#     precision = precision_metric.compute(\n",
    "#         predictions=preds, references=labels, average=\"macro\"\n",
    "#     )[\"precision\"]\n",
    "#     recall = recall_metric.compute(\n",
    "#         predictions=preds, references=labels, average=\"macro\"\n",
    "#     )[\"recall\"]\n",
    "#     f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "#     accuracy = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    \n",
    "#     from sklearn.metrics import classification_report, confusion_matrix\n",
    "#     report = classification_report(labels, preds)\n",
    "#     cm = confusion_matrix(labels, preds)\n",
    "#     print(\"Validation Report:\\n\" + report)\n",
    "#     print(\"Confusion Matrix:\\n\" + str(cm))\n",
    "\n",
    "#     return {\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#         \"f1_macro\": f1,\n",
    "#         \"accuracy\": accuracy,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_educational_classifier(dataset, base_model_name, target_column=\"score\", output_dir=\"./edu_bert_model\"):\n",
    "#     \"\"\"Train a BERT model for educational value classification.\"\"\"\n",
    "#     # Load model and tokenizer\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         base_model_name,\n",
    "#         num_labels=1,  # Regression task\n",
    "#         classifier_dropout=0.0,\n",
    "#         hidden_dropout_prob=0.0,\n",
    "#         output_hidden_states=False,\n",
    "#     )\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\n",
    "#         base_model_name,\n",
    "#         model_max_length=min(model.config.max_position_embeddings, 512),\n",
    "#     )\n",
    "#     if not tokenizer.pad_token:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "#     # Preprocess data\n",
    "#     def preprocess(examples):\n",
    "#         batch = tokenizer(examples[\"text\"], truncation=True)\n",
    "#         batch[\"labels\"] = np.float32(examples[target_column])\n",
    "#         return batch\n",
    "    \n",
    "#     processed_dataset = dataset.map(preprocess, batched=True)\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "#     # Freeze embedding layers to speed up training\n",
    "#     for param in model.bert.embeddings.parameters():\n",
    "#         param.requires_grad = False\n",
    "#     for param in model.bert.encoder.parameters():\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "#     # Training arguments\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         eval_strategy=\"steps\",\n",
    "#         save_strategy=\"steps\",\n",
    "#         eval_steps=1000,\n",
    "#         save_steps=1000,\n",
    "#         logging_steps=100,\n",
    "#         learning_rate=3e-4,\n",
    "#         num_train_epochs=5,\n",
    "#         seed=0,\n",
    "#         per_device_train_batch_size=32,\n",
    "#         per_device_eval_batch_size=64,\n",
    "#         eval_on_start=True,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"f1_macro\",\n",
    "#         greater_is_better=True,\n",
    "#     )\n",
    "    \n",
    "#     # Initialize trainer\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=processed_dataset[\"train\"],\n",
    "#         eval_dataset=processed_dataset[\"test\"],\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "    \n",
    "#     # Train model\n",
    "#     trainer.train()\n",
    "#     trainer.save_model(output_dir)\n",
    "    \n",
    "#     return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment to run with real data)\n",
    "# \n",
    "# dataset_name = \"HuggingFaceFW/fineweb-edu-llama3-annotations\"  \n",
    "# base_model_name = \"Snowflake/snowflake-arctic-embed-m\"\n",
    "\n",
    "# # Prepare dataset\n",
    "# edu_dataset = prepare_educational_dataset(dataset_name, target_column=\"score\")\n",
    "# \n",
    "# # Train model\n",
    "# model, tokenizer = train_educational_classifier(\n",
    "#     edu_dataset, \n",
    "#     base_model_name, \n",
    "#     target_column=\"score\",\n",
    "#     output_dir=\"./edu_bert_model\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def classify_educational_value(dataset, model_path, batch_size=32):\n",
    "    \"\"\"Apply educational value classification to a dataset.\"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Define preprocessing function\n",
    "    def predict_batch(batch):\n",
    "        # Tokenize and prepare inputs\n",
    "        inputs = tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Process predictions\n",
    "        scores = outputs.logits.squeeze().cpu().numpy()\n",
    "        scores = np.clip(np.round(scores), 0, 5).astype(int)\n",
    "        \n",
    "        batch[\"educational_score\"] = scores\n",
    "        return batch\n",
    "    \n",
    "    # Apply model to dataset\n",
    "    scored_dataset = dataset.map(\n",
    "        predict_batch,\n",
    "        batched=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return scored_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715eafa936f342e9bfa9fd57cc99ba22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a36ec7e03e4a598131e9d0876b9a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 200 examples\n",
      "High-quality subset: 114 examples\n"
     ]
    }
   ],
   "source": [
    " \n",
    "dataset_to_score = load_dataset(\"davanstrien/cosmopedia_sample\", split=\"train\")\n",
    "model_path = \"HuggingFaceFW/fineweb-edu-classifier\"  # or use your locally trained edu BERT model \"./edu_bert_model\"\n",
    "\n",
    "scored_dataset = classify_educational_value(dataset_to_score, model_path)\n",
    "\n",
    "# Example of filtering by educational value\n",
    "high_quality = scored_dataset.filter(lambda x: x[\"educational_score\"] >= 4)\n",
    "print(f\"Original dataset: {len(scored_dataset)} examples\")\n",
    "print(f\"High-quality subset: {len(high_quality)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High Quality Examples (score >= 4):\n",
      "\n",
      "Example 1:\n",
      "Score: 4\n",
      "Text:  The terms \"infimum\" and \"supremum\" are fundamental concepts in mathematics, particularly within the branch of order theory. They are often used when discussing partially ordered sets, which refer to ...\n",
      "\n",
      "Example 2:\n",
      "Score: 5\n",
      "Text:  Hello young mathematicians! Today, let's learn about circles and some interesting properties they have.\n",
      "\n",
      "First, do you know what a circle is? A circle is a shape where all points are equidistant (the...\n",
      "\n",
      "Example 3:\n",
      "Score: 4\n",
      "Text:  Absolute value inequalities can indeed be tricky, especially when they involve more than one absolute value expression. The key to solving these types of problems lies in considering all possible cas...\n",
      "\n",
      "Example 4:\n",
      "Score: 5\n",
      "Text:  The fundamental principle of counting is a crucial concept in combinatorics, a branch of mathematics concerned with counting, arrangement, and selection of elements from finite sets. This principle, ...\n",
      "\n",
      "Example 5:\n",
      "Score: 4\n",
      "Text:  Sure, I'll do my best to create an educational piece based on the given snippet that would be appropriate for grade-school students.\n",
      "\n",
      "---\n",
      "\n",
      "Hello young mathematicians! Today, let's explore a fun way t...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c68422667840f09e99777c4867ab1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Low Quality Examples (score < 4):\n",
      "\n",
      "Example 1:\n",
      "Score: 3\n",
      "Text:  Category theory is a branch of mathematics that deals with abstract structure and relationships between mathematical concepts. At its core, it studies how various mathematical structures can be viewe...\n",
      "\n",
      "Example 2:\n",
      "Score: 2\n",
      "Text:  Introduction\n",
      "\n",
      "In contemporary sociological discourse, the economy is increasingly viewed as a complex system embedded within broader social structures (Polanyi, 1944; Granovetter, 1985). This perspec...\n",
      "\n",
      "Example 3:\n",
      "Score: 3\n",
      "Text:  **Relating E-field and B-field Strengths in Electromagnetic Wave Propagation**\n",
      "\n",
      "In the realm of electromagnetic wave (EM) propagation, a fundamental relationship exists between the electric ($E$) and...\n",
      "\n",
      "Example 4:\n",
      "Score: 3\n",
      "Text:  Hello Grade-Schoolers! Today we're going to learn about something really cool - how to tell if a number is \"divisible\" by another number. This means we can figure out whether one number goes into ano...\n",
      "\n",
      "Example 5:\n",
      "Score: 3\n",
      "Text:  **Unit: Midterm Elections**\n",
      "\n",
      "In the grand tapestry of American politics, presidential elections often steal the limelight, casting long shadows over their lesser-known counterparts: midterm elections...\n"
     ]
    }
   ],
   "source": [
    "# Print 5 high quality examples\n",
    "print(\"\\nHigh Quality Examples (score >= 4):\")\n",
    "for i, example in enumerate(high_quality.select(range(5))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Score: {example['educational_score']}\")\n",
    "    print(f\"Text: {example['text'][:200]}...\")  # Print first 200 chars\n",
    "\n",
    "# Print 5 low quality examples\n",
    "low_quality = scored_dataset.filter(lambda x: x[\"educational_score\"] < 4)\n",
    "print(\"\\nLow Quality Examples (score < 4):\")\n",
    "for i, example in enumerate(low_quality.select(range(5))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Score: {example['educational_score']}\")\n",
    "    print(f\"Text: {example['text'][:200]}...\")  # Print first 200 chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Evaluation\n",
    "\n",
    "Evaluating the quality of processed data is crucial to ensure its effectiveness. We can use benchmark tests to assess model performance when trained on our filtered data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Testing with lighteval\n",
    "\n",
    "The Cosmopedia project uses the `lighteval` library to evaluate models on standard benchmarks. Below is an example of how to set up and run these evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of installing lighteval (uncomment to run)\n",
    "# \n",
    "# !git clone https://github.com/huggingface/lighteval.git\n",
    "# !cd lighteval && pip install '.[accelerate,quantization,adapters]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary benchmark used for evaluation is:\n",
    "\n",
    "**HellaSwag**: A challenging common sense reasoning benchmark that tests a model's ability to complete a sentence with the most natural continuation. This is particularly effective for measuring how well a model has learned common sense knowledge and language fluency from the training data.\n",
    "\n",
    "Using a single benchmark like HellaSwag provides an efficient way to measure the impact of different filtering steps without the complexity of multiple benchmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command to run HellaSwag evaluation (uncomment to run)\n",
    "# MODEL=\"your_model_checkpoint\"\n",
    "# OUTPUT_DIR=\"./eval_results\"\n",
    "# \n",
    "# !accelerate launch --num_processes=1 --main_process_port=29600 \\\n",
    "#     \"lighteval/run_evals_accelerate.py\" \\\n",
    "#     --model_args=\"pretrained=$MODEL\" \\\n",
    "#     --custom_tasks \"lighteval_tasks.py\" \\\n",
    "#     --output_dir $OUTPUT_DIR \\\n",
    "#     --override_batch_size 16 \\\n",
    "#     --tasks \"custom|hellaswag|0|1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Impact of Filtering Steps\n",
    "\n",
    "To understand the effectiveness of different filtering approaches, you can train models on different versions of your dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Raw dataset**: Generated data without any filtering\n",
    "2. **Cleaned dataset**: After boilerplate removal\n",
    "3. **Deduplicated dataset**: After removing duplicates\n",
    "4. **Decontaminated dataset**: After removing benchmark overlap\n",
    "5. **High-quality dataset**: After filtering by educational value\n",
    "\n",
    "Then evaluate each model on the benchmarks to compare performance differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example comparative analysis for HellaSwag scores (pseudocode)\n",
    "\n",
    "results = {\n",
    "    \"raw\": 0.45,           # Hypothetical HellaSwag accuracy \n",
    "    \"cleaned\": 0.47,       # After boilerplate removal\n",
    "    \"deduplicated\": 0.49,  # After removing duplicates\n",
    "    \"decontaminated\": 0.51, # After removing benchmark overlap\n",
    "    \"high_quality\": 0.53,  # After filtering by educational value\n",
    "}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Plot results\n",
    "df = pd.DataFrame({'HellaSwag Score': results.values()}, index=results.keys())\n",
    "\n",
    "ax = df.plot(kind=\"bar\", figsize=(10, 6), color='steelblue')\n",
    "ax.set_title(\"Impact of Filtering Steps on HellaSwag Performance\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Dataset Version\")\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(results.values()):\n",
    "    ax.text(i, v + 0.01, f\"{v:.2f}\", ha='center')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covers essential data filtering techniques for synthetic data:\n",
    "\n",
    "1. **Post-Generation Cleanup**: Remove boilerplate and template artifacts with regex\n",
    "2. **Deduplication**: Eliminate redundant content with MinHash and LSH\n",
    "3. **Decontamination**: Prevent data leakage by removing evaluation benchmark overlap\n",
    "4. **Educational Value Classification**: Score and filter content by educational quality\n",
    "5. **Quality Evaluation**: Assess the impact of filtering steps on HellaSwag performance\n",
    "\n",
    "These techniques help create high-quality datasets for training language models. By measuring performance on HellaSwag after each filtering step, we can quantify the impact of our data processing pipeline and ensure we're creating the most effective training data possible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
