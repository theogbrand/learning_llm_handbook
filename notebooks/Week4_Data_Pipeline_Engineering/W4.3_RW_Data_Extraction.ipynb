{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scale PDF Processing Pipeline - Educational Example\n",
    "\n",
    "This notebook provides a simplified version of the web-scale PDF processing pipeline based on the [Web Scale PDF Processing Pipeline](https://github.com/aisingapore/web_scale_pdf_processing_pipeline) pipeline used to extract educational web resources for pretraining large language models.\n",
    "\n",
    "We'll break down the workflow into the following steps:\n",
    "1. Setup and Environment\n",
    "2. PDF Collection & Filtering\n",
    "3. Quality Filtering\n",
    "4. OCR Text Extraction using Marker\n",
    "5. Text Post-processing\n",
    "\n",
    "This simplified version uses a single GPU without distributed computing or Spark, perfect for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "First, let's install the required packages. The main package we'll need is [Marker](https://github.com/VikParuchuri/marker), a GPU-accelerated OCR tool for extracting text from PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install marker-pdf pypdf pandas opencv-python openai transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from marker.convert import convert_single_pdf\n",
    "from marker.models import load_all_models\n",
    "from marker.output import save_markdown\n",
    "import tempfile\n",
    "import shutil\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# Set up directories\n",
    "input_dir = \"./pdf_samples\"  # Directory containing your PDF files\n",
    "output_dir = \"./output\"      # Directory for outputs\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Collection & Filtering\n",
    "\n",
    "In this step, we'll find all PDF files in a directory and filter them based on basic criteria like page count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_pdf_files(directory):\n",
    "    \"\"\"List all PDF files in the given directory.\"\"\"\n",
    "    pdf_files = glob.glob(os.path.join(directory, \"*.pdf\"))\n",
    "    return pdf_files\n",
    "\n",
    "def get_pdf_page_count(pdf_path):\n",
    "    \"\"\"Get the number of pages in a PDF file.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            return len(pdf_reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_pdf_page_count for {pdf_path}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def filter_pdfs_by_page_count(pdf_files, min_pages=2, max_pages=500):\n",
    "    \"\"\"Filter PDFs by page count.\"\"\"\n",
    "    filtered_pdfs = []\n",
    "    for pdf_path in pdf_files:\n",
    "        page_count = get_pdf_page_count(pdf_path)\n",
    "        if min_pages <= page_count <= max_pages:\n",
    "            filtered_pdfs.append((pdf_path, page_count))\n",
    "    return filtered_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all PDF files\n",
    "pdf_files = list_pdf_files(input_dir)\n",
    "print(f\"Found {len(pdf_files)} PDF files\")\n",
    "\n",
    "# Filter PDFs by page count\n",
    "filtered_pdfs = filter_pdfs_by_page_count(pdf_files)\n",
    "print(f\"After filtering by page count: {len(filtered_pdfs)} PDFs\")\n",
    "\n",
    "# Create a DataFrame with the filtered PDFs\n",
    "pdf_df = pd.DataFrame(filtered_pdfs, columns=[\"pdf_path\", \"page_count\"])\n",
    "pdf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quality Filtering\n",
    "\n",
    "This step determines if a PDF contains relevant content for our needs. We'll implement two approaches:\n",
    "\n",
    "1. **Rule-based filtering**: A simple approach using basic text metrics\n",
    "2. **LLM-based filtering**: More sophisticated approach using language models\n",
    "   - API-based models (OpenAI's GPT-4o-mini)\n",
    "   - Open source models (Llama-3 8B)\n",
    "   \n",
    "You can choose which filtering method to use based on your needs and available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_basic_text(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyPDF (not as good as Marker but faster for initial filtering).\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            # Only extract from first few pages for quick filtering\n",
    "            max_pages = min(5, len(pdf_reader.pages))\n",
    "            text = \"\"\n",
    "            for i in range(max_pages):\n",
    "                text += pdf_reader.pages[i].extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_basic_text for {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 1. RULE-BASED FILTERING\n",
    "\n",
    "def is_relevant_rule_based(text, keywords=None, min_text_length=100):\n",
    "    \"\"\"Simple relevance check based on text length and optional keywords.\"\"\"\n",
    "    if len(text) < min_text_length:\n",
    "        return False\n",
    "        \n",
    "    if keywords:\n",
    "        return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# 2. LLM-BASED FILTERING\n",
    "\n",
    "# 2.1 OpenAI API Model (GPT-4o-mini)\n",
    "def is_relevant_openai(text, api_key=None, model=\"gpt-4o-mini\", domain=\"education\"):\n",
    "    \"\"\"Use OpenAI's API to determine if a PDF is relevant for a specific domain.\"\"\"\n",
    "    try:\n",
    "        import openai\n",
    "        \n",
    "        # You would need to set your API key\n",
    "        if api_key:\n",
    "            openai.api_key = api_key\n",
    "        elif os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "        else:\n",
    "            print(\"Warning: No OpenAI API key provided. Skipping OpenAI filtering.\")\n",
    "            return True  # Default to True if no API key\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        \n",
    "        # Truncate the text to avoid excessive token usage\n",
    "        truncated_text = text[:15000]  # Using first 15k chars, adjust as needed\n",
    "        \n",
    "        # Create the prompt based on the domain\n",
    "        prompt = f\"\"\"You are an expert content evaluator. Your task is to determine if the following document is relevant for {domain} content.\n",
    "\n",
    "Here is a sample of the document:\n",
    "\n",
    "<document_sample>\n",
    "{truncated_text}\n",
    "</document_sample>\n",
    "\n",
    "Is this document relevant for {domain} purposes? Answer only with 'true' or 'false'.\"\"\"\n",
    "        \n",
    "        # Call the OpenAI API\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You help determine if documents are relevant for specific domains.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Extract the answer\n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        is_relevant = \"true\" in answer\n",
    "        \n",
    "        return is_relevant\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in is_relevant_openai: {str(e)}\")\n",
    "        return True  # Default to True in case of error\n",
    "\n",
    "# 2.2 Open Source Model (Llama-3)\n",
    "def is_relevant_llama(text, domain=\"education\"):\n",
    "    \"\"\"Use Llama-3 to determine if a PDF is relevant for a specific domain.\"\"\"\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        import torch\n",
    "        \n",
    "        # Truncate the text to avoid excessive token usage\n",
    "        truncated_text = text[:5000]  # Smaller context for local models\n",
    "        \n",
    "        # Load model and tokenizer (cached after first run)\n",
    "        model_name = \"meta-llama/Llama-3-8b-instruct\"  # Or any other appropriate model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = f\"\"\"<|system|>\n",
    "You are an expert content evaluator. You determine if documents are relevant for specific domains.\n",
    "<|user|>\n",
    "Is the following document relevant for {domain} content? Answer only with 'true' or 'false'.\n",
    "\n",
    "{truncated_text}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode the output\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the assistant's response (after the prompt)\n",
    "        assistant_response = response.split(\"<|assistant|>\")[-1].strip().lower()\n",
    "        \n",
    "        is_relevant = \"true\" in assistant_response\n",
    "        return is_relevant\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in is_relevant_llama: {str(e)}\")\n",
    "        return True  # Default to True in case of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text for filtering\n",
    "pdf_df[\"ocr_text\"] = pdf_df[\"pdf_path\"].apply(extract_basic_text)\n",
    "\n",
    "# Choose your filtering method\n",
    "filtering_method = \"rule-based\"  # Options: \"rule-based\", \"openai\", \"llama\"\n",
    "\n",
    "# Parameters for filtering\n",
    "domain = \"education\"  # Target domain for content\n",
    "keywords = [\"education\", \"research\", \"study\", \"learning\"]  # For rule-based filtering\n",
    "\n",
    "# Apply the selected filtering method\n",
    "if filtering_method == \"rule-based\":\n",
    "    print(\"Using rule-based filtering...\")\n",
    "    pdf_df[\"is_relevant\"] = pdf_df[\"ocr_text\"].apply(lambda text: is_relevant_rule_based(text, keywords))\n",
    "\n",
    "elif filtering_method == \"openai\":\n",
    "    # You would need to set your API key: os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "    print(\"Using OpenAI API filtering...\")\n",
    "    # Check a small sample first (comment out for full dataset)\n",
    "    sample_size = min(3, len(pdf_df))\n",
    "    pdf_df = pdf_df.head(sample_size)  # For testing API usage\n",
    "    pdf_df[\"is_relevant\"] = pdf_df[\"ocr_text\"].apply(lambda text: is_relevant_openai(text, domain=domain))\n",
    "\n",
    "elif filtering_method == \"llama\":\n",
    "    print(\"Using Llama-3 filtering...\")\n",
    "    # Comment out the next line for the full dataset\n",
    "    pdf_df = pdf_df.head(min(3, len(pdf_df)))  # Small sample for testing\n",
    "    pdf_df[\"is_relevant\"] = pdf_df[\"ocr_text\"].apply(lambda text: is_relevant_llama(text, domain=domain))\n",
    "\n",
    "# Filter relevant PDFs\n",
    "relevant_pdfs = pdf_df[pdf_df[\"is_relevant\"] == True]\n",
    "print(f\"After relevance check: {len(relevant_pdfs)} PDFs out of {len(pdf_df)} total\")\n",
    "relevant_pdfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OCR Text Extraction using Marker\n",
    "\n",
    "Now we'll use Marker, a GPU-accelerated OCR tool, to extract high-quality text from the PDFs. This is the core of the pipeline and the part that benefits most from GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Marker models (only once)\n",
    "print(\"Loading Marker models (this may take a while)...\")\n",
    "model_lst = load_all_models()\n",
    "print(\"Models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_marker(pdf_path, model_lst):\n",
    "    \"\"\"Process a PDF file using Marker's convert_single_pdf function and return the extracted text.\"\"\"\n",
    "    try:\n",
    "        # Convert PDF to text using Marker\n",
    "        full_text, images, out_meta = convert_single_pdf(pdf_path, model_lst)\n",
    "        \n",
    "        # Generate a unique filename\n",
    "        unique_filename = f\"{uuid.uuid4().hex}_{os.path.basename(pdf_path)}\"\n",
    "        \n",
    "        # Save the markdown to a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            md_dir = save_markdown(temp_dir, unique_filename, full_text, images, out_meta)\n",
    "            \n",
    "            # Construct the path to the .md file inside the created directory\n",
    "            md_filename = os.path.basename(md_dir) + \".md\"\n",
    "            md_file_path = os.path.join(md_dir, md_filename)\n",
    "            \n",
    "            # Read the content of the markdown file\n",
    "            with open(md_file_path, \"r\", encoding=\"utf-8\") as md_file:\n",
    "                md_content = md_file.read()\n",
    "        \n",
    "        return md_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {pdf_path}: {str(e)}\")\n",
    "        return f\"Error: Failed to process PDF {pdf_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample of PDFs (for quicker execution in this educational example)\n",
    "# In practice, you might process all relevant PDFs\n",
    "sample_size = min(5, len(relevant_pdfs))\n",
    "sample_pdfs = relevant_pdfs.head(sample_size)\n",
    "\n",
    "# Process each PDF with Marker\n",
    "start_time = time.time()\n",
    "results = []\n",
    "\n",
    "for idx, row in sample_pdfs.iterrows():\n",
    "    print(f\"Processing {row['pdf_path']}...\")\n",
    "    md_content = process_pdf_with_marker(row['pdf_path'], model_lst)\n",
    "    results.append({\n",
    "        \"pdf_path\": row['pdf_path'],\n",
    "        \"page_count\": row['page_count'],\n",
    "        \"is_relevant\": row['is_relevant'],\n",
    "        \"md_extraction_result\": md_content\n",
    "    })\n",
    "    \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Processed {len(results)} PDFs in {duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Post-processing\n",
    "\n",
    "Finally, we'll clean up the extracted text to make it more useful for downstream tasks like language model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meaningful_text(markdown_content):\n",
    "    \"\"\"Extract meaningful text from markdown content.\"\"\"\n",
    "    # Remove metadata and formatting\n",
    "    content = re.sub(r'^---.*?---', '', markdown_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove title tags but keep the title text\n",
    "    content = re.sub(r'^#\\s*(.*?)\\n', r'\\1\\n', content)\n",
    "    \n",
    "    # Remove #### symbols but keep the header content\n",
    "    content = re.sub(r'^####\\s*(.*?)\\n', r'\\1\\n', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'^##\\s*(.*?)\\n', r'\\1\\n', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove bold formatting\n",
    "    content = re.sub(r'\\*\\*.*?\\*\\*', '', content)\n",
    "    \n",
    "    # Remove math formulas\n",
    "    content = re.sub(r'\\$.*?\\$', '', content)\n",
    "    \n",
    "    # Remove citations and references\n",
    "    content = re.sub(r'\\[.*?\\]', '', content)\n",
    "    content = re.sub(r'\\(.*?\\)', '', content)\n",
    "    \n",
    "    # Remove Markdown tables\n",
    "    content = re.sub(r'\\|[^\\n]*\\|(\\n\\|[-:| ]+\\|)?(\\n\\|[^\\n]*\\|)*', '', content)\n",
    "    \n",
    "    # Remove Keywords section\n",
    "    content = re.sub(r'Keywords:.*?(?=\\n\\n)', '', content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    content = content.strip()\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text post-processing\n",
    "results_df[\"extracted_meaningful_text\"] = results_df[\"md_extraction_result\"].apply(extract_meaningful_text)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_file = os.path.join(output_dir, \"processed_pdfs.csv\")\n",
    "results_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Display a sample of the extracted text\n",
    "for idx, row in results_df.head(1).iterrows():\n",
    "    print(f\"Sample extracted text from {os.path.basename(row['pdf_path'])}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(row[\"extracted_meaningful_text\"][:500] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated a simplified version of the web-scale PDF processing pipeline, focusing on the core components:\n",
    "\n",
    "1. **PDF Collection & Filtering**: Finding and filtering PDFs based on page count\n",
    "2. **Quality Filtering**: Multiple approaches including rule-based and LLM-based filtering (both API and open source models)\n",
    "3. **OCR Text Extraction**: Using Marker for high-quality text extraction with GPU acceleration\n",
    "4. **Text Post-processing**: Cleaning the extracted text for downstream use\n",
    "\n",
    "The full pipeline in the `web_scale_pdf_processing_pipeline` directory includes additional components for distributed processing using Apache Spark and SLURM, which enables scaling to thousands or millions of PDFs.\n",
    "\n",
    "### Key Differences from Full Pipeline\n",
    "\n",
    "- **Distribution**: This simplified version runs on a single machine with one GPU, while the full pipeline can distribute work across multiple nodes and GPUs\n",
    "- **Parallelism**: Our example processes PDFs sequentially, while the full pipeline uses Spark for parallel processing\n",
    "- **Scale**: This example is designed for educational purposes with a small number of PDFs, while the full pipeline can handle web-scale datasets\n",
    "\n",
    "### Tips for Quality Filtering\n",
    "\n",
    "- **Rule-based filtering** is fast and requires no API keys or model downloads, but less accurate\n",
    "- **OpenAI API filtering** provides high-quality results but requires an API key and has usage costs\n",
    "- **Open source model filtering** with Llama-3 offers a balance between quality and cost, but requires GPU resources\n",
    "\n",
    "For production use, you would want to use the full pipeline with proper distribution and parallelism for efficiency at scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
