{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scale PDF Processing Pipeline - Educational Example\n",
    "\n",
    "This notebook provides a simplified version of the web-scale PDF processing pipeline based on the [Web Scale PDF Processing Pipeline](https://github.com/aisingapore/web_scale_pdf_processing_pipeline) pipeline used to extract educational web resources for pretraining large language models.\n",
    "\n",
    "We'll break down the workflow into the following steps:\n",
    "1. Setup and Environment\n",
    "2. PDF Collection & Filtering\n",
    "3. Quality Filtering\n",
    "4. OCR Text Extraction using Marker\n",
    "5. Text Post-processing\n",
    "\n",
    "This simplified version uses a single GPU without distributed computing or Spark, perfect for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "First, let's install the required packages. The main package we'll need is [Marker](https://github.com/VikParuchuri/marker), a GPU-accelerated OCR tool for extracting text from PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install marker-pdf pypdf pandas opencv-python openai transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from marker.convert import convert_single_pdf\n",
    "from marker.models import load_all_models\n",
    "from marker.output import save_markdown\n",
    "import tempfile\n",
    "import shutil\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# Set up directories\n",
    "input_dir = \"./pdf_samples\"  # Directory containing your PDF files\n",
    "output_dir = \"./output\"      # Directory for outputs\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Collection & Filtering\n",
    "\n",
    "In this step, we'll find all PDF files in a directory and filter them based on basic criteria like page count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_pdf_files(directory):\n",
    "    \"\"\"List all PDF files in the given directory.\"\"\"\n",
    "    pdf_files = glob.glob(os.path.join(directory, \"*.pdf\"))\n",
    "    return pdf_files\n",
    "\n",
    "def get_pdf_page_count(pdf_path):\n",
    "    \"\"\"Get the number of pages in a PDF file.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            return len(pdf_reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_pdf_page_count for {pdf_path}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def filter_pdfs_by_page_count(pdf_files, min_pages=2, max_pages=500):\n",
    "    \"\"\"Filter PDFs by page count.\"\"\"\n",
    "    filtered_pdfs = []\n",
    "    for pdf_path in pdf_files:\n",
    "        page_count = get_pdf_page_count(pdf_path)\n",
    "        if min_pages <= page_count <= max_pages:\n",
    "            filtered_pdfs.append((pdf_path, page_count))\n",
    "    return filtered_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all PDF files\n",
    "pdf_files = list_pdf_files(input_dir)\n",
    "print(f\"Found {len(pdf_files)} PDF files\")\n",
    "\n",
    "# Filter PDFs by page count\n",
    "filtered_pdfs = filter_pdfs_by_page_count(pdf_files)\n",
    "print(f\"After filtering by page count: {len(filtered_pdfs)} PDFs\")\n",
    "\n",
    "# Create a DataFrame with the filtered PDFs\n",
    "pdf_df = pd.DataFrame(filtered_pdfs, columns=[\"pdf_path\", \"page_count\"])\n",
    "pdf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quality Filtering\n",
    "\n",
    "This step determines if a PDF contains relevant content for our needs. We'll implement two approaches:\n",
    "\n",
    "1. **Rule-based filtering**: A simple approach using basic text metrics\n",
    "2. **LLM-based filtering**: More sophisticated approach using language models\n",
    "   - API-based models (OpenAI's GPT-4o-mini)\n",
    "   - Open source models (Llama-3 8B)\n",
    "   \n",
    "You can choose which filtering method to use based on your needs and available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_basic_text(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyPDF (not as good as Marker but faster for initial filtering).\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            # Only extract from first few pages for quick filtering\n",
    "            max_pages = min(5, len(pdf_reader.pages))\n",
    "            text = \"\"\n",
    "            for i in range(max_pages):\n",
    "                text += pdf_reader.pages[i].extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_basic_text for {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 1. RULE-BASED FILTERING\n",
    "\n",
    "def is_relevant_rule_based(text, keywords=None, min_text_length=100):\n",
    "    \"\"\"Simple relevance check based on text length and optional keywords.\"\"\"\n",
    "    if len(text) < min_text_length:\n",
    "        return False\n",
    "        \n",
    "    if keywords:\n",
    "        return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# 2. LLM-BASED FILTERING\n",
    "\n",
    "# 2.1 OpenAI API Model (GPT-4o-mini)\n",
    "def is_relevant_openai(text, api_key=None, model=\"gpt-4o-mini\", domain=\"education\"):\n",
    "    \"\"\"Use OpenAI's API to determine if a PDF is relevant for a specific domain.\"\"\"\n",
    "    try:\n",
    "        import openai\n",
    "        \n",
    "        # You would need to set your API key\n",
    "        if api_key:\n",
    "            openai.api_key = api_key\n",
    "        elif os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "        else:\n",
    "            print(\"Warning: No OpenAI API key provided. Skipping OpenAI filtering.\")\n",
    "            return True  # Default to True if no API key\n",
    "        \n",
    "        client = openai.OpenAI()\n",
    "        \n",
    "        # Truncate the text to avoid excessive token usage\n",
    "        truncated_text = text[:15000]  # Using first 15k chars, adjust as needed\n",
    "        \n",
    "        # Create the prompt based on the domain\n",
    "        prompt = f\"\"\"You are an expert content evaluator. Your task is to determine if the following document is relevant for {domain} content.\n",
    "\n",
    "Here is a sample of the document:\n",
    "\n",
    "<document_sample>\n",
    "{truncated_text}\n",
    "</document_sample>\n",
    "\n",
    "Is this document relevant for {domain} purposes? Answer only with 'true' or 'false'.\"\"\"\n",
    "        \n",
    "        # Call the OpenAI API\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You help determine if documents are relevant for specific domains.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Extract the answer\n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        is_relevant = \"true\" in answer\n",
    "        \n",
    "        return is_relevant\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in is_relevant_openai: {str(e)}\")\n",
    "        return True  # Default to True in case of error\n",
    "\n",
    "# 2.2 Open Source Model (Llama-3)\n",
    "def is_relevant_llama(text, domain=\"education\"):\n",
    "    \"\"\"Use Llama-3 to determine if a PDF is relevant for a specific domain.\"\"\"\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        import torch\n",
    "        \n",
    "        # Truncate the text to avoid excessive token usage\n",
    "        truncated_text = text[:5000]  # Smaller context for local models\n",
    "        \n",
    "        # Load model and tokenizer (cached after first run)\n",
    "        model_name = \"meta-llama/Llama-3-8b-instruct\"  # Or any other appropriate model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = f\"\"\"<|system|>\n",
    "You are an expert content evaluator. You determine if documents are relevant for specific domains.\n",
    "<|user|>\n",
    "Is the following document relevant for {domain} content? Answer only with 'true' or 'false'.\n",
    "\n",
    "{truncated_text}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode the output\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the assistant's response (after the prompt)\n",
    "        assistant_response = response.split(\"<|assistant|>\")[-1].strip().lower()\n",
    "        \n",
    "        is_relevant = \"true\" in assistant_response\n",
    "        return is_relevant\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in is_relevant_llama: {str(e)}\")\n",
    "        return True  # Default to True in case of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text for filtering\n",
    "pdf_df[\"ocr_text\"] = pdf_df[\"pdf_path\"].apply(extract_basic_text)\n",
    "\n",
    "# Choose your filtering method\n",
    "filtering_method = \"rule-based\"  # Options: \"rule-based\", \"openai\", \"llama\"\n",
    "\n",
    "# Parameters for filtering\n",
    "domain = \"education\"  # Target domain for content\n",
    "keywords = [\"education\", \"research\", \"study\", \"learning\"]  # For rule-based filtering\n",
    "\n",
    "# Apply the selected filtering method\n",
    "if filtering_method == \"rule-based\":\n",
    "    print(\"Using rule-based filtering...\")\n",
    "    pdf_df[\"is_relevant\"] = pdf_df[\"ocr_text\"].apply(lambda text: is_relevant_rule_based(text, keywords))\n",
    "\n",
    "elif filtering_method == \"openai\":\n",
    "    # You would need to set your API key: os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "    print(\"Using OpenAI API filtering...\")\n",
    "    # Check a small sample first (comment out for full dataset)\n",
    "    sample_size = min(3, len(pdf_df))\n",
    "    pdf_df = pdf_df.head(sample_size)  # For testing API usage\n",
    "    pdf_df[\"is_relevant\"] = pdf_df[\"ocr_text\"].apply(lambda text: is_relevant_openai(text, domain=domain))\n",
    "\n",
    "elif filtering_method == \"llama\":\n",
    "    print(\"Using Llama-3 filtering...\")\n",
    "    # Comment out the next line for the full dataset\n",
    "    pdf_df = pdf_df.head(min(3, len(pdf_df)))  # Small sample for testing\n",
    "    pdf_df[\"is_relevant\"] = pdf_df[\"ocr_text\"].apply(lambda text: is_relevant_llama(text, domain=domain))\n",
    "\n",
    "# Filter relevant PDFs\n",
    "relevant_pdfs = pdf_df[pdf_df[\"is_relevant\"] == True]\n",
    "print(f\"After relevance check: {len(relevant_pdfs)} PDFs out of {len(pdf_df)} total\")\n",
    "relevant_pdfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OCR Text Extraction using Marker\n",
    "\n",
    "Now we'll use Marker, a GPU-accelerated OCR tool, to extract high-quality text from the PDFs. This is the core of the pipeline and the part that benefits most from GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Marker models (only once)\n",
    "print(\"Loading Marker models (this may take a while)...\")\n",
    "model_lst = load_all_models()\n",
    "print(\"Models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_marker(pdf_path, model_lst):\n",
    "    \"\"\"Process a PDF file using Marker's convert_single_pdf function and return the extracted text.\"\"\"\n",
    "    try:\n",
    "        # Convert PDF to text using Marker\n",
    "        full_text, images, out_meta = convert_single_pdf(pdf_path, model_lst)\n",
    "        \n",
    "        # Generate a unique filename\n",
    "        unique_filename = f\"{uuid.uuid4().hex}_{os.path.basename(pdf_path)}\"\n",
    "        \n",
    "        # Save the markdown to a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            md_dir = save_markdown(temp_dir, unique_filename, full_text, images, out_meta)\n",
    "            \n",
    "            # Construct the path to the .md file inside the created directory\n",
    "            md_filename = os.path.basename(md_dir) + \".md\"\n",
    "            md_file_path = os.path.join(md_dir, md_filename)\n",
    "            \n",
    "            # Read the content of the markdown file\n",
    "            with open(md_file_path, \"r\", encoding=\"utf-8\") as md_file:\n",
    "                md_content = md_file.read()\n",
    "        \n",
    "        return md_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {pdf_path}: {str(e)}\")\n",
    "        return f\"Error: Failed to process PDF {pdf_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample of PDFs (for quicker execution in this educational example)\n",
    "# In practice, you might process all relevant PDFs\n",
    "sample_size = min(5, len(relevant_pdfs))\n",
    "sample_pdfs = relevant_pdfs.head(sample_size)\n",
    "\n",
    "# Process each PDF with Marker\n",
    "start_time = time.time()\n",
    "results = []\n",
    "\n",
    "for idx, row in sample_pdfs.iterrows():\n",
    "    print(f\"Processing {row['pdf_path']}...\")\n",
    "    md_content = process_pdf_with_marker(row['pdf_path'], model_lst)\n",
    "    results.append({\n",
    "        \"pdf_path\": row['pdf_path'],\n",
    "        \"page_count\": row['page_count'],\n",
    "        \"is_relevant\": row['is_relevant'],\n",
    "        \"md_extraction_result\": md_content\n",
    "    })\n",
    "    \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Processed {len(results)} PDFs in {duration:.2f} seconds\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Post-processing\n",
    "\n",
    "Finally, we'll clean up the extracted text to make it more useful for downstream tasks like language model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meaningful_text(markdown_content):\n",
    "    \"\"\"Extract meaningful text from markdown content.\"\"\"\n",
    "    # Remove metadata and formatting\n",
    "    content = re.sub(r'^---.*?---', '', markdown_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove title tags but keep the title text\n",
    "    content = re.sub(r'^#\\s*(.*?)\\n', r'\\1\\n', content)\n",
    "    \n",
    "    # Remove #### symbols but keep the header content\n",
    "    content = re.sub(r'^####\\s*(.*?)\\n', r'\\1\\n', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'^##\\s*(.*?)\\n', r'\\1\\n', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove bold formatting\n",
    "    content = re.sub(r'\\*\\*.*?\\*\\*', '', content)\n",
    "    \n",
    "    # Remove math formulas\n",
    "    content = re.sub(r'\\$.*?\\$', '', content)\n",
    "    \n",
    "    # Remove citations and references\n",
    "    content = re.sub(r'\\[.*?\\]', '', content)\n",
    "    content = re.sub(r'\\(.*?\\)', '', content)\n",
    "    \n",
    "    # Remove Markdown tables\n",
    "    content = re.sub(r'\\|[^\\n]*\\|(\\n\\|[-:| ]+\\|)?(\\n\\|[^\\n]*\\|)*', '', content)\n",
    "    \n",
    "    # Remove Keywords section\n",
    "    content = re.sub(r'Keywords:.*?(?=\\n\\n)', '', content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    content = content.strip()\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text post-processing\n",
    "results_df[\"extracted_meaningful_text\"] = results_df[\"md_extraction_result\"].apply(extract_meaningful_text)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_file = os.path.join(output_dir, \"processed_pdfs.csv\")\n",
    "results_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Display a sample of the extracted text\n",
    "for idx, row in results_df.head(1).iterrows():\n",
    "    print(f\"Sample extracted text from {os.path.basename(row['pdf_path'])}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(row[\"extracted_meaningful_text\"][:500] + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated a simplified version of the web-scale PDF processing pipeline, focusing on the core components:\n",
    "\n",
    "1. **PDF Collection & Filtering**: Finding and filtering PDFs based on page count\n",
    "2. **Quality Filtering**: Multiple approaches including rule-based and LLM-based filtering (both API and open source models)\n",
    "3. **OCR Text Extraction**: Using Marker for high-quality text extraction with GPU acceleration\n",
    "4. **Text Post-processing**: Cleaning the extracted text for downstream use\n",
    "\n",
    "The full pipeline in the `web_scale_pdf_processing_pipeline` directory includes additional components for distributed processing using Apache Spark and SLURM, which enables scaling to thousands or millions of PDFs.\n",
    "\n",
    "### Key Differences from Full Pipeline\n",
    "\n",
    "- **Distribution**: This simplified version runs on a single machine with one GPU, while the full pipeline can distribute work across multiple nodes and GPUs\n",
    "- **Parallelism**: Our example processes PDFs sequentially, while the full pipeline uses Spark for parallel processing\n",
    "- **Scale**: This example is designed for educational purposes with a small number of PDFs, while the full pipeline can handle web-scale datasets\n",
    "\n",
    "### Tips for Quality Filtering\n",
    "\n",
    "- **Rule-based filtering** is fast and requires no API keys or model downloads, but less accurate\n",
    "- **OpenAI API filtering** provides high-quality results but requires an API key and has usage costs\n",
    "- **Open source model filtering** with Llama-3 offers a balance between quality and cost, but requires GPU resources\n",
    "\n",
    "For production use, you would want to use the full pipeline with proper distribution and parallelism for efficiency at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multilingual Translation of Extracted Text\n",
    "\n",
    "After extracting text from PDFs, you might need to translate content between languages to make it more accessible or to standardize your dataset. This section demonstrates two approaches:\n",
    "\n",
    "1. **Google Translate API**: Cloud-based translation service with high quality and broad language support\n",
    "2. **Meta's NLLB Model**: Open-source multilingual model that can translate between 200+ languages locally\n",
    "\n",
    "Both methods have their advantages:\n",
    "- Google Translate API is simple to use but requires an API key and has usage costs\n",
    "- NLLB is free to use and works offline, but requires more computational resources\n",
    "\n",
    "Let's see how to implement both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional required packages\n",
    "!uv pip install google-cloud-translate langdetect transformers sentencepiece protobuf sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import translation-related libraries\n",
    "import langdetect\n",
    "from google.cloud import translate_v2 as translate\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Add these imports at the top of your notebook\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect the language of a text using langdetect.\"\"\"\n",
    "    try:\n",
    "        return langdetect.detect(text)\n",
    "    except:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Google Translate API Implementation\n",
    "\n",
    "Google Cloud Translation provides a simple, scalable API for translating text with high quality results. It supports over 100 languages and is reliable for production use.\n",
    "\n",
    "**Note**: You'll need a Google Cloud account and API key to use this service. The code below assumes you've set up authentication via environment variables or service account credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_google(text, target_language='en', source_language=None):\n",
    "    \"\"\"\n",
    "    Translate text using Google Cloud Translation API.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to translate\n",
    "        target_language (str): Target language code (e.g., 'en', 'fr', 'zh-CN')\n",
    "        source_language (str, optional): Source language code. If None, Google will auto-detect\n",
    "    \n",
    "    Returns:\n",
    "        str: Translated text\n",
    "    \"\"\"\n",
    "    # Check if text is too long (Google limits to 100K characters per request)\n",
    "    if len(text) > 100000:\n",
    "        # Split into smaller chunks (for simplicity, this splits by periods)\n",
    "        chunks = text.split('. ')\n",
    "        translated_chunks = []\n",
    "        \n",
    "        for i in range(0, len(chunks), 50):  # Process 50 sentences at a time\n",
    "            chunk_text = '. '.join(chunks[i:i+50]) + ('.' if i+50 < len(chunks) else '')\n",
    "            result = translate_with_google(chunk_text, target_language, source_language)\n",
    "            translated_chunks.append(result)\n",
    "            \n",
    "        return ' '.join(translated_chunks)\n",
    "    \n",
    "    try:\n",
    "        # Initialize the Google Translate client\n",
    "        # Note: This assumes you've set up authentication via environment variables\n",
    "        # or a service account. You may need to adjust based on your setup.\n",
    "        translate_client = translate.Client()\n",
    "        \n",
    "        # Perform the translation\n",
    "        if source_language:\n",
    "            result = translate_client.translate(\n",
    "                text,\n",
    "                target_language=target_language,\n",
    "                source_language=source_language\n",
    "            )\n",
    "        else:\n",
    "            result = translate_client.translate(\n",
    "                text,\n",
    "                target_language=target_language\n",
    "            )\n",
    "        \n",
    "        # Return the translated text\n",
    "        return result['translatedText']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in translation: {e}\")\n",
    "        return \"Error: Translation failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using Google Translate API\n",
    "# Set your Google Cloud credentials (if not using a service account)\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/your/credentials.json\"\n",
    "\n",
    "# Sample translation with Google API\n",
    "# Note: This code is commented out as it requires API credentials\n",
    "# Feel free to uncomment and run this code after setting up your credentials\n",
    "\n",
    "sample_text = results_df[\"extracted_meaningful_text\"].iloc[0][:1000]  # First 1000 chars of first document\n",
    "detected_lang = detect_language(sample_text)\n",
    "print(f\"Detected language: {detected_lang}\")\n",
    "\n",
    "# Translate to English if not already English\n",
    "if detected_lang != \"en\" and detected_lang != \"unknown\":\n",
    "    translated_text = translate_with_google(sample_text, target_language=\"en\", source_language=detected_lang)\n",
    "    print(f\"Original text ({detected_lang}): {sample_text[:200]}...\")\n",
    "    print(f\"Translated text (en): {translated_text[:200]}...\")\n",
    "else:\n",
    "    print(f\"Sample text is already in English or language couldn't be detected.\")\n",
    "    # Example of translating English to French\n",
    "    # translated_text = translate_with_google(sample_text, target_language=\"fr\", source_language=\"en\")\n",
    "    # print(f\"Original text (en): {sample_text[:200]}...\")\n",
    "    # print(f\"Translated text (fr): {translated_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Meta's NLLB (No Language Left Behind) Model Implementation\n",
    "\n",
    "NLLB is an open-source machine translation model developed by Meta AI that can translate between 200+ languages. It's particularly useful for:\n",
    "\n",
    "- **Low-resource languages** that might not be well-supported by commercial services\n",
    "- **Offline translation** without requiring API calls\n",
    "- **Cost-free translation** for large volumes of text\n",
    "\n",
    "The model is available on Hugging Face in different sizes (distilled, 1.3B, 3.3B). We'll use the 3.3B parameter version, which offers a good balance between quality and computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nllb_model(model_name=\"facebook/nllb-200-3.3B\", device=None):\n",
    "    \"\"\"\n",
    "    Load the NLLB model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name or path of the NLLB model to load\n",
    "        device (str, optional): Device to load the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Loading NLLB model {model_name} on {device}...\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load with lower precision for GPU memory efficiency if using CUDA\n",
    "    if device == \"cuda\":\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    else:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def translate_with_nllb(text, model, tokenizer, target_language='eng_Latn', source_language=None, device=None):\n",
    "    \"\"\"\n",
    "    Translate text using the NLLB model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to translate\n",
    "        model: The NLLB model\n",
    "        tokenizer: The NLLB tokenizer\n",
    "        target_language (str): Target language code in NLLB format (e.g., 'eng_Latn', 'fra_Latn')\n",
    "        source_language (str, optional): Source language code. If None, will use langdetect to guess\n",
    "        device (str, optional): Device to use for translation\n",
    "        \n",
    "    Returns:\n",
    "        str: Translated text\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # If no source language provided, try to detect it\n",
    "    if source_language is None:\n",
    "        # Map langdetect codes to NLLB codes (simplified mapping for common languages)\n",
    "        lang_map = {\n",
    "            \"en\": \"eng_Latn\", \"fr\": \"fra_Latn\", \"es\": \"spa_Latn\", \"de\": \"deu_Latn\", \n",
    "            \"zh\": \"zho_Hans\", \"ja\": \"jpn_Jpan\", \"ko\": \"kor_Hang\", \"ru\": \"rus_Cyrl\",\n",
    "            \"ar\": \"arb_Arab\", \"hi\": \"hin_Deva\", \"pt\": \"por_Latn\", \"it\": \"ita_Latn\",\n",
    "        }\n",
    "        \n",
    "        detected = detect_language(text)\n",
    "        source_language = lang_map.get(detected, \"eng_Latn\")  # Default to English if not found\n",
    "    \n",
    "    # Function to split text into manageable chunks to avoid GPU OOM errors\n",
    "    def chunk_text(text, max_length=800):\n",
    "        # Simple splitting by sentences to avoid cutting in middle of sentences\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence.split())\n",
    "            if current_length + sentence_length <= max_length:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "            else:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_length\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    # Process text in chunks to avoid OOM errors\n",
    "    chunks = chunk_text(text)\n",
    "    translated_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            # Prepare the input text with language tags\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Generate translation\n",
    "            with torch.no_grad():\n",
    "                # Set the language we're translating to as forced first token\n",
    "                forced_bos_token_id = tokenizer.lang_code_to_id[target_language]\n",
    "                \n",
    "                # Generate translation\n",
    "                translated_tokens = model.generate(\n",
    "                    **inputs, \n",
    "                    forced_bos_token_id=forced_bos_token_id,\n",
    "                    max_length=4096,\n",
    "                    num_beams=5,\n",
    "                    length_penalty=1.0\n",
    "                )\n",
    "            \n",
    "            # Decode the translated tokens\n",
    "            translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "            translated_chunks.append(translated_text)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error translating chunk: {e}\")\n",
    "            translated_chunks.append(\"[Translation Error]\")\n",
    "    \n",
    "    # Combine the translated chunks\n",
    "    return ' '.join(translated_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using NLLB for translation\n",
    "# Note: This code is commented out to prevent accidental execution\n",
    "# It requires significant resources, especially VRAM if using the full model\n",
    "\n",
    "# For educational purposes, you can use a smaller model version like:\n",
    "# - \"facebook/nllb-200-distilled-600M\" (smallest, fastest)\n",
    "# - \"facebook/nllb-200-1.3B\" (medium size)\n",
    "# - \"facebook/nllb-200-3.3B\" (larger, better quality)\n",
    "\n",
    "# Load the NLLB model (only do this once and reuse for multiple translations)\n",
    "model, tokenizer = load_nllb_model(\"facebook/nllb-200-distilled-600M\")  # Use smaller model for demo\n",
    "\n",
    "# Get a sample text\n",
    "sample_text = results_df[\"extracted_meaningful_text\"].iloc[0][:500]  # First 500 chars of first document\n",
    "\n",
    "# Detect language with langdetect\n",
    "detected_lang = detect_language(sample_text)\n",
    "print(f\"Detected language: {detected_lang}\")\n",
    "\n",
    "# Map to NLLB language code\n",
    "nllb_lang_map = {\"en\": \"eng_Latn\", \"fr\": \"fra_Latn\", \"es\": \"spa_Latn\", \"de\": \"deu_Latn\", \n",
    "                \"zh\": \"zho_Hans\", \"ja\": \"jpn_Jpan\", \"ko\": \"kor_Hang\", \"ru\": \"rus_Cyrl\"}\n",
    "source_lang = nllb_lang_map.get(detected_lang, \"eng_Latn\")\n",
    "\n",
    "# Translate to French\n",
    "translated_text = translate_with_nllb(\n",
    "    sample_text, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    target_language=\"fra_Latn\",  # French\n",
    "    source_language=source_lang\n",
    ")\n",
    "\n",
    "print(f\"Original text ({detected_lang}): {sample_text[:200]}...\")\n",
    "print(f\"Translated text (fra_Latn): {translated_text[:200]}...\")\n",
    "\n",
    "# Translate to Spanish\n",
    "translated_text_es = translate_with_nllb(\n",
    "    sample_text, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    target_language=\"spa_Latn\",  # Spanish \n",
    "    source_language=source_lang\n",
    ")\n",
    "\n",
    "print(f\"Translated text (spa_Latn): {translated_text_es[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Combined Translation Pipeline\n",
    "\n",
    "Now let's create a unified translation pipeline that:\n",
    "1. Automatically detects document language\n",
    "2. Chooses the appropriate translation method\n",
    "3. Translates all documents to the target language\n",
    "4. Updates our dataset with the translated text\n",
    "\n",
    "This pipeline allows for flexible translation with either Google Translate API or NLLB model, depending on your preferences and available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_documents(df, text_column, target_language='en', method='google',\n",
    "                    nllb_model=None, nllb_tokenizer=None, nllb_model_name=\"facebook/nllb-200-distilled-600M\"):\n",
    "    \"\"\"\n",
    "    Translate documents in a DataFrame to the target language.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing documents\n",
    "        text_column (str): Column name containing text to translate\n",
    "        target_language (str): Target language code \n",
    "        method (str): Translation method - 'google', 'nllb', or 'auto'\n",
    "        nllb_model: Pre-loaded NLLB model (required if method is 'nllb')\n",
    "        nllb_tokenizer: Pre-loaded NLLB tokenizer (required if method is 'nllb')\n",
    "        nllb_model_name (str): NLLB model name to load if model not provided\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added translated_text column\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Add a column for detected language\n",
    "    result_df['detected_language'] = result_df[text_column].apply(detect_language)\n",
    "    \n",
    "    # Map target language code for NLLB if needed\n",
    "    nllb_target_map = {\n",
    "        'en': 'eng_Latn', 'fr': 'fra_Latn', 'es': 'spa_Latn', 'de': 'deu_Latn', \n",
    "        'zh': 'zho_Hans', 'ja': 'jpn_Jpan', 'ko': 'kor_Hang', 'ru': 'rus_Cyrl',\n",
    "        'ar': 'arb_Arab', 'hi': 'hin_Deva', 'pt': 'por_Latn', 'it': 'ita_Latn'\n",
    "    }\n",
    "    \n",
    "    # Load NLLB model if needed and not provided\n",
    "    if method == 'nllb' and (nllb_model is None or nllb_tokenizer is None):\n",
    "        nllb_model, nllb_tokenizer = load_nllb_model(nllb_model_name)\n",
    "    \n",
    "    # Translate each document\n",
    "    translated_texts = []\n",
    "    \n",
    "    for idx, row in result_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        detected_lang = row['detected_language']\n",
    "        \n",
    "        # Skip translation if already in target language\n",
    "        if detected_lang == target_language:\n",
    "            translated_texts.append(text)\n",
    "            print(f\"Document {idx}: Already in {target_language}, skipping translation\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Document {idx}: Translating from {detected_lang} to {target_language}...\")\n",
    "        \n",
    "        # Choose translation method\n",
    "        if method == 'auto':\n",
    "            # Use Google for common languages, NLLB for others\n",
    "            common_langs = ['en', 'fr', 'es', 'de', 'zh', 'ja', 'pt', 'it']\n",
    "            chosen_method = 'google' if detected_lang in common_langs else 'nllb'\n",
    "        else:\n",
    "            chosen_method = method\n",
    "        \n",
    "        # Perform translation\n",
    "        try:\n",
    "            if chosen_method == 'google':\n",
    "                translated_text = translate_with_google(text, target_language=target_language, \n",
    "                                                      source_language=detected_lang)\n",
    "            else:  # nllb\n",
    "                nllb_source = nllb_target_map.get(detected_lang, 'eng_Latn')\n",
    "                nllb_target = nllb_target_map.get(target_language, 'eng_Latn')\n",
    "                \n",
    "                translated_text = translate_with_nllb(\n",
    "                    text, nllb_model, nllb_tokenizer,\n",
    "                    target_language=nllb_target,\n",
    "                    source_language=nllb_source\n",
    "                )\n",
    "            \n",
    "            translated_texts.append(translated_text)\n",
    "            print(f\"âœ“ Translation completed using {chosen_method}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error translating document {idx}: {e}\")\n",
    "            translated_texts.append(f\"[Translation Error: {str(e)}]\")\n",
    "    \n",
    "    # Add translated texts to the DataFrame\n",
    "    result_df['translated_text'] = translated_texts\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the translation pipeline (commented out to prevent accidental execution)\n",
    "# Choose your translation method: 'google', 'nllb', or 'auto'\n",
    "translation_method = 'google'  # Change as needed\n",
    "\n",
    "# Target language\n",
    "target_language = 'en'  # Change as needed\n",
    "\n",
    "# Select a small sample to demonstrate (for educational purposes)\n",
    "sample_size = min(3, len(results_df))\n",
    "sample_df = results_df.head(sample_size)\n",
    "\n",
    "# NLLB model and tokenizer (only needed for 'nllb' or 'auto' methods)\n",
    "nllb_model = None\n",
    "nllb_tokenizer = None\n",
    "\n",
    "if translation_method in ['nllb', 'auto']:\n",
    "    # For educational purposes, use a smaller model\n",
    "    nllb_model, nllb_tokenizer = load_nllb_model(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "# Run the translation pipeline\n",
    "translated_df = translate_documents(\n",
    "    sample_df, \n",
    "    text_column='extracted_meaningful_text',\n",
    "    target_language=target_language,\n",
    "    method=translation_method,\n",
    "    nllb_model=nllb_model,\n",
    "    nllb_tokenizer=nllb_tokenizer\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTranslation Results (method: {translation_method}):\")\n",
    "for idx, row in translated_df.iterrows():\n",
    "    print('-' * 80)\n",
    "    print(f\"Document {idx} | Original language: {row['detected_language']}\")\n",
    "    \n",
    "    # Show original and translated snippets\n",
    "    original_text = row['extracted_meaningful_text'][:200] + \"...\"\n",
    "    translated_text = row['translated_text'][:200] + \"...\"\n",
    "    \n",
    "    print(f\"\\nOriginal: {original_text}\")\n",
    "    print(f\"\\nTranslated ({target_language}): {translated_text}\")\n",
    "\n",
    "# Save the results\n",
    "translated_csv = os.path.join(output_dir, f\"translated_documents_{translation_method}.csv\")\n",
    "translated_df.to_csv(translated_csv, index=False)\n",
    "print(f\"\\nSaved translated documents to: {translated_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've demonstrated the complete pipeline for processing educational PDF content:\n",
    "\n",
    "1. **PDF Collection & Basic Filtering**: Finding PDFs and filtering by page count\n",
    "2. **Quality Filtering**: Using rule-based and LLM-based approaches\n",
    "3. **OCR Text Extraction**: Extracting high-quality text with Marker\n",
    "4. **Text Post-processing**: Cleaning the extracted text for downstream use\n",
    "5. **Multilingual Translation**: Translating content with Google Translate API or NLLB model\n",
    "\n",
    "These components can be used as building blocks for creating comprehensive data pipelines for educational content. The full `web_scale_pdf_processing_pipeline` directory contains additional components for distributed processing at web scale.\n",
    "\n",
    "### Considerations for Production Use\n",
    "\n",
    "- **GPU Resources**: Marker and NLLB benefit significantly from GPU acceleration\n",
    "- **API Costs**: Consider API costs when using Google Translate API for large volumes\n",
    "- **Language Coverage**: NLLB may provide better coverage for low-resource languages\n",
    "- **Batch Processing**: Process documents in batches for better efficiency\n",
    "- **Quality Evaluation**: Implement quality checks for both OCR and translation results\n",
    "\n",
    "### Further Improvements\n",
    "\n",
    "- Implement parallel processing for faster execution\n",
    "- Add quality metrics for translated content\n",
    "- Integrate with document databases for storage and retrieval\n",
    "- Add support for specialized domain terminology\n",
    "- Implement caching to avoid redundant translations\n",
    "\n",
    "The combination of high-quality OCR extraction and translation capabilities makes this pipeline versatile for educational content processing across languages and domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
