{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filtering Techniques for Synthetic Data\n",
    "\n",
    "This notebook explores techniques for filtering and processing synthetic data generated by large language models. Based on the Cosmopedia project, we'll cover:\n",
    "\n",
    "1. Post-Generation Cleanup\n",
    "2. Deduplication Pipeline\n",
    "3. Decontamination Process\n",
    "4. Educational Value Classification\n",
    "5. Quality Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.8 environment at: /Users/ob1/Desktop/NVIDIA teaching kits/aisg_github_course/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.8 environment at: /Users/ob1/Desktop/NVIDIA teaching kits/aisg_github_course/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 46ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "!uv pip install datasets transformers numpy scikit-learn\n",
    "\n",
    "# For deduplication\n",
    "!uv pip install \"datatrove[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Post-Generation Cleanup\n",
    "\n",
    "After generating synthetic data with an LLM, we often need to clean up boilerplate text such as introductions, greetings, and template artifacts that instruction-tuned models are trained to generate as chatbots. We can start with regular expressions to identify and remove these patterns, but more complex postprocessing can be applied when you have a better understanding of the data you are generating. This will differ based on the model you are using, and in general, you should iteratively edit this post-generation cleanup process based on new observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define patterns to remove common boilerplate text\n",
    "patterns = [\n",
    "    # Greeting patterns\n",
    "    r\"^Hello.*?[.!]\\s+\",\n",
    "    r\"^My name is.*?[.!]\\s+\",\n",
    "    r\"^You've just arrived.*?[.!]\\s+\",\n",
    "    \n",
    "    # Documentation patterns\n",
    "    r\"^\\*\\*Welcome, .*?[.!]\\*\\*\\s+\",\n",
    "    r\"^(\\*\\*)?Warning:.*?[.!]\\s+\",\n",
    "    r\"^We're thrilled.*?[.!]\\s+\",\n",
    "    \n",
    "    # General introductions\n",
    "    r\"^Welcome, .*?[.!]\\s+\",\n",
    "]\n",
    "\n",
    "# Compile the patterns\n",
    "compiled_patterns = [re.compile(p, flags=re.IGNORECASE|re.MULTILINE) for p in patterns]\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(sample):\n",
    "    # Keep the original text\n",
    "    sample['completion_unfiltered'] = sample['text']\n",
    "    \n",
    "    # Apply each pattern to remove boilerplate\n",
    "    for pattern in compiled_patterns:\n",
    "        sample['text'] = pattern.sub('', sample['text'].strip())\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Cleanup Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Hello there! I'm excited to share this information with you.\n",
      "\n",
      "In this article, we'll explore the fascinating world of synthetic data processing.\n",
      "Let's begin by understanding what synthetic data is and why it matters.\n",
      "\n",
      "\n",
      "Cleaned text:\n",
      " I'm excited to share this information with you.\n",
      "\n",
      "In this article, we'll explore the fascinating world of synthetic data processing.\n",
      "Let's begin by understanding what synthetic data is and why it matters.\n"
     ]
    }
   ],
   "source": [
    "# Example text with boilerplate\n",
    "example_text = \"\"\"Hello there! I'm excited to share this information with you.\n",
    "\n",
    "In this article, we'll explore the fascinating world of synthetic data processing.\n",
    "Let's begin by understanding what synthetic data is and why it matters.\n",
    "\"\"\"\n",
    "\n",
    "# Manual application of cleanup\n",
    "sample = {'text': example_text}\n",
    "cleaned = clean_text(sample)\n",
    "\n",
    "print(\"Original text:\\n\", sample['completion_unfiltered'])\n",
    "print(\"\\nCleaned text:\\n\", sample['text']) # as you can tell, the clean up process is not perfect, but it is a good start. You can likely also remove \"I'm excited to share this information with you\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a44ebf62b97432ca9374d9169ed7777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"HuggingFaceTB/cosmopedia\" # or replace with your dataset you uploaded from the previous notebook after generating data\n",
    "data = load_dataset(dataset_name, \"khanacademy\")\n",
    "cleaned_data = data.map(clean_text, num_proc=4)  # num_proc uses multiprocessing for larger datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deduplication Pipeline\n",
    "\n",
    "Deduplication is critical for removing redundant content from the dataset. We'll use the MinHash algorithm via the datatrove library, which efficiently identifies similar documents in large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatrove.pipeline.dedup import MinhashDedupSignature\n",
    "from datatrove.pipeline.dedup.minhash import (\n",
    "    MinhashConfig,\n",
    "    MinhashDedupBuckets,\n",
    "    MinhashDedupCluster,\n",
    "    MinhashDedupFilter,\n",
    ")\n",
    "from datatrove.pipeline.readers import HuggingFaceDatasetReader\n",
    "from datatrove.pipeline.tokens import TokensCounter\n",
    "from datatrove.pipeline.writers.jsonl import JsonlWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deduplication process consists of four stages:\n",
    "\n",
    "1. **Compute signatures**: Generate MinHash signatures for each document\n",
    "2. **Find matches**: Use LSH (Locality-Sensitive Hashing) to find potential matches\n",
    "3. **Create clusters**: Group similar documents into clusters\n",
    "4. **Filter duplicates**: Keep only one document per cluster\n",
    "\n",
    "Below is an example of how to set up this pipeline using datatrove:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datatrove.executor.local import LocalPipelineExecutor\n",
    "from datatrove.utils.hashing import HashConfig\n",
    "\n",
    "# Configuration for MinHash deduplication\n",
    "minhash_config = MinhashConfig(  hash_config=HashConfig(precision=64),\n",
    "    num_buckets=14,\n",
    "    hashes_per_bucket=8,)  # Default configuration\n",
    "\n",
    "# Define the output paths (adjust as needed)\n",
    "output_base_path = \"./dedup_output\"\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    " \n",
    "reader = HuggingFaceDatasetReader( # Define the reader for input data, here we are loading from HF so HuggingFaceDatasetReader reader is used. Refer to the datatrove documentation for more details on other readers. Refer to this example pipeline used for FineWeb: https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py\n",
    "    dataset=dataset_name,\n",
    "    dataset_options={\"split\": \"train\", \"name\": \"khanacademy\"}, # https://github.com/huggingface/datatrove/blob/main/tests/pipeline/test_hf_reader.py#L12\n",
    "    text_key=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Compute MinHash Signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:09:16.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36madd_task_logger\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mLaunching pipeline for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:09:16.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36mlog_pipeline\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1m\n",
      "--- ðŸ› ï¸ PIPELINE ðŸ› \n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 1\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0214d5e626f84339b90e87b3397fbd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:11:27.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.dedup.minhash\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m262\u001b[0m - \u001b[1mSorting buckets...\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:27.531\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m98\u001b[0m - \u001b[32m\u001b[1mProcessing done for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:27.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: Task 0 ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 2 minutes and 10 seconds\n",
      "\n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "    Runtime: (0.04%) 0 seconds [2.32 millisecondsÂ±0.61 milliseconds/batch]\n",
      "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 1\n",
      "    Runtime: (99.96%) 2 minutes and 10 seconds [2 minutes, 10 seconds and 656.96 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {total: 24123}\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:27.537\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: All 1 tasks ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 2 minutes and 10 seconds\n",
      "\n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "    Runtime: (0.04%) 0 seconds [2.32 millisecondsÂ±0.61 milliseconds/batch]\n",
      "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 1\n",
      "    Runtime: (99.96%) 2 minutes and 10 seconds [2 minutes, 10 seconds and 656.96 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {total: 24123}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ðŸ“‰ðŸ“‰ðŸ“‰ Stats ðŸ“‰ðŸ“‰ðŸ“‰\n",
       "\n",
       "Total Runtime: 2 minutes and 10 seconds\n",
       "\n",
       "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
       "    Runtime: (0.04%) 0 seconds [2.32 millisecondsÂ±0.61 milliseconds/batch]\n",
       "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
       "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 1\n",
       "    Runtime: (99.96%) 2 minutes and 10 seconds [2 minutes, 10 seconds and 656.96 millisecondsÂ±0 milliseconds/doc]\n",
       "    Stats: {total: 24123}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 1: Generate signatures\n",
    "stage1 = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        reader,\n",
    "        MinhashDedupSignature(output_folder=f\"{output_base_path}/signatures\", config=minhash_config),\n",
    "    ],\n",
    "    tasks=1,  # Increase for larger datasets\n",
    ")\n",
    "stage1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Find Matches Between Signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 09:11:29.537 | INFO     | datatrove.utils.logging:add_task_logger:58 - Launching pipeline for rank=0\n",
      "2025-05-14 09:11:29.539 | INFO     | datatrove.utils.logging:log_pipeline:90 - \n",
      "--- ðŸ› ï¸ PIPELINE ðŸ› \n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 2\n",
      "2025-05-14 09:11:29.541 | INFO     | datatrove.pipeline.dedup.minhash:run:359 - Running worker 1/1 on bucket 000. Hash range: [0, np.uint64(2305843009213693951)]\n",
      "2025-05-14 09:11:29.542 | INFO     | datatrove.pipeline.dedup.minhash:run:406 - Finished initializing signatures priority queue.\n",
      "2025-05-14 09:11:29.568 | SUCCESS  | datatrove.executor.base:_run_for_rank:98 - Processing done for rank=0\n",
      "2025-05-14 09:11:29.578 | INFO     | datatrove.executor.base:_run_for_rank:104 - \n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: Task 0 ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds\n",
      "\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 2\n",
      "    Runtime: (100.00%) 0 seconds [27.40 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {total_matches: 53}\n",
      "2025-05-14 09:11:29.584 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 1/14 tasks completed.\n",
      "2025-05-14 09:11:29.635 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 2/14 tasks completed.\n",
      "2025-05-14 09:11:29.725 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 3/14 tasks completed.\n",
      "2025-05-14 09:11:29.803 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 4/14 tasks completed.\n",
      "2025-05-14 09:11:29.903 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 5/14 tasks completed.\n",
      "2025-05-14 09:11:29.990 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 6/14 tasks completed.\n",
      "2025-05-14 09:11:30.105 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 7/14 tasks completed.\n",
      "2025-05-14 09:11:30.183 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 8/14 tasks completed.\n",
      "2025-05-14 09:11:30.295 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 9/14 tasks completed.\n",
      "2025-05-14 09:11:30.379 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 10/14 tasks completed.\n",
      "2025-05-14 09:11:30.479 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 11/14 tasks completed.\n",
      "2025-05-14 09:11:30.566 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 12/14 tasks completed.\n",
      "2025-05-14 09:11:30.645 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 13/14 tasks completed.\n",
      "2025-05-14 09:11:30.738 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 14/14 tasks completed.\n",
      "\u001b[32m2025-05-14 09:11:30.775\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: All 14 tasks ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds Â± 0 seconds/task\n",
      "\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 2\n",
      "    Runtime: (100.00%) 0 secondsÂ±0 seconds/task, min=0 seconds, max=0 seconds [24.64 millisecondsÂ±1.41 milliseconds/doc]\n",
      "    Stats: {total_matches: 726}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ðŸ“‰ðŸ“‰ðŸ“‰ Stats ðŸ“‰ðŸ“‰ðŸ“‰\n",
       "\n",
       "Total Runtime: 0 seconds Â± 0 seconds/task\n",
       "\n",
       "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 2\n",
       "    Runtime: (100.00%) 0 secondsÂ±0 seconds/task, min=0 seconds, max=0 seconds [24.64 millisecondsÂ±1.41 milliseconds/doc]\n",
       "    Stats: {total_matches: 726}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 2: Find matches\n",
    "!export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES # requierd for multiprocessing used in datatrove\n",
    "\n",
    "stage2 = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        MinhashDedupBuckets(\n",
    "            input_folder=f\"{output_base_path}/signatures\",\n",
    "            output_folder=f\"{output_base_path}/buckets\",\n",
    "            config=minhash_config,\n",
    "        ),\n",
    "    ],\n",
    "    depends=stage1,\n",
    "    tasks=minhash_config.num_buckets,\n",
    ")\n",
    "stage2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Create Clusters of Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:11:30.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36madd_task_logger\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mLaunching pipeline for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36mlog_pipeline\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1m\n",
      "--- ðŸ› ï¸ PIPELINE ðŸ› \n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 3\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.dedup.minhash\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m511\u001b[0m - \u001b[1mLoading dup files...\u001b[0m\n",
      "Reading dup files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 5647.26it/s]\n",
      "\u001b[32m2025-05-14 09:11:30.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.pipeline.dedup.minhash\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m521\u001b[0m - \u001b[1mFinished reading dup files.\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.847\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m98\u001b[0m - \u001b[32m\u001b[1mProcessing done for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: Task 0 ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds\n",
      "\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 3\n",
      "    Runtime: (100.00%) 0 seconds [5.31 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {duplicates: 69, cluster_size: 69 [min=2, max=18, 5.75Â±5/cluster], to_remove: 57}\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.852\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: All 1 tasks ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 0 seconds\n",
      "\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 3\n",
      "    Runtime: (100.00%) 0 seconds [5.31 millisecondsÂ±0 milliseconds/doc]\n",
      "    Stats: {duplicates: 69, cluster_size: 69 [min=2, max=18, 5.75Â±5/cluster], to_remove: 57}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ðŸ“‰ðŸ“‰ðŸ“‰ Stats ðŸ“‰ðŸ“‰ðŸ“‰\n",
       "\n",
       "Total Runtime: 0 seconds\n",
       "\n",
       "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 3\n",
       "    Runtime: (100.00%) 0 seconds [5.31 millisecondsÂ±0 milliseconds/doc]\n",
       "    Stats: {duplicates: 69, cluster_size: 69 [min=2, max=18, 5.75Â±5/cluster], to_remove: 57}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 3: Create clusters\n",
    "stage3 = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        MinhashDedupCluster(\n",
    "            input_folder=f\"{output_base_path}/buckets\",\n",
    "            output_folder=f\"{output_base_path}/remove_ids\",\n",
    "            config=minhash_config,\n",
    "        ),\n",
    "    ],\n",
    "    depends=stage2,\n",
    "    tasks=1,\n",
    ")\n",
    "stage3.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4: Filter Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:11:30.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36madd_task_logger\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mLaunching pipeline for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:11:30.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.utils.logging\u001b[0m:\u001b[36mlog_pipeline\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1m\n",
      "--- ðŸ› ï¸ PIPELINE ðŸ› \n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "ðŸ”¢ - TOKENIZER: ðŸ“Š Counter\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 4\n",
      "ðŸ’½ - WRITER: ðŸ¿ Jsonl\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c45ed6fad2a48ff9603207d8e909d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 09:12:05.418\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m98\u001b[0m - \u001b[32m\u001b[1mProcessing done for rank=0\u001b[0m\n",
      "\u001b[32m2025-05-14 09:12:05.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdatatrove.executor.base\u001b[0m:\u001b[36m_run_for_rank\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: Task 0 ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 30 seconds\n",
      "\n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "    Runtime: (0.20%) 0 seconds [2.47 millisecondsÂ±0.59 milliseconds/batch]\n",
      "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
      "ðŸ”¢ - TOKENIZER: ðŸ“Š Counter\n",
      "    Runtime: (88.77%) 27 seconds [9 seconds and 89.86 millisecondsÂ±3 seconds and 101.20 milliseconds/batch]\n",
      "    Stats: {tokens: 20953163 [min=2, max=4439, 868.60Â±388/doc]}\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 4\n",
      "    Runtime: (0.10%) 0 seconds [0.00 millisecondsÂ±0.01 milliseconds/doc]\n",
      "    Stats: {total: 24123, forwarded: 24066, dropped: 57}\n",
      "ðŸ’½ - WRITER: ðŸ¿ Jsonl\n",
      "    Runtime: (10.93%) 3 seconds [0.14 millisecondsÂ±0.28 milliseconds/doc]\n",
      "    Stats: {XXXXX.jsonl.gz: 24066, total: 24066, doc_len: 88402371 [min=5, max=18344, 3673.33Â±1446/doc], doc_len_tokens: 20952551 [min=2, max=4439, 870.63Â±386/doc]}\u001b[0m\n",
      "\u001b[32m2025-05-14 09:12:05.424\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdatatrove.executor.local\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m148\u001b[0m - \u001b[32m\u001b[1m\n",
      "\n",
      "ðŸ“‰ðŸ“‰ðŸ“‰ Stats: All 1 tasks ðŸ“‰ðŸ“‰ðŸ“‰\n",
      "\n",
      "Total Runtime: 30 seconds\n",
      "\n",
      "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
      "    Runtime: (0.20%) 0 seconds [2.47 millisecondsÂ±0.59 milliseconds/batch]\n",
      "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
      "ðŸ”¢ - TOKENIZER: ðŸ“Š Counter\n",
      "    Runtime: (88.77%) 27 seconds [9 seconds and 89.86 millisecondsÂ±3 seconds and 101.20 milliseconds/batch]\n",
      "    Stats: {tokens: 20953163 [min=2, max=4439, 868.60Â±388/doc]}\n",
      "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 4\n",
      "    Runtime: (0.10%) 0 seconds [0.00 millisecondsÂ±0.01 milliseconds/doc]\n",
      "    Stats: {total: 24123, forwarded: 24066, dropped: 57}\n",
      "ðŸ’½ - WRITER: ðŸ¿ Jsonl\n",
      "    Runtime: (10.93%) 3 seconds [0.14 millisecondsÂ±0.28 milliseconds/doc]\n",
      "    Stats: {XXXXX.jsonl.gz: 24066, total: 24066, doc_len: 88402371 [min=5, max=18344, 3673.33Â±1446/doc], doc_len_tokens: 20952551 [min=2, max=4439, 870.63Â±386/doc]}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "ðŸ“‰ðŸ“‰ðŸ“‰ Stats ðŸ“‰ðŸ“‰ðŸ“‰\n",
       "\n",
       "Total Runtime: 30 seconds\n",
       "\n",
       "ðŸ“– - READER: ðŸ¤— HuggingFace\n",
       "    Runtime: (0.20%) 0 seconds [2.47 millisecondsÂ±0.59 milliseconds/batch]\n",
       "    Stats: {doc_len: 88405138 [min=5, max=18344, 3664.77Â±1455/doc], documents: 24123}\n",
       "ðŸ”¢ - TOKENIZER: ðŸ“Š Counter\n",
       "    Runtime: (88.77%) 27 seconds [9 seconds and 89.86 millisecondsÂ±3 seconds and 101.20 milliseconds/batch]\n",
       "    Stats: {tokens: 20953163 [min=2, max=4439, 868.60Â±388/doc]}\n",
       "ðŸ«‚ - DEDUP: ðŸŽ¯ MinHash stage 4\n",
       "    Runtime: (0.10%) 0 seconds [0.00 millisecondsÂ±0.01 milliseconds/doc]\n",
       "    Stats: {total: 24123, forwarded: 24066, dropped: 57}\n",
       "ðŸ’½ - WRITER: ðŸ¿ Jsonl\n",
       "    Runtime: (10.93%) 3 seconds [0.14 millisecondsÂ±0.28 milliseconds/doc]\n",
       "    Stats: {XXXXX.jsonl.gz: 24066, total: 24066, doc_len: 88402371 [min=5, max=18344, 3673.33Â±1446/doc], doc_len_tokens: 20952551 [min=2, max=4439, 870.63Â±386/doc]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 4: Filter duplicates\n",
    "# Uncomment to run with a real dataset\n",
    "#\n",
    "stage4 = LocalPipelineExecutor(\n",
    "    pipeline=[\n",
    "        reader,\n",
    "        TokensCounter(),  # Count tokens before and after deduplication\n",
    "        MinhashDedupFilter(\n",
    "            input_folder=f\"{output_base_path}/remove_ids\",\n",
    "            exclusion_writer=JsonlWriter(f\"{output_base_path}/removed\"),\n",
    "        ),\n",
    "        JsonlWriter(output_folder=f\"{output_base_path}/deduplicated_output\"),\n",
    "    ],\n",
    "    depends=stage3,\n",
    "    tasks=1,  # Match the number of tasks from stage 1\n",
    ")\n",
    "stage4.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deduplicated samples: 24066\n",
      "Number of removed samples: 57\n",
      "\n",
      "Sample statistics:\n",
      "Deduplicated avg length: 3673.33\n",
      "Removed avg length: 48.54\n",
      "\n",
      "Example removed samples:\n",
      "\n",
      "Removed sample 1:\n",
      " Let's dive into our next lesson: adding 8 + ...\n",
      "\n",
      "Removed sample 2:\n",
      " Now that we've learned how to multiply 1-digit numbers by 10, 100, and 1...\n",
      "\n",
      "Removed sample 3:\n",
      " **Topic C: Lessons 26-2...\n",
      "\n",
      "Removed sample 4:\n",
      " Dividing Whole Numbers by 1...\n",
      "\n",
      "Removed sample 5:\n",
      " Welcome to Unit 11: Operations and Algebra < 1...\n",
      "\n",
      "Removed sample 6:\n",
      " Welcome to Unit 17: Operations and Algebra 218-2...\n",
      "\n",
      "Removed sample 7:\n",
      " Welcome to Unit 17: Operations and Algebra 218-2...\n",
      "\n",
      "Removed sample 8:\n",
      " Welcome to Unit 18: Operations and Algebra 222-2...\n",
      "\n",
      "Removed sample 9:\n",
      " When working with powers of 1...\n",
      "\n",
      "Removed sample 10:\n",
      " Welcome to Unit 20: Operations and Algebra ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "output_base_path = \"dedup_output\"\n",
    "# Load deduplicated samples created from pipeline above\n",
    "dedup_path = Path(f\"{output_base_path}/deduplicated_output\")\n",
    "dedup_files = list(dedup_path.glob(\"*.jsonl.gz\"))\n",
    "dedup_samples = []\n",
    "for file in dedup_files:\n",
    "    with gzip.open(file, 'rt') as f:\n",
    "        for line in f:\n",
    "            dedup_samples.append(json.loads(line))\n",
    "\n",
    "# Load removed samples\n",
    "removed_path = Path(f\"{output_base_path}/removed\")\n",
    "removed_files = list(removed_path.glob(\"*.jsonl.gz\"))\n",
    "removed_samples = []\n",
    "for file in removed_files:\n",
    "    with gzip.open(file, 'rt') as f:\n",
    "        for line in f:\n",
    "            removed_samples.append(json.loads(line))\n",
    "\n",
    "print(f\"Number of deduplicated samples: {len(dedup_samples)}\")\n",
    "print(f\"Number of removed samples: {len(removed_samples)}\")\n",
    "\n",
    "# Compare some statistics\n",
    "if dedup_samples and removed_samples:\n",
    "    print(\"\\nSample statistics:\")\n",
    "    print(f\"Deduplicated avg length: {sum(len(s['text']) for s in dedup_samples) / len(dedup_samples):.2f}\")\n",
    "    print(f\"Removed avg length: {sum(len(s['text']) for s in removed_samples) / len(removed_samples):.2f}\")\n",
    "    \n",
    "    # Show a few examples of removed samples\n",
    "    print(\"\\nExample removed samples:\")\n",
    "    for i, sample in enumerate(removed_samples[:10]):\n",
    "        print(f\"\\nRemoved sample {i+1}:\")\n",
    "        print(sample['text'][:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decontamination Process\n",
    "\n",
    "Decontamination prevents data leakage by removing content that overlaps with evaluation benchmarks. This is especially important for synthetically generated data to ensure fair evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization and N-gram Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): # find out what and how this function does!\n",
    "    \"\"\"Normalize text by removing diacritics and tokenize.\"\"\"\n",
    "    text = \"\".join(c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\")\n",
    "    tokens = re.findall(\"\\\\w+\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_ngrams(tokens, n): # analyze what this function does!\n",
    "    \"\"\"Generate n-grams from tokens.\"\"\"\n",
    "    return set(zip(*[tokens[i:] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding N-gram Matches with Benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_ngrams_batch(batch, eval_ngrams, eval_datasets, eval_texts, ngram_len):\n",
    "    \"\"\"Find contaminated samples based on n-grams.\"\"\"\n",
    "    new_batch = {\"completion\": [], \"ngram\": [], \"bench_name\": [], \"bench_text\": []}\n",
    "    for completion in batch[\"text\"]:\n",
    "        tokens = tokenize(completion)\n",
    "        ngrams = get_ngrams(tokens, ngram_len)\n",
    "        for ngram in ngrams:\n",
    "            if ngram in eval_ngrams:\n",
    "                idx = eval_ngrams[ngram]\n",
    "                new_batch[\"completion\"].append(completion)\n",
    "                new_batch[\"ngram\"].append(ngram)\n",
    "                new_batch[\"bench_name\"].append(eval_datasets[idx])\n",
    "                new_batch[\"bench_text\"].append(eval_texts[idx])\n",
    "                break\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Matching for Better Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_strings(string1, string2):\n",
    "    \"\"\"Find matching parts between two strings.\"\"\"\n",
    "    matcher = difflib.SequenceMatcher(None, string1.lower(), string2.lower(), autojunk=False)\n",
    "    matching_blocks = matcher.get_matching_blocks()\n",
    "    matches = []\n",
    "    for block in matching_blocks:\n",
    "        start_a, start_b, length = block\n",
    "        if length > 5:  # Ignore very short matches\n",
    "            match = string1[start_a:start_a + length]\n",
    "            matches.append(match)\n",
    "    return matches\n",
    "\n",
    "def add_match_stats(example):\n",
    "    \"\"\"Add match statistics to identify contamination severity.\"\"\"\n",
    "    gen_text = \" \".join(tokenize(example[\"completion\"]))\n",
    "    bench_text = \" \".join(tokenize(example[\"bench_text\"]))\n",
    "    matching_parts = diff_strings(gen_text, bench_text)\n",
    "    match = \" \".join(\"\".join(matching_parts).split())\n",
    "    example[\"diff\"] = matching_parts\n",
    "    example[\"diff_ratio\"] = len(match) / len(bench_text) if len(bench_text) > 0 else 0\n",
    "    example[\"diff_length\"] = len(match)\n",
    "    example[\"longest_diff_part\"] = max(matching_parts, key=len, default=\"\")\n",
    "    example[\"longest_diff_part_length\"] = len(example[\"longest_diff_part\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Decontamination Pipeline\n",
    "\n",
    "The function below demonstrates the complete decontamination process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontaminate_dataset(train_dataset, eval_dataset, ngram_length=10, diff_threshold=0.5, num_proc=4):\n",
    "    \"\"\"\n",
    "    Decontaminate a dataset by removing examples that have significant overlap with evaluation benchmarks.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: Dataset to decontaminate\n",
    "        eval_dataset: Evaluation benchmark dataset\n",
    "        ngram_length: Length of n-grams to use for initial matching\n",
    "        diff_threshold: Similarity threshold for filtering (0.5 = 50% match)\n",
    "        num_proc: Number of processes for parallel processing\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (decontaminated dataset, contamination report)\n",
    "    \"\"\"\n",
    "    # Build n-gram index from evaluation data\n",
    "    eval_ngrams, eval_datasets, eval_texts = {}, [], []\n",
    "    \n",
    "    print(\"Building n-gram index from evaluation data...\")\n",
    "    for example in tqdm(eval_dataset):\n",
    "        tokens = tokenize(example[\"text\"])\n",
    "        ngrams = get_ngrams(tokens, ngram_length)\n",
    "        if ngrams:\n",
    "            idx = len(eval_texts)\n",
    "            eval_ngrams.update(zip(ngrams, [idx] * len(ngrams)))\n",
    "            eval_datasets.append(example.get(\"task_name\", \"unknown\"))\n",
    "            eval_texts.append(example[\"text\"])\n",
    "            \n",
    "    print(f\"Created n-gram index with {len(eval_ngrams)} unique n-grams\")\n",
    "    \n",
    "    # Find contamination candidates\n",
    "    print(\"Finding contamination candidates...\")\n",
    "    contamination_report = train_dataset.map(\n",
    "        lambda batch: retrieve_ngrams_batch(batch, eval_ngrams, eval_datasets, eval_texts, ngram_length),\n",
    "        batched=True, batch_size=1000, num_proc=num_proc, remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Add detailed matching statistics\n",
    "    print(\"Analyzing match details...\")\n",
    "    contamination_report = contamination_report.map(\n",
    "        lambda example: add_match_stats(example), num_proc=num_proc\n",
    "    )\n",
    "    \n",
    "    # Filter based on similarity threshold\n",
    "    print(\"Filtering contaminated examples...\")\n",
    "    contamination_report_filtered = contamination_report.filter(lambda x: x[\"diff_ratio\"] > diff_threshold)\n",
    "    \n",
    "    # Create decontaminated dataset\n",
    "    contaminated_completions = set(contamination_report_filtered[\"completion\"])\n",
    "    filtered_data = train_dataset.filter(lambda x: x[\"text\"] not in contaminated_completions)\n",
    "    \n",
    "    print(f\"Found {len(contamination_report_filtered)} contaminated examples out of {len(train_dataset)}\")\n",
    "    print(f\"Decontaminated dataset has {len(filtered_data)} examples\")\n",
    "    \n",
    "    return filtered_data, contamination_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building n-gram index from evaluation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10badebbaa2346a1a50f523eb8a2b371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created n-gram index with 93444 unique n-grams\n",
      "Finding contamination candidates...\n",
      "Analyzing match details...\n",
      "Filtering contaminated examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef7150ac0d04a81b7c1b94cd5b42cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 contaminated examples out of 24066\n",
      "Decontaminated dataset has 24062 examples\n"
     ]
    }
   ],
   "source": [
    "# Example usage (uncomment to run with real datasets)\n",
    "# train_dataset_name = \"HuggingFaceTB/cosmopedia-sample\" # or replace with \n",
    "train_dataset_name = \"dedup_output/deduplicated_output\"\n",
    "# eval_dataset_name = \"VGraf/unseen_wildchat_eval_subset\"\n",
    "eval_dataset_name = \"davanstrien/cosmopedia_sample\"\n",
    "# \n",
    "train_data = load_dataset(train_dataset_name, split=\"train\")\n",
    "eval_data = load_dataset(eval_dataset_name, split=\"train\")\n",
    "\n",
    "decontaminated_data, contamination_report = decontaminate_dataset(\n",
    "    train_data, \n",
    "    eval_data,\n",
    "    ngram_length=10,\n",
    "    diff_threshold=0.5,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Scoring for educational value\n",
    "\n",
    "Classifying educational value helps identify high-quality content for learning purposes. We'll fine-tune a BERT model to score educational value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1976\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:38\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/activations_tf.py:27\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1976\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1964\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1965\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1978\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1979\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1980\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1981\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1976\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:999\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/trainer.py:41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     42\u001b[39m     get_reporting_integration_callbacks,\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1964\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1965\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1978\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1979\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1980\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1981\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     AutoTokenizer,\n\u001b[32m      3\u001b[39m     DataCollatorWithPadding,\n\u001b[32m      4\u001b[39m     TrainingArguments,\n\u001b[32m      5\u001b[39m     Trainer,\n\u001b[32m      6\u001b[39m     AutoModelForSequenceClassification,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1964\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1962\u001b[39m     value = Placeholder\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1965\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1966\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NVIDIA teaching kits/aisg_github_course/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1978\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1979\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1980\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1981\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_educational_dataset(dataset_name, target_column=\"score\"):\n",
    "    \"\"\"Prepare a dataset for educational value classification training.\"\"\"\n",
    "    # Load dataset with annotations\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "    # Normalize scores to 0-5 range\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {target_column: np.clip(int(x[target_column]), 0, 5)},\n",
    "        num_proc=4,\n",
    "    )\n",
    "    \n",
    "    # Convert to classification labels\n",
    "    from datasets import ClassLabel\n",
    "    dataset = dataset.cast_column(\n",
    "        target_column, ClassLabel(names=[str(i) for i in range(6)])\n",
    "    )\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    dataset = dataset.train_test_split(\n",
    "        train_size=0.9, seed=42, stratify_by_column=target_column\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute evaluation metrics for educational value classification.\"\"\"\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.round(logits.squeeze()).clip(0, 5).astype(int)\n",
    "    labels = np.round(labels.squeeze()).astype(int)\n",
    "    \n",
    "    precision = precision_metric.compute(\n",
    "        predictions=preds, references=labels, average=\"macro\"\n",
    "    )[\"precision\"]\n",
    "    recall = recall_metric.compute(\n",
    "        predictions=preds, references=labels, average=\"macro\"\n",
    "    )[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    report = classification_report(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    print(\"Validation Report:\\n\" + report)\n",
    "    print(\"Confusion Matrix:\\n\" + str(cm))\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_educational_classifier(dataset, base_model_name, target_column=\"score\", output_dir=\"./edu_bert_model\"):\n",
    "    \"\"\"Train a BERT model for educational value classification.\"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=1,  # Regression task\n",
    "        classifier_dropout=0.0,\n",
    "        hidden_dropout_prob=0.0,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_name,\n",
    "        model_max_length=min(model.config.max_position_embeddings, 512),\n",
    "    )\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Preprocess data\n",
    "    def preprocess(examples):\n",
    "        batch = tokenizer(examples[\"text\"], truncation=True)\n",
    "        batch[\"labels\"] = np.float32(examples[target_column])\n",
    "        return batch\n",
    "    \n",
    "    processed_dataset = dataset.map(preprocess, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Freeze embedding layers to speed up training\n",
    "    for param in model.bert.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bert.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        save_steps=1000,\n",
    "        logging_steps=100,\n",
    "        learning_rate=3e-4,\n",
    "        num_train_epochs=5,\n",
    "        seed=0,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=64,\n",
    "        eval_on_start=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_dataset[\"train\"],\n",
    "        eval_dataset=processed_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment to run with real data)\n",
    "# \n",
    "# dataset_name = \"HuggingFaceTB/LLM_juries_fineweb_430k_annotations\"  \n",
    "# base_model_name = \"Snowflake/snowflake-arctic-embed-m\"\n",
    "# \n",
    "# # Prepare dataset\n",
    "# edu_dataset = prepare_educational_dataset(dataset_name, target_column=\"score\")\n",
    "# \n",
    "# # Train model\n",
    "# model, tokenizer = train_educational_classifier(\n",
    "#     edu_dataset, \n",
    "#     base_model_name, \n",
    "#     target_column=\"score\",\n",
    "#     output_dir=\"./edu_bert_model\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_educational_value(dataset, model_path, batch_size=32):\n",
    "    \"\"\"Apply educational value classification to a dataset.\"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Define preprocessing function\n",
    "    def predict_batch(batch):\n",
    "        # Tokenize and prepare inputs\n",
    "        inputs = tokenizer(batch[\"completion\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Process predictions\n",
    "        scores = outputs.logits.squeeze().cpu().numpy()\n",
    "        scores = np.clip(np.round(scores), 0, 5).astype(int)\n",
    "        \n",
    "        batch[\"educational_score\"] = scores\n",
    "        return batch\n",
    "    \n",
    "    # Apply model to dataset\n",
    "    scored_dataset = dataset.map(\n",
    "        predict_batch,\n",
    "        batched=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return scored_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment to run with real data)\n",
    "# \n",
    "# import torch\n",
    "# \n",
    "# dataset_to_score = load_dataset(\"HuggingFaceTB/cosmopedia-sample\", split=\"train\")\n",
    "# model_path = \"./edu_bert_model\"  # or use \"HuggingFaceTB/fineweb-edu-scorer\"\n",
    "# \n",
    "# scored_dataset = classify_educational_value(dataset_to_score, model_path)\n",
    "# \n",
    "# # Example of filtering by educational value\n",
    "# high_quality = scored_dataset.filter(lambda x: x[\"educational_score\"] >= 4)\n",
    "# print(f\"Original dataset: {len(scored_dataset)} examples\")\n",
    "# print(f\"High-quality subset: {len(high_quality)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Evaluation\n",
    "\n",
    "Evaluating the quality of processed data is crucial to ensure its effectiveness. We can use benchmark tests to assess model performance when trained on our filtered data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Testing with lighteval\n",
    "\n",
    "The Cosmopedia project uses the `lighteval` library to evaluate models on standard benchmarks. Below is an example of how to set up and run these evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of installing lighteval (uncomment to run)\n",
    "# \n",
    "# !git clone https://github.com/huggingface/lighteval.git\n",
    "# !cd lighteval && pip install '.[accelerate,quantization,adapters]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary benchmark used for evaluation is:\n",
    "\n",
    "**HellaSwag**: A challenging common sense reasoning benchmark that tests a model's ability to complete a sentence with the most natural continuation. This is particularly effective for measuring how well a model has learned common sense knowledge and language fluency from the training data.\n",
    "\n",
    "Using a single benchmark like HellaSwag provides an efficient way to measure the impact of different filtering steps without the complexity of multiple benchmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command to run HellaSwag evaluation (uncomment to run)\n",
    "# MODEL=\"your_model_checkpoint\"\n",
    "# OUTPUT_DIR=\"./eval_results\"\n",
    "# \n",
    "# !accelerate launch --num_processes=1 --main_process_port=29600 \\\n",
    "#     \"lighteval/run_evals_accelerate.py\" \\\n",
    "#     --model_args=\"pretrained=$MODEL\" \\\n",
    "#     --custom_tasks \"lighteval_tasks.py\" \\\n",
    "#     --output_dir $OUTPUT_DIR \\\n",
    "#     --override_batch_size 16 \\\n",
    "#     --tasks \"custom|hellaswag|0|1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Impact of Filtering Steps\n",
    "\n",
    "To understand the effectiveness of different filtering approaches, you can train models on different versions of your dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Raw dataset**: Generated data without any filtering\n",
    "2. **Cleaned dataset**: After boilerplate removal\n",
    "3. **Deduplicated dataset**: After removing duplicates\n",
    "4. **Decontaminated dataset**: After removing benchmark overlap\n",
    "5. **High-quality dataset**: After filtering by educational value\n",
    "\n",
    "Then evaluate each model on the benchmarks to compare performance differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comparative analysis for HellaSwag scores (pseudocode)\n",
    "# \n",
    "# results = {\n",
    "#     \"raw\": 0.45,           # Hypothetical HellaSwag accuracy \n",
    "#     \"cleaned\": 0.47,       # After boilerplate removal\n",
    "#     \"deduplicated\": 0.49,  # After removing duplicates\n",
    "#     \"decontaminated\": 0.51, # After removing benchmark overlap\n",
    "#     \"high_quality\": 0.53,  # After filtering by educational value\n",
    "# }\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# \n",
    "# # Plot results\n",
    "# df = pd.DataFrame({'HellaSwag Score': results.values()}, index=results.keys())\n",
    "# \n",
    "# ax = df.plot(kind=\"bar\", figsize=(10, 6), color='steelblue')\n",
    "# ax.set_title(\"Impact of Filtering Steps on HellaSwag Performance\")\n",
    "# ax.set_ylabel(\"Accuracy\")\n",
    "# ax.set_xlabel(\"Dataset Version\")\n",
    "# \n",
    "# # Add value labels on bars\n",
    "# for i, v in enumerate(results.values()):\n",
    "#     ax.text(i, v + 0.01, f\"{v:.2f}\", ha='center')\n",
    "#     \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covers essential data filtering techniques for synthetic data:\n",
    "\n",
    "1. **Post-Generation Cleanup**: Remove boilerplate and template artifacts with regex\n",
    "2. **Deduplication**: Eliminate redundant content with MinHash and LSH\n",
    "3. **Decontamination**: Prevent data leakage by removing evaluation benchmark overlap\n",
    "4. **Educational Value Classification**: Score and filter content by educational quality\n",
    "5. **Quality Evaluation**: Assess the impact of filtering steps on HellaSwag performance\n",
    "\n",
    "These techniques help create high-quality datasets for training language models. By measuring performance on HellaSwag after each filtering step, we can quantify the impact of our data processing pipeline and ensure we're creating the most effective training data possible.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
