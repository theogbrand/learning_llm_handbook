{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Fine-tuning with Direct Preference Optimization (DPO) using LoRA\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to implement Direct Preference Optimization (DPO) using only Low-Rank Adaptation (LoRA) for efficient preference fine-tuning of large language models.\n",
    "\n",
    "Preference fine-tuning is a powerful technique that optimizes language models based on human-provided preferences, teaching them to produce more aligned, helpful, harmless, and honest responses. \n",
    "\n",
    "**We'll cover two main steps:**\n",
    "1. Supervised Fine-Tuning (SFT) with LoRA\n",
    "2. Direct Preference Optimization (DPO) with LoRA\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic understanding of language models and fine-tuning\n",
    "- Access to a GPU for training\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n",
    "First, let's install the necessary packages for our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q accelerate peft bitsandbytes transformers trl sentencepiece datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised Fine-Tuning (SFT) with LoRA\n",
    "\n",
    "Before we can perform preference optimization, we first need to fine-tune our base model on instruction data. We'll use LoRA for parameter-efficient fine-tuning, which significantly reduces the number of trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preparation for SFT\n",
    "\n",
    "We'll prepare our dataset for the SFT stage. We'll use a sample of the UltraChat dataset for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a tokenizer to use its chat template\n",
    "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt using the chat template the model was trained with\"\"\"\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Load and format the dataset (limit to a small sample for demonstration)\n",
    "dataset = (\n",
    "    load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft\")\n",
    "    .shuffle(seed=42)\n",
    "    .select(range(1_000))  # Using a small sample for demonstration\n",
    ")\n",
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example to verify formatting\n",
    "print(dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model Setup with LoRA\n",
    "\n",
    "Now, let's set up our model with LoRA for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# We'll use a small pre-trained model for this demonstration\n",
    "base_model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# Load the model to train\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,  # Use float16 precision for efficiency\n",
    ")\n",
    "model.config.use_cache = False  # Disable KV caching for training\n",
    "model.config.pretraining_tp = 1  # Set tensor parallelism to 1\n",
    "\n",
    "# Load tokenizer if not already loaded\n",
    "if 'tokenizer' not in locals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # Right padding for causal language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 LoRA Configuration\n",
    "\n",
    "Next, we'll set up our LoRA configuration. LoRA works by adding small trainable rank decomposition matrices to key layers of the model, significantly reducing the number of parameters that need to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,               # Rank of the update matrices\n",
    "    lora_alpha=32,      # LoRA scaling factor\n",
    "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",        # Whether to train bias parameters\n",
    "    task_type=\"CAUSAL_LM\",  # Task type (causal language modeling)\n",
    "    # Target modules to apply LoRA to\n",
    "    target_modules=[\n",
    "        \"q_proj\",     # Query projection\n",
    "        \"k_proj\",     # Key projection\n",
    "        \"v_proj\",     # Value projection\n",
    "        \"o_proj\",     # Output projection\n",
    "        \"gate_proj\",  # Gating projection (for MLP blocks)\n",
    "        \"up_proj\",    # Upward projection (for MLP blocks)\n",
    "        \"down_proj\",  # Downward projection (for MLP blocks)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters information\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training Configuration for SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Output directory for saving models\n",
    "output_dir = \"./sft_lora_model\"\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,     # Batch size per GPU\n",
    "    gradient_accumulation_steps=4,     # Number of updates to accumulate before backward pass\n",
    "    learning_rate=2e-4,                # Learning rate\n",
    "    num_train_epochs=1,                # Number of training epochs\n",
    "    lr_scheduler_type=\"cosine\",        # Learning rate scheduler type\n",
    "    warmup_ratio=0.1,                  # Warmup ratio\n",
    "    optim=\"adamw_torch\",              # Optimizer\n",
    "    logging_steps=50,                  # Log every X steps\n",
    "    save_strategy=\"epoch\",             # Save at the end of each epoch\n",
    "    fp16=True,                         # Use mixed precision training\n",
    "    gradient_checkpointing=True,       # Use gradient checkpointing to save memory\n",
    "    remove_unused_columns=False,       # Keep all columns in the dataset\n",
    ")\n",
    "\n",
    "# Create SFT trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,    # Maximum sequence length\n",
    "    peft_config=lora_config # Use LoRA configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Train the Model with SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained LoRA adapters\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Test the SFT Model\n",
    "\n",
    "Let's see how our SFT model performs before moving to the DPO stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a test prompt in the model's chat format\n",
    "test_prompt = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"What are the key benefits of fine-tuning with LoRA?\"}],\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# Load the fine-tuned LoRA model\n",
    "sft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=sft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# Generate a response\n",
    "response = pipe(test_prompt)[0][\"generated_text\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Direct Preference Optimization (DPO) with LoRA\n",
    "\n",
    "Now that we have our SFT model, we can move on to preference optimization. DPO trains the model to prefer certain outputs over others based on human preference data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Preference Data\n",
    "\n",
    "We'll use a preference dataset containing pairs of responses where one is preferred over the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Function to format data for DPO\n",
    "def format_dpo_data(example):\n",
    "    \"\"\"Format the data for DPO training\"\"\"\n",
    "    if \"system\" in example:\n",
    "        system = example[\"system\"]\n",
    "        system_prompt = f\"<|system|>\\n{system}</s>\\n\"\n",
    "    else:\n",
    "        system_prompt = \"\"\n",
    "        \n",
    "    prompt = f\"<|user|>\\n{example['input']}</s>\\n<|assistant|>\\n\"\n",
    "    chosen = example['chosen'] + \"</s>\\n\"\n",
    "    rejected = example['rejected'] + \"</s>\\n\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": system_prompt + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# Load a preference dataset\n",
    "dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "\n",
    "# Filter the dataset to include only high-quality preference pairs\n",
    "dpo_dataset = dpo_dataset.filter(\n",
    "    lambda r: \n",
    "        r[\"status\"] != \"tie\" and  # Exclude tied preferences\n",
    "        r[\"chosen_score\"] >= 8 and  # Include only high-scored chosen responses\n",
    "        len(r[\"chosen\"]) < 1500 and  # Limit length for efficiency\n",
    "        len(r[\"rejected\"]) < 1500    # Limit length for efficiency\n",
    ")\n",
    "\n",
    "# Format the dataset for DPO\n",
    "dpo_dataset = dpo_dataset.map(\n",
    "    format_dpo_data, \n",
    "    remove_columns=dpo_dataset.column_names  # Remove original columns\n",
    ")\n",
    "\n",
    "# Take a smaller sample for demonstration\n",
    "dpo_dataset = dpo_dataset.shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first example\n",
    "print(\"PROMPT:\\n\", dpo_dataset[0][\"prompt\"])\n",
    "print(\"\\nCHOSEN:\\n\", dpo_dataset[0][\"chosen\"])\n",
    "print(\"\\nREJECTED:\\n\", dpo_dataset[0][\"rejected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Configure LoRA for DPO\n",
    "\n",
    "Now we'll set up another LoRA configuration for the DPO phase. We'll apply this to our SFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration for DPO\n",
    "dpo_lora_config = LoraConfig(\n",
    "    r=8,                # Using a smaller rank for DPO\n",
    "    lora_alpha=16,      # Scaling factor\n",
    "    lora_dropout=0.05,  # Dropout probability\n",
    "    bias=\"none\",        # Don't train bias parameters\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target modules to apply LoRA to (same as SFT)\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 DPO Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "# Output directory for DPO model\n",
    "dpo_output_dir = \"./dpo_lora_model\"\n",
    "\n",
    "# Configure DPO training arguments\n",
    "dpo_args = DPOConfig(\n",
    "    output_dir=dpo_output_dir,\n",
    "    per_device_train_batch_size=2,     # Smaller batch size due to memory constraints\n",
    "    gradient_accumulation_steps=4,     # Accumulate gradients to compensate for smaller batch size\n",
    "    learning_rate=5e-5,                # Lower learning rate for DPO\n",
    "    lr_scheduler_type=\"cosine\",        # Learning rate scheduler\n",
    "    max_steps=500,                     # Fixed number of training steps\n",
    "    warmup_ratio=0.1,                  # Warmup ratio\n",
    "    optim=\"adamw_torch\",              # Optimizer\n",
    "    logging_steps=10,                  # Log every X steps\n",
    "    save_strategy=\"steps\",             # Save strategy\n",
    "    save_steps=100,                    # Save every X steps\n",
    "    fp16=True,                         # Use mixed precision training\n",
    "    gradient_checkpointing=True,       # Use gradient checkpointing\n",
    "    remove_unused_columns=False,       # Keep all columns\n",
    "    beta=0.1,                          # DPO beta parameter (controls KL penalty)\n",
    "    max_prompt_length=512,             # Maximum prompt length\n",
    "    max_length=1024,                   # Maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Initialize DPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reference model is the SFT model (we don't need to load it again as DPOTrainer will handle this)\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=sft_model,             # Use the SFT model as the starting point\n",
    "    ref_model=None,              # DPOTrainer will create a copy of the model for reference\n",
    "    args=dpo_args,               # Training arguments\n",
    "    train_dataset=dpo_dataset,   # Training data\n",
    "    tokenizer=tokenizer,         # Tokenizer\n",
    "    peft_config=dpo_lora_config, # LoRA configuration for DPO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Train with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with DPO\n",
    "dpo_trainer.train()\n",
    "\n",
    "# Save the trained LoRA adapters\n",
    "dpo_trainer.model.save_pretrained(dpo_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Merge LoRA Adapters and Test\n",
    "\n",
    "Finally, let's test our DPO-tuned model to see the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, AutoPeftModelForCausalLM\n",
    "\n",
    "# Load base model and SFT LoRA\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load and merge SFT LoRA adapters\n",
    "sft_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "merged_sft_model = sft_model.merge_and_unload()\n",
    "\n",
    "# Load and merge DPO LoRA adapters on top of the SFT model\n",
    "dpo_model = PeftModel.from_pretrained(merged_sft_model, dpo_output_dir)\n",
    "merged_dpo_model = dpo_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various prompts to compare\n",
    "test_prompts = [\n",
    "    \"What are the ethical considerations when developing AI systems?\",\n",
    "    \"How can I optimize my study habits for better retention?\",\n",
    "    \"What are some strategies for dealing with workplace stress?\"\n",
    "]\n",
    "\n",
    "# Function to generate responses from a model\n",
    "def generate_response(model, prompt, tokenizer):\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False\n",
    "    )\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    return pipe(formatted_prompt)[0][\"generated_text\"]\n",
    "\n",
    "# Compare base model, SFT model, and DPO model\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n\\n===== PROMPT: {prompt} =====\")\n",
    "    \n",
    "    print(\"\\n----- BASE MODEL RESPONSE -----\")\n",
    "    print(generate_response(base_model, prompt, tokenizer))\n",
    "    \n",
    "    print(\"\\n----- SFT MODEL RESPONSE -----\")\n",
    "    print(generate_response(merged_sft_model, prompt, tokenizer))\n",
    "    \n",
    "    print(\"\\n----- DPO MODEL RESPONSE -----\")\n",
    "    print(generate_response(merged_dpo_model, prompt, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated a complete workflow for preference fine-tuning using LoRA:\n",
    "\n",
    "1. **SFT Stage**: We first performed supervised fine-tuning with LoRA adapters to teach the model to follow instructions.\n",
    "2. **DPO Stage**: We then applied Direct Preference Optimization with LoRA to improve the model's outputs based on human preferences.\n",
    "\n",
    "The key benefits of this approach include:\n",
    "\n",
    "- **Efficiency**: LoRA allows us to fine-tune models with minimal computational resources\n",
    "- **Quality**: DPO helps models produce more preferred outputs without reinforcement learning\n",
    "- **Modularity**: We can stack multiple LoRA adapters for different tasks\n",
    "\n",
    "By combining these techniques, we can create more helpful, accurate, and aligned language models with relatively limited computational resources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}