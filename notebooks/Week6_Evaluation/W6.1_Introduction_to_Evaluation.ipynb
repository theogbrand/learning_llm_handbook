{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6.1: Introduction to Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "* Understand the fundamentals of LLM evaluation\n",
    "* Learn how different prompt formulations affect model performance\n",
    "* Compare Multi-Choice Question Answering (MCQA) vs Generative evaluation approaches\n",
    "* Explore how to use lighteval for systematic evaluation\n",
    "* Analyze evaluation results using metrics like loglikelihood accuracy and exact match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=ï¿½ NOTE: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to LLM Evaluation\n",
    "\n",
    "Evaluating Large Language Models is crucial for understanding their capabilities and limitations. In this notebook, we'll explore how different prompt formulations can dramatically affect model performance on the same task.\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "Before we begin, let's clarify two important evaluation paradigms we'll be using:\n",
    "\n",
    "#### 1. **Multi-Choice Question Answering (MCQA) Evaluation**\n",
    "- **What it means**: The model calculates probability scores for each predefined choice\n",
    "- **How it works**: Given a question and choices A, B, C, D, the model assigns a likelihood score to each option\n",
    "- **What we measure**: Whether the model assigns the highest probability to the correct choice\n",
    "- **Metric used**: `loglikelihood_acc_norm` (normalized accuracy based on log probabilities)\n",
    "- **Key point**: The model doesn't generate new text - it only scores existing options\n",
    "\n",
    "#### 2. **Generative Evaluation**\n",
    "- **What it means**: The model generates free-form text as its answer\n",
    "- **How it works**: Given a question, the model produces its own text response token by token\n",
    "- **What we measure**: Whether the generated text matches the expected answer\n",
    "- **Metric used**: `quasi_exact_match` (flexible string matching)\n",
    "- **Key point**: The model creates new text rather than selecting from choices\n",
    "\n",
    "### Important Distinction\n",
    "\"Generative evaluation\" refers to **how the model produces answers** (by generating text), not how we judge those answers. It's different from \"LLM-as-a-judge\" evaluation, where another LLM would evaluate the quality of responses. We will cover LLM-as-a-judge evaluation in the another notebook.\n",
    "\n",
    "### Why Compare Both Approaches?\n",
    "The same model can perform very differently when:\n",
    "- Asked to select the best option (MCQA) vs. generate an answer (Generative)\n",
    "- Different prompt formulations can favor one approach over the other\n",
    "- Understanding these differences helps us design better evaluation strategies\n",
    "\n",
    "Let's explore these concepts through hands-on examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lighteval==0.6.2\n",
    "!pip install great-tables\n",
    "!pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from types import ModuleType\n",
    "from ast import literal_eval\n",
    "\n",
    "# For data visualization\n",
    "from great_tables import GT\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from datasets import load_dataset\n",
    "\n",
    "# For evaluation\n",
    "import lighteval\n",
    "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
    "from lighteval.models.model_config import BaseModelConfig, VLLMModelConfig\n",
    "from lighteval.pipeline import ParallelismManager, Pipeline, PipelineParameters\n",
    "from lighteval.metrics.metrics import Metrics\n",
    "from lighteval.tasks.lighteval_task import LightevalTaskConfig, Doc\n",
    "from lighteval.utils.utils import as_list, EnvConfig\n",
    "from lighteval.utils.imports import is_accelerate_available, is_tgi_available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cache directory and sample size for demonstration\n",
    "cache_dir = \"tmp\"\n",
    "max_samples = 10  # Small sample for demonstration; remove for full evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the ARC Dataset\n",
    "\n",
    "We'll use the AI2 Reasoning Challenge (ARC) dataset for our experiments. ARC is a multiple-choice science question dataset designed to test AI systems' reasoning abilities.\n",
    "\n",
    "Each question in ARC has:\n",
    "- A question text\n",
    "- Multiple choice options (typically 4)\n",
    "- A correct answer key\n",
    "\n",
    "Let's explore the dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine a sample from the ARC dataset\n",
    "arc_dataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "print(\"Sample ARC question:\")\n",
    "print(arc_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our Evaluation Task\n",
    "\n",
    "We'll create a custom task configuration that allows us to test different prompt formulations on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcExplorationTask(LightevalTaskConfig):\n",
    "    \"\"\"Custom task configuration for exploring different ARC prompt formulations.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, prompt_function, metric):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            prompt_function=prompt_function,\n",
    "            metric=as_list(metric),\n",
    "            # It's a custom defined task\n",
    "            suite=[\"custom\"],\n",
    "            # This defines our dataset and subsets\n",
    "            hf_repo=\"allenai/ai2_arc\",\n",
    "            hf_subset=\"ARC-Challenge\",\n",
    "            hf_avail_splits=[\"train\", \"validation\", \"test\"],\n",
    "            evaluation_splits=[\"test\"],\n",
    "            # The few shot sample selection parameters\n",
    "            few_shots_split=\"validation\",\n",
    "            few_shots_select=\"random\", \n",
    "            # Other task parameters\n",
    "            stop_sequence=[\".\", \"\\n\"],\n",
    "            generation_size=100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Evaluation Metrics\n",
    "\n",
    "We'll use two key metrics to evaluate our models:\n",
    "\n",
    "1. **Loglikelihood Accuracy (Normalized)**: For MCQA evaluation - measures if the model assigns the highest probability to the correct choice\n",
    "2. **Quasi-Exact Match**: For generative evaluation - measures if the generated text matches the expected answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our evaluation metrics\n",
    "metric_mcqa = Metrics.loglikelihood_acc_norm  # For multiple choice\n",
    "metric_gen = Metrics.quasi_exact_match        # For generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Formulations\n",
    "\n",
    "Now let's define different ways to present the same question to the model. Each formulation represents a different hypothesis about what might help the model perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Base Formulation\n",
    "The simplest possible prompt - just the question itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_base(line, task_name: str = None):\n",
    "    \"\"\"Base prompt: just the question.\n",
    "    \n",
    "    Example output:\n",
    "    'Cities control the amount of pollution?'\n",
    "    \"\"\"\n",
    "    query = f\"{line['question']}\"\n",
    "    choices = line[\"choices\"][\"text\"]\n",
    "    \n",
    "    return Doc(\n",
    "        task_name=task_name,\n",
    "        query=query,\n",
    "        choices=choices,\n",
    "        gold_index=line[\"choices\"][\"label\"].index(line[\"answerKey\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Context Formulation\n",
    "Add explicit \"Question:\" and \"Answer:\" labels to provide structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_context(line, task_name: str = None):\n",
    "    \"\"\"Add context with Question/Answer structure.\n",
    "    \n",
    "    Example output:\n",
    "    'Question: Cities control the amount of pollution?\n",
    "     Answer: '\n",
    "    \"\"\"\n",
    "    query = f\"Question: {line['question']}\"\n",
    "    query += \"\\nAnswer: \"\n",
    "    choices = line[\"choices\"][\"text\"]\n",
    "    \n",
    "    return Doc(\n",
    "        task_name=task_name,\n",
    "        query=query,\n",
    "        choices=choices,\n",
    "        gold_index=line[\"choices\"][\"label\"].index(line[\"answerKey\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Context with Choices\n",
    "Include the multiple choice options directly in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = list(string.ascii_uppercase)\n",
    "\n",
    "def arc_context_choices(line, task_name: str = None):\n",
    "    \"\"\"Include choices in the prompt.\n",
    "    \n",
    "    Example output:\n",
    "    'Question: Cities control the amount of pollution?\n",
    "     A. The air stays cleaner\n",
    "     B. The air becomes more polluted\n",
    "     C. Cars run more efficiently\n",
    "     D. It becomes safer to drive\n",
    "     Answer: '\n",
    "    \"\"\"\n",
    "    query = f\"Question: {line['question']}\\n\"\n",
    "    query += \"\\n\".join([f\"{letters[ix]}. {choice}\" \n",
    "                       for ix, choice in enumerate(line[\"choices\"][\"text\"])])\n",
    "    query += \"\\nAnswer: \"\n",
    "    choices = line[\"choices\"][\"text\"]\n",
    "    \n",
    "    return Doc(\n",
    "        task_name=task_name,\n",
    "        query=query,\n",
    "        choices=choices,\n",
    "        gold_index=line[\"choices\"][\"label\"].index(line[\"answerKey\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Context with Letter Labels\n",
    "Show choices in the prompt but expect letter responses (A, B, C, D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_context_labels(line, task_name: str = None):\n",
    "    \"\"\"Show choices but evaluate on letter labels.\n",
    "    \n",
    "    Same prompt as arc_context_choices, but expects 'A', 'B', 'C', or 'D' as answer.\n",
    "    \"\"\"\n",
    "    query = f\"Question: {line['question']}\\n\"\n",
    "    query += \"\\n\".join([f\"{letters[ix]}. {choice}\" \n",
    "                       for ix, choice in enumerate(line[\"choices\"][\"text\"])])\n",
    "    query += \"\\nAnswer: \"\n",
    "    # Key difference: choices are now letters instead of full text\n",
    "    choices = [letters[ix] for ix in range(len(line[\"choices\"][\"text\"]))]\n",
    "    \n",
    "    return Doc(\n",
    "        task_name=task_name,\n",
    "        query=query,\n",
    "        choices=choices,\n",
    "        gold_index=line[\"choices\"][\"label\"].index(line[\"answerKey\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Task Suite\n",
    "\n",
    "Now let's combine all our prompt formulations into a suite of tasks to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a module to hold our tasks\n",
    "task_module = ModuleType(\"task_module\")\n",
    "task_module.__file__ = \".\",\n",
    "task_module.TASKS_TABLE = [\n",
    "    ArcExplorationTask(\n",
    "        name=\"arc_base\", \n",
    "        prompt_function=arc_base, \n",
    "        metric=[metric_mcqa, metric_gen]\n",
    "    ),\n",
    "    ArcExplorationTask(\n",
    "        name=\"arc_context\", \n",
    "        prompt_function=arc_context, \n",
    "        metric=[metric_mcqa, metric_gen]\n",
    "    ),\n",
    "    ArcExplorationTask(\n",
    "        name=\"arc_context_choice\", \n",
    "        prompt_function=arc_context_choices, \n",
    "        metric=[metric_mcqa, metric_gen]\n",
    "    ),\n",
    "    ArcExplorationTask(\n",
    "        name=\"arc_context_labels\", \n",
    "        prompt_function=arc_context_labels, \n",
    "        metric=[metric_mcqa, metric_gen]\n",
    "    )\n",
    "]\n",
    "\n",
    "task_names = [\"arc_base\", \"arc_context\", \"arc_context_choice\", \"arc_context_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Evaluation\n",
    "\n",
    "Now let's set up and run our evaluation pipeline. We'll use a small model (SmolLM-1.7B) for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize accelerator if available\n",
    "if is_accelerate_available():\n",
    "    from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "    accelerator = Accelerator(kwargs_handlers=[InitProcessGroupKwargs(timeout=timedelta(seconds=3000))])\n",
    "else:\n",
    "    accelerator = None"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Set up evaluation tracking with save_details enabled\nevaluation_tracker = EvaluationTracker(\n    output_dir=cache_dir,\n    save_details=True,  # IMPORTANT: This saves full prompts and other details\n    # Optionally push results to HuggingFace Hub\n    # push_to_hub=True,\n    # hub_results_org=\"your_username\", \n)\n\n# Configure pipeline parameters\npipeline_params = PipelineParameters(\n    launcher_type=ParallelismManager.ACCELERATE,\n    env_config=EnvConfig(cache_dir=cache_dir),\n    override_batch_size=1,\n    max_samples=max_samples,  # Remove this for full evaluation\n    custom_tasks_directory=task_module\n)\n\n# Configure the model\nmodel_config = BaseModelConfig(\n    pretrained=\"HuggingFaceTB/SmolLM-1.7B\",\n    dtype=\"bfloat16\",\n    use_chat_template=False,\n)\n\n# Format tasks for evaluation (3 shots, 0 for generative)\ntasks = \",\".join([f\"custom|{task}|3|0\" for task in task_names])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the evaluation pipeline\n",
    "print(\"Starting evaluation...\")\n",
    "pipeline = Pipeline(\n",
    "    tasks=tasks,\n",
    "    pipeline_parameters=pipeline_params,\n",
    "    evaluation_tracker=evaluation_tracker,\n",
    "    model_config=model_config,\n",
    ")\n",
    "\n",
    "pipeline.evaluate()\n",
    "pipeline.save_and_push_results()\n",
    "print(\"Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Extract and process results\nresults = pipeline.get_results()[\"results\"]\nresults_processed = []\n\nfor eval_name, eval_results in results.items():\n    results_processed.append({\n        \"Prompt function\": (eval_name.split(\":\")[1] if \":\" in eval_name else eval_name).replace(\"_\", \" \"), \n        \"Quasi Exact Match\": eval_results[\"qem\"], \n        \"Normalized Accuracy\": eval_results[\"acc_norm\"]\n    })\n\n# Create a polars DataFrame\nresults_data = pl.from_dicts(results_processed, strict=False)\n\n# Display results in a nice table\n(GT(results_data)\n    .tab_header(\"Evaluation Results by Prompt Formulation\")\n    .tab_spanner(label=\"Metrics\", columns=[\"Quasi Exact Match\", \"Normalized Accuracy\"])\n)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Results\n",
    "\n",
    "Let's examine how different prompt formulations affected model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Viewing the Full Prompts Sent to the Model\n\nOne of the most valuable features of lighteval is the ability to see exactly what prompts are sent to the model. This is crucial for debugging and understanding model behavior. Let's examine the full prompts that were used in our evaluation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load the detailed results which contain the full prompts\nimport glob\n\n# Find the latest evaluation results\ndetail_files = glob.glob(f\"{cache_dir}/details/HuggingFaceTB/SmolLM-1.7B/**/*.parquet\", recursive=True)\n\n# Load one of the detail files to examine the full prompts\nif detail_files:\n    # Let's look at the arc_base task details\n    arc_base_file = [f for f in detail_files if \"arc_base\" in f][0]\n    details_df = load_dataset(\"parquet\", data_files=arc_base_file, split=\"train\")\n    \n    print(f\"Loaded details from: {arc_base_file}\")\n    print(f\"\\nColumns available in details: {details_df.column_names}\")\nelse:\n    print(\"No detail files found. Make sure the evaluation has completed.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Examining the Full Prompts\n\nNow let's look at the actual prompts that were sent to the model. The `full_prompt` column contains exactly what the model received, including any few-shot examples.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Let's examine the first full prompt from the arc_base task\nif 'full_prompt' in details_df.column_names:\n    print(\"=== FULL PROMPT SENT TO MODEL (arc_base formulation) ===\\n\")\n    print(details_df[0]['full_prompt'])\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    # Also show the individual components\n    print(\"Components of this evaluation sample:\")\n    print(f\"- Example (query only): {details_df[0]['example']}\")\n    print(f\"- Gold answer: {details_df[0]['gold']}\")\n    print(f\"- Model prediction: {details_df[0]['predictions']}\")\n    print(f\"- Choices provided: {details_df[0]['choices']}\")\nelse:\n    print(\"The 'full_prompt' column is not available. Make sure save_details=True was set.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Comparing Full Prompts Across Different Formulations\n\nLet's compare the full prompts for the same question across all our different formulations to see exactly how they differ.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load full prompts from all formulations and compare\nprompt_comparison = {}\n\nfor detail_file in detail_files:\n    # Extract task name from filename\n    task_name = None\n    for name in task_names:\n        if name in detail_file:\n            task_name = name\n            break\n    \n    if task_name:\n        # Load the details\n        task_details = load_dataset(\"parquet\", data_files=detail_file, split=\"train\")\n        \n        if 'full_prompt' in task_details.column_names:\n            # Store the first prompt for comparison\n            prompt_comparison[task_name] = task_details[0]['full_prompt']\n\n# Display the prompts side by side\nprint(\"=== COMPARING FULL PROMPTS ACROSS FORMULATIONS ===\\n\")\n\nfor task_name, full_prompt in sorted(prompt_comparison.items()):\n    print(f\"\\n{'='*60}\")\n    print(f\"TASK: {task_name}\")\n    print(f\"{'='*60}\")\n    print(full_prompt)\n    print(f\"{'='*60}\\n\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding Few-Shot Examples in the Prompts\n\nNotice that the full prompts include few-shot examples (3 in our case). These examples help the model understand the task format. Let's analyze how few-shot examples are included.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze the structure of few-shot examples\nif prompt_comparison:\n    # Take the arc_context_choice prompt as an example\n    example_prompt = prompt_comparison.get('arc_context_choice', list(prompt_comparison.values())[0])\n    \n    # Count the number of \"Question:\" occurrences to understand few-shot structure\n    num_questions = example_prompt.count(\"Question:\")\n    \n    print(f\"Number of questions in the full prompt: {num_questions}\")\n    print(f\"This includes {num_questions - 1} few-shot examples + 1 actual test question\\n\")\n    \n    # Split the prompt to show structure\n    prompt_parts = example_prompt.split(\"Question:\")\n    \n    if len(prompt_parts) > 2:\n        print(\"Structure of the prompt:\")\n        print(\"1. Few-shot examples (with answers)\")\n        print(\"2. Test question (without answer - model needs to complete this)\")\n        \n        # Show the last question (the actual test question)\n        print(f\"\\nThe actual test question starts with:\\nQuestion:{prompt_parts[-1][:200]}...\")\n        \n    # Check the metadata\n    if detail_files:\n        task_details = load_dataset(\"parquet\", data_files=detail_files[0], split=\"train\")\n        print(f\"\\nMetadata from evaluation:\")\n        print(f\"- Number of few-shot examples requested: {task_details[0].get('num_asked_few_shots', 'N/A')}\")\n        print(f\"- Number of effective few-shot examples: {task_details[0].get('num_effective_few_shots', 'N/A')}\")\n        print(f\"- Was the prompt truncated?: {task_details[0].get('truncated', 'N/A')}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Creating a Summary Table of Full Prompts\n\nLet's create a more structured view to understand the differences between formulations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a summary of how each formulation structures its prompts\nprompt_summaries = []\n\nfor detail_file in detail_files:\n    # Extract task name\n    task_name = None\n    for name in task_names:\n        if name in detail_file:\n            task_name = name\n            break\n    \n    if task_name:\n        # Load details\n        task_details = load_dataset(\"parquet\", data_files=detail_file, split=\"train\")\n        \n        if len(task_details) > 0:\n            sample = task_details[0]\n            \n            # Extract just the test question part (without few-shot examples)\n            full_prompt = sample.get('full_prompt', '')\n            # Find the last occurrence of the question pattern\n            test_question_start = full_prompt.rfind(sample['example']) if 'example' in sample else -1\n            \n            if test_question_start != -1:\n                test_question_only = full_prompt[test_question_start:]\n            else:\n                test_question_only = \"Could not extract test question\"\n            \n            prompt_summaries.append({\n                \"Task\": task_name.replace(\"_\", \" \").title(),\n                \"Full Prompt Length\": len(full_prompt),\n                \"Contains Few-Shot\": \"Yes\" if full_prompt.count(sample.get('example', '')) > 1 else \"No\",\n                \"Test Question Format\": test_question_only[:150] + \"...\" if len(test_question_only) > 150 else test_question_only,\n                \"Choices Format\": sample.get('choices', 'N/A'),\n                \"Gold Answer\": sample.get('gold', 'N/A'),\n                \"Model Prediction\": sample.get('predictions', 'N/A')\n            })\n\n# Display as a table\nif prompt_summaries:\n    summary_df = pl.from_dicts(prompt_summaries)\n    display(GT(summary_df)\n        .tab_header(\"Full Prompt Analysis Across Formulations\")\n        .fmt_number(columns=[\"Full Prompt Length\"], decimals=0)\n    )",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis: Looking at Individual Examples\n",
    "\n",
    "Let's examine specific examples to understand how the model responds to different prompt formulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detailed results\n",
    "path = f\"{cache_dir}/details/HuggingFaceTB/SmolLM-1.7B/\"\n",
    "results = {}\n",
    "\n",
    "for root, _, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if \"|\" in file:\n",
    "            eval_name = file.split(\"|\")[1]\n",
    "            results[eval_name] = load_dataset(\"parquet\", data_files=f\"{root}/{file}\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and display detailed results\n",
    "transformed_data = []\n",
    "keys = [\"example\", \"gold\", \"predictions\", \"metrics\"]\n",
    "\n",
    "# Look at first few examples\n",
    "for ix in range(min(5, max_samples)):\n",
    "    for key in keys:\n",
    "        cur_sample = {\"Sample\": f\"Sample {ix}\", \"Type\": key.capitalize()}\n",
    "        \n",
    "        for eval_name, df in sorted(results.items()):\n",
    "            try:\n",
    "                cur_result = literal_eval(results[eval_name][ix][key])\n",
    "                if isinstance(cur_result, list):\n",
    "                    if len(cur_result) == 1:\n",
    "                        cur_sample[eval_name] = cur_result[0]\n",
    "                    else:\n",
    "                        cur_sample[eval_name] = \"\\n\".join([str(i) for i in cur_result])\n",
    "                elif isinstance(cur_result, dict):\n",
    "                    for metric, value in cur_result.items():\n",
    "                        cur_sample[eval_name] = str(value)\n",
    "                        cur_sample[\"Type\"] = f\"{key.capitalize()}: {metric}\"\n",
    "            except:\n",
    "                cur_sample[eval_name] = results[eval_name][ix][key]\n",
    "                \n",
    "        # Replace newlines for better display\n",
    "        for k, v in cur_sample.items():\n",
    "            if isinstance(v, str):\n",
    "                cur_sample[k] = v.replace(\"\\n\", \"<br />\")\n",
    "        \n",
    "        transformed_data.append(cur_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display detailed comparison table\n",
    "pl_data = pl.from_dicts(transformed_data, strict=False, infer_schema_length=200)\n",
    "\n",
    "(GT(pl_data.head(20))\n",
    "    .tab_header(\"Comparing Different Prompt Formulations - Detailed Examples\")\n",
    "    .tab_spanner(label=\"Prompt Variations\", columns=cs.starts_with(\"arc\"))\n",
    "    .tab_stub(rowname_col=\"Type\", groupname_col=\"Sample\")\n",
    "    .fmt_markdown(columns=cs.starts_with(\"arc\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "From our experiments, we can observe several important patterns:\n",
    "\n",
    "1. **Base Format Challenges**: The simplest prompt (just the question) often struggles, especially in generative mode where the model may not understand it should provide a specific answer.\n",
    "\n",
    "2. **Context Helps**: Adding \"Question:\" and \"Answer:\" labels provides structure that helps the model understand the task format.\n",
    "\n",
    "3. **Choices in Prompt**: Including the multiple choice options directly in the prompt significantly helps the model select from valid options.\n",
    "\n",
    "4. **Label vs Full Text**: When choices are shown but the model must predict letters (A, B, C, D), performance may differ from predicting the full text answer.\n",
    "\n",
    "5. **MCQA vs Generation**: The same prompt can perform very differently when evaluated as multiple choice (loglikelihood) versus generation (exact match)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Try Different Models**: Replace `SmolLM-1.7B` with other models (e.g., `microsoft/phi-2`, `TinyLlama/TinyLlama-1.1B-Chat-v1.0`) and compare results.\n",
    "\n",
    "2. **Create New Prompt Formulations**: Design your own prompt templates. For example:\n",
    "   - Add instructions like \"Choose the best answer:\"\n",
    "   - Use different formatting (numbered lists instead of letters)\n",
    "   - Add few-shot examples in the prompt\n",
    "\n",
    "3. **Explore Other Datasets**: Adapt this code to work with other multiple-choice datasets like:\n",
    "   - `commonsense_qa`\n",
    "   - `winogrande`\n",
    "   - `hellaswag`\n",
    "\n",
    "4. **Analyze Error Patterns**: Look at which types of questions benefit most from different prompt formulations.\n",
    "\n",
    "5. **Statistical Significance**: With full evaluation (remove `max_samples`), calculate confidence intervals for the performance differences."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Additional Readings\n\nFor more in-depth understanding of LLM evaluation concepts, especially LLM-as-a-Judge approaches:\n\n- [LLM & VLM-as-a-Judge: A Comprehensive Guide](https://dylandigitalgarden.com/Dylan/2024/July/July+31%2C+2024+LLM+%26+VLM-as-a-Judge) - This article provides detailed insights into using LLMs as evaluators, including best practices, limitations, and real-world applications. While this notebook focused on traditional metrics (exact match, loglikelihood), LLM-as-a-Judge represents a more sophisticated evaluation paradigm that we'll explore in later notebooks.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how seemingly minor changes in prompt formulation can significantly impact model performance. Key takeaways:\n",
    "\n",
    "- **Prompt engineering matters**: The way we present tasks to LLMs can be as important as the model itself\n",
    "- **Evaluation methodology affects results**: MCQA vs generative evaluation can yield very different insights\n",
    "- **Systematic testing is crucial**: Tools like lighteval enable rigorous comparison of different approaches\n",
    "- **Context and structure help**: Adding labels and formatting generally improves model understanding\n",
    "\n",
    "In the next notebooks, we'll explore more advanced evaluation techniques including synthetic data generation and agentic evaluation methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}