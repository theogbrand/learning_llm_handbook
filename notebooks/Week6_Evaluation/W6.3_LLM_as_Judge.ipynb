{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6.3: LLM as Judge - Automated Evaluation with Language Models\n",
    "\n",
    "In this notebook, we'll explore how to use Large Language Models (LLMs) as judges to evaluate the quality of other model outputs. This approach, popularized by Stanford's AlpacaEval, provides a fast, scalable, and reproducible alternative to human evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Requirements\n",
    "\n",
    "This notebook can be run on:\n",
    "- **Google Colab**: Free tier (no GPU required)\n",
    "- **Local machine**: With Python 3.8+ and API access to OpenAI or other LLM providers\n",
    "- **Estimated time**: 45-60 minutes\n",
    "- **API costs**: Minimal (< $1 for all examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- ✅ Understand the fundamentals of LLM-as-Judge evaluation\n",
    "- ✅ Implement a basic pairwise comparison evaluator\n",
    "- ✅ Learn how to design effective evaluation prompts\n",
    "- ✅ Recognize common biases and limitations (length bias, position bias, etc.)\n",
    "- ✅ Understand length-controlled evaluation techniques\n",
    "- ✅ Apply batch evaluation for efficiency\n",
    "- ✅ Know when to use automated vs. human evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai pandas numpy matplotlib seaborn tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# For API calls\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up visualization defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to LLM-as-Judge Evaluation\n",
    "\n",
    "### What is LLM-as-Judge?\n",
    "\n",
    "LLM-as-Judge is an evaluation paradigm where we use a powerful language model (like GPT-4 or Claude) to assess the quality of outputs from other language models. This approach has gained popularity because:\n",
    "\n",
    "1. **Scalability**: Can evaluate thousands of examples quickly\n",
    "2. **Consistency**: Same judge applies same criteria across all evaluations\n",
    "3. **Cost-effectiveness**: Much cheaper than human evaluation\n",
    "4. **Reproducibility**: Results can be replicated with same prompts and models\n",
    "\n",
    "### The Core Innovation of AlpacaEval\n",
    "\n",
    "AlpacaEval revolutionized LLM evaluation by introducing several key concepts:\n",
    "\n",
    "#### 1. **Pairwise Comparison vs Absolute Scoring**\n",
    "Instead of asking \"Rate this response from 1-10\", AlpacaEval asks \"Which response is better, A or B?\". This is crucial because:\n",
    "- Humans (and LLMs) are better at relative comparisons than absolute scoring\n",
    "- It reduces variability and increases agreement rates\n",
    "- It matches how humans naturally evaluate quality\n",
    "\n",
    "#### 2. **Reference-Based Evaluation**\n",
    "AlpacaEval compares your model against strong reference models (like GPT-4):\n",
    "```\n",
    "Your Model Output <--compare--> GPT-4 Output\n",
    "                       |\n",
    "                   Judge LLM\n",
    "                       |\n",
    "                  Win Rate %\n",
    "```\n",
    "\n",
    "#### 3. **Win Rate as Primary Metric**\n",
    "The win rate tells you: \"What percentage of time does your model beat the reference?\"\n",
    "- 50% win rate = Your model is as good as GPT-4\n",
    "- 30% win rate = Your model wins 30% of the time\n",
    "- This single number is easy to interpret and track\n",
    "\n",
    "### How AlpacaEval Works Under the Hood\n",
    "\n",
    "The evaluation pipeline consists of three main stages:\n",
    "\n",
    "```\n",
    "Stage 1: Data Collection\n",
    "├── 805 diverse instructions (questions/tasks)\n",
    "├── Your model generates responses\n",
    "└── Reference model responses (pre-computed)\n",
    "\n",
    "Stage 2: Pairwise Evaluation\n",
    "├── For each instruction:\n",
    "│   ├── Create evaluation prompt\n",
    "│   ├── Send to judge LLM (GPT-4)\n",
    "│   └── Record verdict (win/loss/tie)\n",
    "└── Handle edge cases (refusals, errors)\n",
    "\n",
    "Stage 3: Aggregation\n",
    "├── Calculate win rate\n",
    "├── Compute confidence intervals\n",
    "├── Apply length control (v2.0)\n",
    "└── Generate leaderboard entry\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the AlpacaEval pipeline\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyBboxPatch, Rectangle\n",
    "\n",
    "def visualize_alpaca_eval_pipeline():\n",
    "    \"\"\"Create a visual representation of the AlpacaEval pipeline.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(5, 9.5, 'AlpacaEval Pipeline', fontsize=20, fontweight='bold', ha='center')\n",
    "    \n",
    "    # Stage 1: Inputs\n",
    "    input_box = FancyBboxPatch((0.5, 6.5), 2, 2, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(input_box)\n",
    "    ax.text(1.5, 7.5, '805 Test\\nInstructions', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Your Model\n",
    "    your_model = FancyBboxPatch((0.5, 4), 2, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                                facecolor='lightgreen', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(your_model)\n",
    "    ax.text(1.5, 4.75, 'Your Model', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Reference Model\n",
    "    ref_model = FancyBboxPatch((0.5, 2), 2, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='lightcoral', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(ref_model)\n",
    "    ax.text(1.5, 2.75, 'Reference\\n(GPT-4)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Arrows from instructions to models\n",
    "    ax.arrow(1.5, 6.5, 0, -0.8, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    ax.arrow(1.5, 6.5, 0, -2.8, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Stage 2: Comparison\n",
    "    compare_box = FancyBboxPatch((4, 3), 2.5, 3, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor='lightyellow', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(compare_box)\n",
    "    ax.text(5.25, 5.5, 'Pairwise\\nComparison', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(5.25, 4.8, 'Output A vs B', ha='center', va='center', fontsize=9)\n",
    "    ax.text(5.25, 4.3, 'for each\\ninstruction', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Judge LLM\n",
    "    judge_box = FancyBboxPatch((4.25, 0.5), 2, 1.5, boxstyle=\"round,pad=0.1\",\n",
    "                               facecolor='gold', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(judge_box)\n",
    "    ax.text(5.25, 1.25, 'Judge LLM\\n(GPT-4)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Arrows\n",
    "    ax.arrow(2.5, 4.75, 1.3, -0.3, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    ax.arrow(2.5, 2.75, 1.3, 0.7, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    ax.arrow(5.25, 3, 0, -0.8, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Stage 3: Results\n",
    "    results_box = FancyBboxPatch((7.5, 3), 2, 3, boxstyle=\"round,pad=0.1\",\n",
    "                                 facecolor='lightgray', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(results_box)\n",
    "    ax.text(8.5, 5.5, 'Results', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(8.5, 4.8, 'Win Rate: X%', ha='center', va='center', fontsize=9)\n",
    "    ax.text(8.5, 4.3, 'Confidence: ±Y%', ha='center', va='center', fontsize=9)\n",
    "    ax.text(8.5, 3.8, 'Length-controlled', ha='center', va='center', fontsize=9)\n",
    "    ax.text(8.5, 3.3, 'win rate: Z%', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Arrow from judge to results\n",
    "    ax.arrow(6.25, 1.25, 1.1, 2.2, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Add annotations\n",
    "    ax.text(3, 7.5, '1. Generate\\nResponses', ha='center', fontsize=9, style='italic')\n",
    "    ax.text(5.25, 7, '2. Compare\\nPairwise', ha='center', fontsize=9, style='italic')\n",
    "    ax.text(8.5, 7, '3. Aggregate\\nResults', ha='center', fontsize=9, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_alpaca_eval_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's set up our OpenAI client\n",
    "# You'll need to set your API key as an environment variable\n",
    "# For Google Colab: use Secrets tab or os.environ['OPENAI_API_KEY'] = 'your-key'\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
    ")\n",
    "\n",
    "# Let's create a simple example dataset\n",
    "evaluation_examples = [\n",
    "    {\n",
    "        \"instruction\": \"Explain what recursion is in programming.\",\n",
    "        \"output_a\": \"Recursion is when a function calls itself to solve a smaller instance of the same problem. It continues until reaching a base case that stops the recursion.\",\n",
    "        \"output_b\": \"Recursion in programming is a technique where a function calls itself. It's like a loop but uses function calls. Each recursive call works on a smaller piece of the problem until it reaches a simple case that can be solved directly. For example, calculating factorial: factorial(5) = 5 * factorial(4), and so on until factorial(1) = 1.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of France?\",\n",
    "        \"output_a\": \"The capital of France is Paris.\",\n",
    "        \"output_b\": \"Paris is the capital city of France. It's located in the north-central part of the country and is known for landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a haiku about coding.\",\n",
    "        \"output_a\": \"Lines of logic flow\\nBugs hide in syntax errors\\nDebugger finds peace\",\n",
    "        \"output_b\": \"Code\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(evaluation_examples)} evaluation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic LLM-as-Judge Implementation\n",
    "\n",
    "Let's implement a simple pairwise evaluator. We'll create a function that asks an LLM to compare two outputs and decide which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_prompt(instruction: str, output_a: str, output_b: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for the judge LLM to evaluate two outputs.\n",
    "    \n",
    "    Args:\n",
    "        instruction: The original instruction/question\n",
    "        output_a: First model's response\n",
    "        output_b: Second model's response\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt for the judge\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a helpful assistant that evaluates the quality of AI model responses.\n",
    "\n",
    "Given an instruction and two responses, determine which response is better.\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "Response A: {output_a}\n",
    "\n",
    "Response B: {output_b}\n",
    "\n",
    "Please evaluate which response is better by considering:\n",
    "1. Accuracy and correctness\n",
    "2. Helpfulness and completeness\n",
    "3. Clarity and coherence\n",
    "4. Following the instruction properly\n",
    "\n",
    "Output only \"A\" if Response A is better, \"B\" if Response B is better, or \"TIE\" if they are equally good.\n",
    "\n",
    "Your verdict:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def evaluate_pair(instruction: str, output_a: str, output_b: str, \n",
    "                  model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"\n",
    "    Use an LLM to judge which output is better.\n",
    "    \n",
    "    Returns: \"A\", \"B\", or \"TIE\"\n",
    "    \"\"\"\n",
    "    prompt = create_evaluation_prompt(instruction, output_a, output_b)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful evaluation assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,  # Use deterministic output\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        verdict = response.choices[0].message.content.strip().upper()\n",
    "        \n",
    "        # Validate the response\n",
    "        if verdict not in [\"A\", \"B\", \"TIE\"]:\n",
    "            print(f\"Warning: Unexpected verdict '{verdict}'. Defaulting to TIE.\")\n",
    "            return \"TIE\"\n",
    "        \n",
    "        return verdict\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return \"TIE\"\n",
    "\n",
    "\n",
    "# Test our evaluator on the first example\n",
    "example = evaluation_examples[0]\n",
    "verdict = evaluate_pair(\n",
    "    example[\"instruction\"], \n",
    "    example[\"output_a\"], \n",
    "    example[\"output_b\"]\n",
    ")\n",
    "\n",
    "print(f\"Instruction: {example['instruction']}\")\n",
    "print(f\"\\\\nVerdict: Output {verdict} is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Evaluation on Multiple Examples\n",
    "\n",
    "Now let's evaluate all our examples and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all examples\n",
    "results = []\n",
    "\n",
    "for i, example in enumerate(evaluation_examples):\n",
    "    print(f\"Evaluating example {i+1}/{len(evaluation_examples)}...\")\n",
    "    \n",
    "    verdict = evaluate_pair(\n",
    "        example[\"instruction\"],\n",
    "        example[\"output_a\"],\n",
    "        example[\"output_b\"]\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"instruction\": example[\"instruction\"],\n",
    "        \"output_a_length\": len(example[\"output_a\"]),\n",
    "        \"output_b_length\": len(example[\"output_b\"]),\n",
    "        \"verdict\": verdict,\n",
    "        \"winner\": \"output_a\" if verdict == \"A\" else (\"output_b\" if verdict == \"B\" else \"tie\")\n",
    "    })\n",
    "    \n",
    "    # Add a small delay to avoid rate limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"Evaluation Results:\")\n",
    "print(results_df[[\"instruction\", \"verdict\", \"output_a_length\", \"output_b_length\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Understanding Evaluation Prompts - The Heart of AlpacaEval\n",
    "\n",
    "The quality of LLM-as-Judge evaluation heavily depends on the prompt design. Let's explore the actual prompts used in AlpacaEval:\n",
    "\n",
    "### AlpacaEval's Prompt Evolution\n",
    "\n",
    "#### Version 1.0: Ranking-based Approach\n",
    "AlpacaEval 1.0 asked the judge to rank outputs from best to worst. This had issues:\n",
    "- Ties were ambiguous\n",
    "- Position bias was strong (first option preferred)\n",
    "- Hard to extract clean signal\n",
    "\n",
    "#### Version 2.0: Binary Classification with Logprobs\n",
    "The breakthrough came with using token probabilities:\n",
    "- Ask for a single token response ('m' or 'M')\n",
    "- Use logprobs to get confidence scores\n",
    "- Enables length-controlled evaluation\n",
    "\n",
    "### The Actual AlpacaEval Prompt Template\n",
    "\n",
    "Here's a simplified version of the real prompt used in AlpacaEval 2.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual AlpacaEval 2.0 style prompt template\n",
    "ALPACA_EVAL_V2_PROMPT = \"\"\"I need you to help me evaluate responses to instructions.\n",
    "\n",
    "For the instruction below, which response is better?\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "Response (a): {output_a}\n",
    "\n",
    "Response (b): {output_b}\n",
    "\n",
    "IMPORTANT: To ensure fair evaluation, please consider:\n",
    "- Helpfulness: Does it solve the user's problem?\n",
    "- Accuracy: Is the information correct?\n",
    "- Clarity: Is it easy to understand?\n",
    "- Completeness: Does it fully address the instruction?\n",
    "- Conciseness: Avoid preferring longer answers just for being longer\n",
    "\n",
    "Please respond with ONLY a single character:\n",
    "- \"a\" if response (a) is better\n",
    "- \"b\" if response (b) is better\n",
    "\n",
    "Your answer: \"\"\"\n",
    "\n",
    "def create_alpaca_v2_style_prompt(instruction: str, output_a: str, output_b: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt following AlpacaEval v2 style.\n",
    "    Key innovations:\n",
    "    1. Single token response for clean extraction\n",
    "    2. Explicit instruction to avoid length bias\n",
    "    3. Clear evaluation criteria\n",
    "    \"\"\"\n",
    "    return ALPACA_EVAL_V2_PROMPT.format(\n",
    "        instruction=instruction,\n",
    "        output_a=output_a,\n",
    "        output_b=output_b\n",
    "    )\n",
    "\n",
    "# The actual prompt used for batch evaluation in AlpacaEval\n",
    "ALPACA_BATCH_PROMPT = \"\"\"You are a helpful assistant that evaluates language model outputs.\n",
    "\n",
    "You will see {batch_size} examples. For each example:\n",
    "1. There is an instruction\n",
    "2. There are two responses: (a) and (b)\n",
    "3. You must choose which is better\n",
    "\n",
    "## Evaluation Criteria\n",
    "- Helpful: Addresses the user's needs\n",
    "- Harmless: Avoids unsafe or inappropriate content  \n",
    "- Honest: Provides accurate information\n",
    "- Clear: Easy to understand\n",
    "- Complete: Fully addresses the instruction\n",
    "\n",
    "IMPORTANT: Do not prefer longer responses unless the length adds value.\n",
    "\n",
    "{examples}\n",
    "\n",
    "## Your Task\n",
    "For each example, respond with just \"a\" or \"b\" on a new line.\n",
    "No explanations needed.\n",
    "\n",
    "Example 1:\"\"\"\n",
    "\n",
    "# Let's see how the prompt structure affects evaluation\n",
    "def compare_prompt_styles(instruction: str, output_a: str, output_b: str):\n",
    "    \"\"\"Compare different prompt styles and their effects.\"\"\"\n",
    "    \n",
    "    # Style 1: Basic prompt (our original)\n",
    "    basic_prompt = create_evaluation_prompt(instruction, output_a, output_b)\n",
    "    \n",
    "    # Style 2: AlpacaEval v2 style\n",
    "    alpaca_prompt = create_alpaca_v2_style_prompt(instruction, output_a, output_b)\n",
    "    \n",
    "    # Style 3: Detailed criteria prompt\n",
    "    detailed_prompt = f\"\"\"Evaluate these responses using specific criteria.\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "Response A: {output_a}\n",
    "\n",
    "Response B: {output_b}\n",
    "\n",
    "Rate each response on:\n",
    "1. Accuracy (factually correct?)\n",
    "2. Helpfulness (solves the problem?)\n",
    "3. Clarity (easy to understand?)\n",
    "4. Safety (appropriate content?)\n",
    "5. Completeness (fully addresses request?)\n",
    "\n",
    "Consider all factors, then output only \"A\" or \"B\" for the better response.\n",
    "\n",
    "Your verdict:\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"basic\": basic_prompt,\n",
    "        \"alpaca_v2\": alpaca_prompt,\n",
    "        \"detailed\": detailed_prompt\n",
    "    }\n",
    "\n",
    "# Example to show prompt differences\n",
    "example = {\n",
    "    \"instruction\": \"How do I make coffee?\",\n",
    "    \"output_a\": \"Add hot water to coffee grounds.\",\n",
    "    \"output_b\": \"To make coffee: 1) Boil water to 195-205°F, 2) Add 2 tablespoons of ground coffee per 6 oz of water, 3) Pour water over grounds, 4) Let steep for 4 minutes, 5) Filter and serve.\"\n",
    "}\n",
    "\n",
    "prompts = compare_prompt_styles(\n",
    "    example[\"instruction\"],\n",
    "    example[\"output_a\"],\n",
    "    example[\"output_b\"]\n",
    ")\n",
    "\n",
    "print(\"COMPARISON OF PROMPT STYLES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. BASIC PROMPT (Simple):\")\n",
    "print(\"-\"*40)\n",
    "print(prompts[\"basic\"][:200] + \"...\")\n",
    "\n",
    "print(\"\\n\\n2. ALPACA V2 STYLE (Single token):\")\n",
    "print(\"-\"*40)\n",
    "print(prompts[\"alpaca_v2\"][:300] + \"...\")\n",
    "\n",
    "print(\"\\n\\n3. DETAILED CRITERIA:\")\n",
    "print(\"-\"*40)\n",
    "print(prompts[\"detailed\"][:250] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Batch Evaluation for Efficiency\n",
    "\n",
    "Evaluating examples one by one can be slow and expensive. AlpacaEval uses batch evaluation to process multiple examples in a single API call. Let's implement this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Batch Evaluation for Efficiency - Real-World Optimization\n",
    "\n",
    "In production, evaluating thousands of examples requires optimization. AlpacaEval uses several strategies:\n",
    "\n",
    "### Key Optimization Strategies:\n",
    "\n",
    "1. **Batching**: Process multiple examples in one API call\n",
    "2. **Caching**: Store results to avoid re-evaluation\n",
    "3. **Parallel Processing**: Use multiple API calls concurrently\n",
    "4. **Smart Retries**: Handle failures gracefully\n",
    "5. **Cost Optimization**: Balance batch size vs API limits\n",
    "\n",
    "Let's implement a production-ready batch evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Optional, Callable\n",
    "import time\n",
    "\n",
    "class OptimizedBatchEvaluator:\n",
    "    \"\"\"\n",
    "    Production-ready batch evaluator with AlpacaEval-style optimizations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: str = \"gpt-3.5-turbo\",\n",
    "                 cache_dir: Optional[str] = \"./eval_cache\",\n",
    "                 max_workers: int = 5,\n",
    "                 max_retries: int = 3):\n",
    "        self.model = model\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.cache = {}  # In-memory cache (could use Redis in production)\n",
    "        self.max_workers = max_workers\n",
    "        self.max_retries = max_retries\n",
    "        \n",
    "        # Batch size optimization based on model\n",
    "        self.optimal_batch_sizes = {\n",
    "            \"gpt-3.5-turbo\": 10,      # Can handle larger batches\n",
    "            \"gpt-4\": 5,               # More expensive, smaller batches\n",
    "            \"gpt-4-turbo\": 8,         # Balance of cost and capability\n",
    "        }\n",
    "    \n",
    "    def _get_cache_key(self, instruction: str, output_a: str, output_b: str) -> str:\n",
    "        \"\"\"Generate deterministic cache key for an evaluation.\"\"\"\n",
    "        content = f\"{instruction}|{output_a}|{output_b}|{self.model}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def _create_optimized_batch_prompt(self, examples: List[Dict]) -> str:\n",
    "        \"\"\"Create an optimized batch prompt following AlpacaEval style.\"\"\"\n",
    "        # Use minimal tokens while maintaining clarity\n",
    "        prompt = f\"Evaluate {len(examples)} response pairs. Output only 'a' or 'b' for each.\\n\"\n",
    "        prompt += \"Criteria: helpful, accurate, clear. Avoid length bias.\\n\\n\"\n",
    "        \n",
    "        for i, ex in enumerate(examples, 1):\n",
    "            # Compact format to save tokens\n",
    "            prompt += f\"{i}. {ex['instruction'][:100]}{'...' if len(ex['instruction']) > 100 else ''}\\n\"\n",
    "            prompt += f\"a: {ex['output_a'][:200]}{'...' if len(ex['output_a']) > 200 else ''}\\n\"\n",
    "            prompt += f\"b: {ex['output_b'][:200]}{'...' if len(ex['output_b']) > 200 else ''}\\n\\n\"\n",
    "        \n",
    "        prompt += \"Verdicts (one per line):\\n\"\n",
    "        return prompt\n",
    "    \n",
    "    def _evaluate_batch_with_retry(self, batch: List[Dict], attempt: int = 1) -> List[str]:\n",
    "        \"\"\"Evaluate a batch with retry logic.\"\"\"\n",
    "        if attempt > self.max_retries:\n",
    "            return [\"TIE\"] * len(batch)\n",
    "        \n",
    "        try:\n",
    "            prompt = self._create_optimized_batch_prompt(batch)\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert evaluator. Be concise.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=len(batch) * 3,  # Allow for some extra tokens\n",
    "                timeout=30  # Add timeout\n",
    "            )\n",
    "            \n",
    "            # Parse verdicts robustly\n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            verdicts = []\n",
    "            \n",
    "            for line in response_text.split('\\n'):\n",
    "                line = line.strip().lower()\n",
    "                if line in ['a', 'b']:\n",
    "                    verdicts.append(line.upper())\n",
    "                elif 'a' in line and 'b' not in line:\n",
    "                    verdicts.append('A')\n",
    "                elif 'b' in line and 'a' not in line:\n",
    "                    verdicts.append('B')\n",
    "            \n",
    "            # Pad with TIEs if needed\n",
    "            while len(verdicts) < len(batch):\n",
    "                verdicts.append('TIE')\n",
    "            \n",
    "            return verdicts[:len(batch)]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Batch evaluation failed (attempt {attempt}): {e}\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            return self._evaluate_batch_with_retry(batch, attempt + 1)\n",
    "    \n",
    "    def evaluate_dataset_parallel(self, examples: List[Dict], \n",
    "                                 show_progress: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Evaluate dataset with parallel processing and optimizations.\n",
    "        \n",
    "        This mimics AlpacaEval's approach:\n",
    "        1. Check cache first\n",
    "        2. Batch remaining examples optimally\n",
    "        3. Process batches in parallel\n",
    "        4. Handle failures gracefully\n",
    "        \"\"\"\n",
    "        results = [None] * len(examples)\n",
    "        to_evaluate = []\n",
    "        \n",
    "        # Step 1: Check cache\n",
    "        for i, example in enumerate(examples):\n",
    "            cache_key = self._get_cache_key(\n",
    "                example[\"instruction\"],\n",
    "                example[\"output_a\"],\n",
    "                example[\"output_b\"]\n",
    "            )\n",
    "            \n",
    "            if cache_key in self.cache:\n",
    "                results[i] = self.cache[cache_key]\n",
    "            else:\n",
    "                to_evaluate.append((i, example))\n",
    "        \n",
    "        if show_progress:\n",
    "            print(f\"Found {len(examples) - len(to_evaluate)} cached results\")\n",
    "            print(f\"Need to evaluate {len(to_evaluate)} examples\")\n",
    "        \n",
    "        # Step 2: Optimal batching\n",
    "        batch_size = self.optimal_batch_sizes.get(self.model, 5)\n",
    "        batches = []\n",
    "        \n",
    "        for i in range(0, len(to_evaluate), batch_size):\n",
    "            batch_indices = []\n",
    "            batch_examples = []\n",
    "            \n",
    "            for j in range(i, min(i + batch_size, len(to_evaluate))):\n",
    "                idx, example = to_evaluate[j]\n",
    "                batch_indices.append(idx)\n",
    "                batch_examples.append(example)\n",
    "            \n",
    "            batches.append((batch_indices, batch_examples))\n",
    "        \n",
    "        # Step 3: Parallel processing\n",
    "        if show_progress:\n",
    "            pbar = tqdm(total=len(batches), desc=\"Processing batches\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_batch = {\n",
    "                executor.submit(self._evaluate_batch_with_retry, batch_examples): (batch_indices, batch_examples)\n",
    "                for batch_indices, batch_examples in batches\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_indices, batch_examples = future_to_batch[future]\n",
    "                try:\n",
    "                    verdicts = future.result()\n",
    "                    \n",
    "                    # Store results and update cache\n",
    "                    for idx, example, verdict in zip(batch_indices, batch_examples, verdicts):\n",
    "                        result = {\n",
    "                            \"instruction\": example[\"instruction\"],\n",
    "                            \"verdict\": verdict,\n",
    "                            \"output_a_len\": len(example[\"output_a\"]),\n",
    "                            \"output_b_len\": len(example[\"output_b\"]),\n",
    "                            \"model\": self.model\n",
    "                        }\n",
    "                        results[idx] = result\n",
    "                        \n",
    "                        # Update cache\n",
    "                        cache_key = self._get_cache_key(\n",
    "                            example[\"instruction\"],\n",
    "                            example[\"output_a\"],\n",
    "                            example[\"output_b\"]\n",
    "                        )\n",
    "                        self.cache[cache_key] = result\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Batch failed completely: {e}\")\n",
    "                \n",
    "                if show_progress:\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        if show_progress:\n",
    "            pbar.close()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_statistics(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate AlpacaEval-style statistics.\"\"\"\n",
    "        total = len(results)\n",
    "        wins = sum(1 for r in results if r and r[\"verdict\"] == \"B\")\n",
    "        losses = sum(1 for r in results if r and r[\"verdict\"] == \"A\")\n",
    "        ties = sum(1 for r in results if r and r[\"verdict\"] == \"TIE\")\n",
    "        \n",
    "        # Win rate calculation (excluding ties like AlpacaEval)\n",
    "        win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "        \n",
    "        # Calculate average length ratio\n",
    "        length_ratios = []\n",
    "        for r in results:\n",
    "            if r and r[\"output_a_len\"] > 0:\n",
    "                length_ratios.append(r[\"output_b_len\"] / r[\"output_a_len\"])\n",
    "        \n",
    "        avg_length_ratio = np.mean(length_ratios) if length_ratios else 1.0\n",
    "        \n",
    "        return {\n",
    "            \"total_examples\": total,\n",
    "            \"wins\": wins,\n",
    "            \"losses\": losses,\n",
    "            \"ties\": ties,\n",
    "            \"win_rate\": win_rate,\n",
    "            \"win_rate_percentage\": win_rate * 100,\n",
    "            \"avg_length_ratio\": avg_length_ratio,\n",
    "            \"evaluation_model\": self.model\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate the optimized evaluator\n",
    "print(\"OPTIMIZED BATCH EVALUATION DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a larger test dataset\n",
    "test_dataset = []\n",
    "instructions = [\n",
    "    \"Explain quantum computing\",\n",
    "    \"What is machine learning?\",\n",
    "    \"How do I make pasta?\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"Explain photosynthesis\",\n",
    "    \"What is blockchain?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"What is climate change?\",\n",
    "    \"Explain the water cycle\",\n",
    "    \"What is artificial intelligence?\"\n",
    "]\n",
    "\n",
    "for i, instruction in enumerate(instructions):\n",
    "    test_dataset.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"output_a\": f\"Short answer to: {instruction}\",\n",
    "        \"output_b\": f\"This is a detailed explanation about {instruction}. \" * 3\n",
    "    })\n",
    "\n",
    "# Initialize and run evaluator\n",
    "evaluator = OptimizedBatchEvaluator(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_workers=3\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.evaluate_dataset_parallel(test_dataset)\n",
    "\n",
    "# Calculate and display statistics\n",
    "stats = evaluator.calculate_statistics(results)\n",
    "\n",
    "print(\"\\nEVALUATION STATISTICS\")\n",
    "print(\"=\"*40)\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Show a few example results\n",
    "print(\"\\nSAMPLE RESULTS\")\n",
    "print(\"=\"*40)\n",
    "for i in range(min(3, len(results))):\n",
    "    r = results[i]\n",
    "    if r:\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Instruction: {r['instruction'][:50]}...\")\n",
    "        print(f\"  Verdict: {r['verdict']}\")\n",
    "        print(f\"  Length ratio: {r['output_b_len']/r['output_a_len']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate common biases in LLM evaluation\n",
    "\n",
    "# 1. Length Bias - LLMs often prefer longer responses\n",
    "length_bias_examples = [\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of Japan?\",\n",
    "        \"output_a\": \"Tokyo\",\n",
    "        \"output_b\": \"The capital of Japan is Tokyo, which is located on the eastern coast of Honshu, the largest of Japan's four main islands. Tokyo has been Japan's capital since 1868, when it was renamed from Edo during the Meiji Restoration. It's not only the political center but also the economic and cultural heart of Japan, with a metropolitan area population exceeding 37 million people, making it the world's most populous metropolitan area.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What color is the sky?\",\n",
    "        \"output_a\": \"Blue\",\n",
    "        \"output_b\": \"The sky appears blue during the day due to a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with gas molecules. Blue light waves are shorter than other colors, so they are scattered in all directions by the tiny molecules of air in Earth's atmosphere. This is why we perceive the sky as blue during clear daylight hours.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2. Position Bias - Order can affect judgment\n",
    "def test_position_bias(instruction: str, output_1: str, output_2: str):\n",
    "    \"\"\"Test if the order of responses affects evaluation.\"\"\"\n",
    "    # Test both orders\n",
    "    verdict_ab = evaluate_pair(instruction, output_1, output_2)\n",
    "    verdict_ba = evaluate_pair(instruction, output_2, output_1)\n",
    "    \n",
    "    # Convert verdicts for comparison\n",
    "    if verdict_ba == \"A\":\n",
    "        verdict_ba_converted = \"B\"\n",
    "    elif verdict_ba == \"B\":\n",
    "        verdict_ba_converted = \"A\"\n",
    "    else:\n",
    "        verdict_ba_converted = \"TIE\"\n",
    "    \n",
    "    return {\n",
    "        \"order_AB\": verdict_ab,\n",
    "        \"order_BA_converted\": verdict_ba_converted,\n",
    "        \"consistent\": verdict_ab == verdict_ba_converted\n",
    "    }\n",
    "\n",
    "# 3. Analyze length bias\n",
    "print(\"Testing Length Bias:\")\n",
    "print(\"=\"*50)\n",
    "for example in length_bias_examples:\n",
    "    verdict = evaluate_pair(example[\"instruction\"], example[\"output_a\"], example[\"output_b\"])\n",
    "    print(f\"\\\\nInstruction: {example['instruction']}\")\n",
    "    print(f\"Output A length: {len(example['output_a'])} chars\")\n",
    "    print(f\"Output B length: {len(example['output_b'])} chars\")\n",
    "    print(f\"Verdict: {verdict}\")\n",
    "    print(f\"Longer response preferred: {'Yes' if verdict == 'B' else 'No'}\")\n",
    "\n",
    "# 4. Test position bias\n",
    "print(\"\\\\n\\\\nTesting Position Bias:\")\n",
    "print(\"=\"*50)\n",
    "position_example = {\n",
    "    \"instruction\": \"What is machine learning?\",\n",
    "    \"output_1\": \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "    \"output_2\": \"Machine learning allows computers to learn patterns from data without explicit programming.\"\n",
    "}\n",
    "\n",
    "position_result = test_position_bias(\n",
    "    position_example[\"instruction\"],\n",
    "    position_example[\"output_1\"],\n",
    "    position_example[\"output_2\"]\n",
    ")\n",
    "\n",
    "print(f\"Instruction: {position_example['instruction']}\")\n",
    "print(f\"When A is first: {position_result['order_AB']}\")\n",
    "print(f\"When B is first (converted): {position_result['order_BA_converted']}\")\n",
    "print(f\"Consistent across orders: {position_result['consistent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Biases Statistically\n",
    "\n",
    "Let's create a more comprehensive analysis of how output length affects evaluation outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset to analyze length bias\n",
    "def create_length_bias_dataset():\n",
    "    \"\"\"Create examples with varying length differences.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    # Same content, different lengths\n",
    "    base_answers = [\n",
    "        (\"2+2=4\", \"The sum of 2 and 2 equals 4, which is a basic arithmetic fact.\"),\n",
    "        (\"Paris\", \"Paris is the capital city of France, known for the Eiffel Tower.\"),\n",
    "        (\"H2O\", \"Water has the chemical formula H2O, consisting of hydrogen and oxygen.\"),\n",
    "        (\"1969\", \"The moon landing occurred in 1969, a historic achievement for humanity.\"),\n",
    "        (\"Python\", \"Python is a popular programming language known for its readability.\")\n",
    "    ]\n",
    "    \n",
    "    instructions = [\n",
    "        \"What is 2+2?\",\n",
    "        \"What is the capital of France?\",\n",
    "        \"What is the chemical formula for water?\",\n",
    "        \"When was the moon landing?\",\n",
    "        \"Name a popular programming language.\"\n",
    "    ]\n",
    "    \n",
    "    for i, (instruction, (short, long)) in enumerate(zip(instructions, base_answers)):\n",
    "        examples.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"output_a\": short,\n",
    "            \"output_b\": long,\n",
    "            \"length_diff\": len(long) - len(short)\n",
    "        })\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Analyze length bias\n",
    "length_examples = create_length_bias_dataset()\n",
    "length_results = []\n",
    "\n",
    "print(\"Analyzing Length Bias Across Examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for example in length_examples:\n",
    "    verdict = evaluate_pair(\n",
    "        example[\"instruction\"],\n",
    "        example[\"output_a\"],\n",
    "        example[\"output_b\"]\n",
    "    )\n",
    "    \n",
    "    length_results.append({\n",
    "        \"instruction\": example[\"instruction\"],\n",
    "        \"verdict\": verdict,\n",
    "        \"length_diff\": example[\"length_diff\"],\n",
    "        \"preferred_longer\": verdict == \"B\"\n",
    "    })\n",
    "    \n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "length_df = pd.DataFrame(length_results)\n",
    "\n",
    "# Calculate statistics\n",
    "prefer_longer_rate = length_df[\"preferred_longer\"].mean()\n",
    "print(f\"\\\\nProportion preferring longer response: {prefer_longer_rate:.2%}\")\n",
    "print(f\"Average length difference: {length_df['length_diff'].mean():.1f} characters\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot 1: Bar chart of preferences\n",
    "plt.subplot(1, 2, 1)\n",
    "verdict_counts = length_df[\"verdict\"].value_counts()\n",
    "plt.bar(verdict_counts.index, verdict_counts.values)\n",
    "plt.title(\"Distribution of Verdicts\")\n",
    "plt.xlabel(\"Verdict\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Plot 2: Length difference vs preference\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(length_df[\"length_diff\"], \n",
    "           length_df[\"preferred_longer\"].astype(int),\n",
    "           alpha=0.6, s=100)\n",
    "plt.xlabel(\"Length Difference (chars)\")\n",
    "plt.ylabel(\"Preferred Longer (1=Yes, 0=No)\")\n",
    "plt.title(\"Length Difference vs Preference\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis: List biases\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"Other Common Biases in LLM Evaluation:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. **Self-Preference Bias**: Models tend to prefer outputs similar to their training style\n",
    "2. **Format Bias**: Preference for bullet points, numbered lists, or structured formats\n",
    "3. **Verbosity Bias**: Beyond length, preference for elaborate explanations\n",
    "4. **Recency Bias**: Later information in the prompt may be weighted more heavily\n",
    "5. **Instruction-Following Bias**: Strict interpretation vs. helpful interpretation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Understanding Logprobs and Length-Controlled Evaluation\n",
    "\n",
    "### What are Logprobs?\n",
    "\n",
    "Logprobs (log probabilities) are the natural logarithm of the probability that a model assigns to each token. They tell us how confident the model is about its choice.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Probability**: How likely the model thinks a token is (0 to 1)\n",
    "- **Log Probability**: The natural log of probability (negative values)\n",
    "- **Why logs?**: Prevents numerical underflow and makes math easier\n",
    "\n",
    "```\n",
    "Example:\n",
    "Token \"A\" probability = 0.7  → logprob = ln(0.7) = -0.357\n",
    "Token \"B\" probability = 0.3  → logprob = ln(0.3) = -1.204\n",
    "\n",
    "Higher logprob = More confident\n",
    "```\n",
    "\n",
    "### How AlpacaEval Uses Logprobs\n",
    "\n",
    "AlpacaEval 2.0's key innovation is using logprobs to control for length bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_length_controlled_evaluation(instruction: str, output_a: str, output_b: str,\n",
    "                                         model: str = \"gpt-3.5-turbo\") -> Dict:\n",
    "    \"\"\"\n",
    "    Detailed implementation showing how AlpacaEval uses logprobs for length control.\n",
    "    \n",
    "    The key insight: If the judge is very confident (high probability) about \n",
    "    preferring the longer output, we should be suspicious of length bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    # AlpacaEval v2 uses single-token responses\n",
    "    prompt = f\"\"\"I need you to help me evaluate responses to instructions.\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "Response m: {output_a}\n",
    "\n",
    "Response M: {output_b}\n",
    "\n",
    "Please respond with just 'm' or 'M' to indicate which is better.\n",
    "IMPORTANT: Focus on quality, not length.\n",
    "\n",
    "Your answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=1,\n",
    "            logprobs=True,\n",
    "            top_logprobs=5  # Get top 5 token probabilities\n",
    "        )\n",
    "        \n",
    "        # Extract the verdict and logprobs\n",
    "        verdict_token = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Get logprobs for both options\n",
    "        logprobs_data = {}\n",
    "        if hasattr(response.choices[0], 'logprobs') and response.choices[0].logprobs:\n",
    "            for item in response.choices[0].logprobs.content[0].top_logprobs:\n",
    "                token = item.token\n",
    "                if token in ['m', 'M']:\n",
    "                    logprobs_data[token] = {\n",
    "                        'logprob': item.logprob,\n",
    "                        'prob': np.exp(item.logprob)\n",
    "                    }\n",
    "        \n",
    "        # Calculate length features\n",
    "        len_a, len_b = len(output_a), len(output_b)\n",
    "        length_ratio = len_b / len_a if len_a > 0 else 1.0\n",
    "        log_length_ratio = np.log(length_ratio) if length_ratio > 0 else 0\n",
    "        \n",
    "        # AlpacaEval's length control formula (simplified)\n",
    "        # The idea: Adjust confidence based on length difference\n",
    "        if verdict_token == 'M' and length_ratio > 1:\n",
    "            # Model prefers the longer output\n",
    "            # Reduce confidence proportionally to length difference\n",
    "            base_prob = logprobs_data.get('M', {}).get('prob', 0.5)\n",
    "            \n",
    "            # Length penalty factor (this is simplified; real formula is more complex)\n",
    "            length_penalty = 1 / (1 + 0.5 * log_length_ratio)\n",
    "            adjusted_prob = base_prob * length_penalty\n",
    "            \n",
    "        elif verdict_token == 'm' and length_ratio < 1:\n",
    "            # Model prefers the longer output (A is longer)\n",
    "            base_prob = logprobs_data.get('m', {}).get('prob', 0.5)\n",
    "            length_penalty = 1 / (1 + 0.5 * abs(log_length_ratio))\n",
    "            adjusted_prob = base_prob * length_penalty\n",
    "            \n",
    "        else:\n",
    "            # No length adjustment needed\n",
    "            base_prob = logprobs_data.get(verdict_token, {}).get('prob', 0.5)\n",
    "            adjusted_prob = base_prob\n",
    "            length_penalty = 1.0\n",
    "        \n",
    "        return {\n",
    "            \"verdict\": \"A\" if verdict_token == 'm' else \"B\",\n",
    "            \"verdict_token\": verdict_token,\n",
    "            \"logprobs_m\": logprobs_data.get('m', {}).get('logprob', None),\n",
    "            \"logprobs_M\": logprobs_data.get('M', {}).get('logprob', None),\n",
    "            \"prob_m\": logprobs_data.get('m', {}).get('prob', None),\n",
    "            \"prob_M\": logprobs_data.get('M', {}).get('prob', None),\n",
    "            \"base_confidence\": base_prob,\n",
    "            \"adjusted_confidence\": adjusted_prob,\n",
    "            \"length_penalty\": length_penalty,\n",
    "            \"length_ratio\": length_ratio,\n",
    "            \"log_length_ratio\": log_length_ratio\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {\"verdict\": \"TIE\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "# Demonstrate with examples\n",
    "print(\"DETAILED LOGPROB-BASED EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Clear quality difference\",\n",
    "        \"instruction\": \"What is 2+2?\",\n",
    "        \"output_a\": \"4\",\n",
    "        \"output_b\": \"2+2 equals 4\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Length bias test\",\n",
    "        \"instruction\": \"What is the capital of France?\",\n",
    "        \"output_a\": \"Paris\",\n",
    "        \"output_b\": \"The capital of France is Paris, which is located in the northern part of the country along the Seine River. It has been the capital since 987 AD and is known for landmarks like the Eiffel Tower.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    print(f\"\\nTest: {test['name']}\")\n",
    "    print(\"-\"*40)\n",
    "    result = detailed_length_controlled_evaluation(\n",
    "        test[\"instruction\"],\n",
    "        test[\"output_a\"],\n",
    "        test[\"output_b\"]\n",
    "    )\n",
    "    \n",
    "    if \"error\" not in result:\n",
    "        print(f\"Verdict: {result['verdict']} (token: '{result['verdict_token']}')\")\n",
    "        print(f\"Length ratio (B/A): {result['length_ratio']:.2f}\")\n",
    "        print(f\"\\nProbabilities:\")\n",
    "        print(f\"  P(m|context) = {result['prob_m']:.3f} (prefers A)\")\n",
    "        print(f\"  P(M|context) = {result['prob_M']:.3f} (prefers B)\")\n",
    "        print(f\"\\nLength Control:\")\n",
    "        print(f\"  Base confidence: {result['base_confidence']:.3f}\")\n",
    "        print(f\"  Length penalty: {result['length_penalty']:.3f}\")\n",
    "        print(f\"  Adjusted confidence: {result['adjusted_confidence']:.3f}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "# Visualize the relationship between length ratio and penalty\n",
    "def visualize_length_penalty_formula():\n",
    "    \"\"\"Show how AlpacaEval's length penalty works.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Length penalty function\n",
    "    length_ratios = np.linspace(0.1, 5, 100)\n",
    "    log_ratios = np.log(length_ratios)\n",
    "    penalties = 1 / (1 + 0.5 * np.abs(log_ratios))\n",
    "    \n",
    "    ax1.plot(length_ratios, penalties, linewidth=2, label='Length Penalty')\n",
    "    ax1.axvline(x=1.0, color='red', linestyle='--', alpha=0.5, label='Equal Length')\n",
    "    ax1.fill_between(length_ratios, penalties, alpha=0.3)\n",
    "    ax1.set_xlabel('Length Ratio (Output B / Output A)')\n",
    "    ax1.set_ylabel('Penalty Factor')\n",
    "    ax1.set_title('AlpacaEval Length Penalty Function')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.set_xlim(0, 5)\n",
    "    \n",
    "    # Plot 2: Effect on confidence\n",
    "    base_confidences = [0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "    for base_conf in base_confidences:\n",
    "        adjusted = base_conf * penalties\n",
    "        ax2.plot(length_ratios, adjusted, linewidth=2, \n",
    "                label=f'Base: {base_conf}')\n",
    "    \n",
    "    ax2.axvline(x=1.0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Length Ratio (Output B / Output A)')\n",
    "    ax2.set_ylabel('Adjusted Confidence')\n",
    "    ax2.set_title('How Length Affects Final Confidence')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(0, 5)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_length_penalty_formula()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Practical Applications and Best Practices\n",
    "\n",
    "Let's create a complete evaluation pipeline that incorporates everything we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Configuration for LLM evaluation.\"\"\"\n",
    "    model: str = \"gpt-3.5-turbo\"\n",
    "    temperature: float = 0.0\n",
    "    use_length_control: bool = True\n",
    "    check_position_bias: bool = True\n",
    "    batch_size: int = 5\n",
    "    \n",
    "    \n",
    "class LLMJudgeEvaluator:\n",
    "    \"\"\"Complete LLM-as-Judge evaluation system.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.results = []\n",
    "        \n",
    "    def evaluate_single(self, instruction: str, output_a: str, output_b: str) -> Dict:\n",
    "        \"\"\"Evaluate a single example with all checks.\"\"\"\n",
    "        result = {\n",
    "            \"instruction\": instruction,\n",
    "            \"output_a_length\": len(output_a),\n",
    "            \"output_b_length\": len(output_b),\n",
    "        }\n",
    "        \n",
    "        # Basic evaluation\n",
    "        basic_verdict = self._get_verdict(instruction, output_a, output_b)\n",
    "        result[\"verdict\"] = basic_verdict\n",
    "        \n",
    "        # Length-controlled evaluation\n",
    "        if self.config.use_length_control:\n",
    "            lc_result = self._length_controlled_eval(instruction, output_a, output_b)\n",
    "            result.update(lc_result)\n",
    "        \n",
    "        # Position bias check\n",
    "        if self.config.check_position_bias:\n",
    "            position_check = self._check_position_bias(instruction, output_a, output_b)\n",
    "            result[\"position_consistent\"] = position_check\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_verdict(self, instruction: str, output_a: str, output_b: str) -> str:\n",
    "        \"\"\"Get basic verdict.\"\"\"\n",
    "        prompt = create_alpaca_style_prompt(instruction, output_a, output_b)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.config.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=self.config.temperature,\n",
    "                max_tokens=10\n",
    "            )\n",
    "            \n",
    "            verdict = response.choices[0].message.content.strip().upper()\n",
    "            return verdict if verdict in [\"A\", \"B\"] else \"TIE\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return \"TIE\"\n",
    "    \n",
    "    def _length_controlled_eval(self, instruction: str, output_a: str, output_b: str) -> Dict:\n",
    "        \"\"\"Perform length-controlled evaluation.\"\"\"\n",
    "        return length_controlled_evaluation(instruction, output_a, output_b, self.config.model)\n",
    "    \n",
    "    def _check_position_bias(self, instruction: str, output_a: str, output_b: str) -> bool:\n",
    "        \"\"\"Check for position bias.\"\"\"\n",
    "        verdict_ab = self._get_verdict(instruction, output_a, output_b)\n",
    "        verdict_ba = self._get_verdict(instruction, output_b, output_a)\n",
    "        \n",
    "        # Convert verdict_ba for comparison\n",
    "        if verdict_ba == \"A\":\n",
    "            verdict_ba = \"B\"\n",
    "        elif verdict_ba == \"B\":\n",
    "            verdict_ba = \"A\"\n",
    "            \n",
    "        return verdict_ab == verdict_ba\n",
    "    \n",
    "    def evaluate_dataset(self, examples: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate a full dataset.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, example in enumerate(tqdm(examples, desc=\"Evaluating\")):\n",
    "            result = self.evaluate_single(\n",
    "                example[\"instruction\"],\n",
    "                example[\"output_a\"],\n",
    "                example[\"output_b\"]\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        self.results = results\n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"Generate evaluation report with statistics.\"\"\"\n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        report = {\n",
    "            \"total_examples\": len(df),\n",
    "            \"verdict_distribution\": df[\"verdict\"].value_counts().to_dict(),\n",
    "            \"avg_length_ratio\": (df[\"output_b_length\"] / df[\"output_a_length\"]).mean(),\n",
    "        }\n",
    "        \n",
    "        if \"position_consistent\" in df.columns:\n",
    "            report[\"position_consistency_rate\"] = df[\"position_consistent\"].mean()\n",
    "        \n",
    "        if \"length_adjusted\" in df.columns:\n",
    "            report[\"length_adjustments_made\"] = df[\"length_adjusted\"].sum()\n",
    "        \n",
    "        # Analyze preference by length\n",
    "        df[\"longer_output\"] = df.apply(\n",
    "            lambda x: \"A\" if x[\"output_a_length\"] > x[\"output_b_length\"] else \"B\", \n",
    "            axis=1\n",
    "        )\n",
    "        df[\"preferred_longer\"] = df[\"verdict\"] == df[\"longer_output\"]\n",
    "        report[\"prefer_longer_rate\"] = df[\"preferred_longer\"].mean()\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "# Create and test the complete evaluator\n",
    "config = EvaluationConfig(\n",
    "    use_length_control=True,\n",
    "    check_position_bias=False,  # Disable for speed in demo\n",
    "    batch_size=3\n",
    ")\n",
    "\n",
    "evaluator = LLMJudgeEvaluator(config)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = [\n",
    "    {\n",
    "        \"instruction\": \"What is Python?\",\n",
    "        \"output_a\": \"Python is a high-level programming language.\",\n",
    "        \"output_b\": \"Python is a versatile, high-level programming language known for its simple syntax and readability. It supports multiple programming paradigms and has a vast ecosystem.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Calculate 15 + 27\",\n",
    "        \"output_a\": \"15 + 27 = 42\",\n",
    "        \"output_b\": \"42\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Name three primary colors\",\n",
    "        \"output_a\": \"Red, blue, and yellow are the three primary colors.\",\n",
    "        \"output_b\": \"The primary colors are:\\n1. Red\\n2. Blue\\n3. Yellow\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running Complete Evaluation Pipeline...\")\n",
    "print(\"=\"*50)\n",
    "results_df = evaluator.evaluate_dataset(test_dataset)\n",
    "\n",
    "# Generate report\n",
    "report = evaluator.generate_report()\n",
    "\n",
    "print(\"\\\\nEvaluation Report:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in report.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Show detailed results\n",
    "print(\"\\\\nDetailed Results:\")\n",
    "print(results_df[[\"instruction\", \"verdict\", \"output_a_length\", \"output_b_length\"]].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. When to Use LLM vs Human Evaluation\n",
    "\n",
    "Understanding when to use each evaluation method is crucial for effective model development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison framework\n",
    "evaluation_comparison = pd.DataFrame({\n",
    "    \"Aspect\": [\n",
    "        \"Speed\",\n",
    "        \"Cost\", \n",
    "        \"Consistency\",\n",
    "        \"Scalability\",\n",
    "        \"Nuanced Understanding\",\n",
    "        \"Cultural Sensitivity\",\n",
    "        \"Domain Expertise\",\n",
    "        \"Bias Detection\",\n",
    "        \"Reproducibility\",\n",
    "        \"Feedback Quality\"\n",
    "    ],\n",
    "    \"LLM Evaluation\": [\n",
    "        \"Very Fast (seconds)\",\n",
    "        \"Low ($0.001-0.01 per eval)\",\n",
    "        \"High (deterministic)\",\n",
    "        \"Excellent (thousands/hour)\",\n",
    "        \"Good for general tasks\",\n",
    "        \"Limited\",\n",
    "        \"General knowledge only\",\n",
    "        \"Can perpetuate biases\",\n",
    "        \"Perfect\",\n",
    "        \"Structured but generic\"\n",
    "    ],\n",
    "    \"Human Evaluation\": [\n",
    "        \"Slow (minutes-hours)\",\n",
    "        \"High ($0.10-1.00 per eval)\",\n",
    "        \"Variable (inter-rater differences)\",\n",
    "        \"Poor (dozens/day)\",\n",
    "        \"Excellent\",\n",
    "        \"High with diverse evaluators\",\n",
    "        \"Can use domain experts\",\n",
    "        \"Better bias awareness\",\n",
    "        \"Difficult\",\n",
    "        \"Rich and specific\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"LLM vs Human Evaluation Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(evaluation_comparison.to_string(index=False))\n",
    "\n",
    "# Decision framework\n",
    "print(\"\\\\n\\\\nDecision Framework:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Use LLM Evaluation When:\n",
    "✓ Rapid iteration during development\n",
    "✓ Large-scale evaluation needed (1000+ examples)\n",
    "✓ Comparing similar models/prompts\n",
    "✓ Budget constraints\n",
    "✓ Need reproducible results\n",
    "✓ General quality assessment\n",
    "\n",
    "Use Human Evaluation When:\n",
    "✓ Final model validation\n",
    "✓ Safety-critical applications\n",
    "✓ Creative or subjective tasks\n",
    "✓ Domain-specific evaluation\n",
    "✓ Detecting subtle biases\n",
    "✓ Understanding user preferences\n",
    "\n",
    "Hybrid Approach (Recommended):\n",
    "1. Use LLM evaluation for rapid development cycles\n",
    "2. Validate with human evaluation on a subset\n",
    "3. Calibrate LLM judge based on human feedback\n",
    "4. Use human evaluation for final validation\n",
    "\"\"\")\n",
    "\n",
    "# Practical example: Agreement analysis\n",
    "def analyze_human_llm_agreement(human_verdicts: List[str], llm_verdicts: List[str]) -> Dict:\n",
    "    \"\"\"Analyze agreement between human and LLM evaluators.\"\"\"\n",
    "    assert len(human_verdicts) == len(llm_verdicts)\n",
    "    \n",
    "    agreements = [h == l for h, l in zip(human_verdicts, llm_verdicts)]\n",
    "    agreement_rate = sum(agreements) / len(agreements)\n",
    "    \n",
    "    # Calculate Cohen's Kappa (simple version)\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Observed agreement\n",
    "    po = agreement_rate\n",
    "    \n",
    "    # Expected agreement by chance\n",
    "    human_counts = Counter(human_verdicts)\n",
    "    llm_counts = Counter(llm_verdicts)\n",
    "    n = len(human_verdicts)\n",
    "    \n",
    "    pe = sum((human_counts[k] * llm_counts[k]) / (n * n) for k in set(human_verdicts))\n",
    "    \n",
    "    # Cohen's Kappa\n",
    "    kappa = (po - pe) / (1 - pe) if pe < 1 else 0\n",
    "    \n",
    "    return {\n",
    "        \"agreement_rate\": agreement_rate,\n",
    "        \"cohens_kappa\": kappa,\n",
    "        \"total_examples\": len(human_verdicts),\n",
    "        \"disagreements\": sum(1 for a in agreements if not a)\n",
    "    }\n",
    "\n",
    "# Simulate human vs LLM agreement\n",
    "np.random.seed(42)\n",
    "n_examples = 20\n",
    "\n",
    "# Simulate verdicts (with some correlation)\n",
    "human_verdicts = np.random.choice([\"A\", \"B\"], size=n_examples, p=[0.6, 0.4])\n",
    "llm_verdicts = []\n",
    "\n",
    "for h in human_verdicts:\n",
    "    if np.random.random() < 0.8:  # 80% agreement\n",
    "        llm_verdicts.append(h)\n",
    "    else:\n",
    "        llm_verdicts.append(\"B\" if h == \"A\" else \"A\")\n",
    "\n",
    "# Analyze agreement\n",
    "agreement_stats = analyze_human_llm_agreement(\n",
    "    human_verdicts.tolist(), \n",
    "    llm_verdicts\n",
    ")\n",
    "\n",
    "print(\"\\\\n\\\\nHuman-LLM Agreement Analysis:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in agreement_stats.items():\n",
    "    print(f\"{key}: {value:.3f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
    "\n",
    "# Visualize agreement\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Confusion matrix style plot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(human_verdicts, llm_verdicts, labels=[\"A\", \"B\"])\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=[\"A\", \"B\"], yticklabels=[\"A\", \"B\"])\n",
    "ax1.set_xlabel('LLM Verdict')\n",
    "ax1.set_ylabel('Human Verdict')\n",
    "ax1.set_title('Human vs LLM Agreement Matrix')\n",
    "\n",
    "# Agreement over examples\n",
    "agreements = [h == l for h, l in zip(human_verdicts, llm_verdicts)]\n",
    "ax2.bar(range(len(agreements)), agreements, alpha=0.7)\n",
    "ax2.set_xlabel('Example Index')\n",
    "ax2.set_ylabel('Agreement (1=Yes, 0=No)')\n",
    "ax2.set_title('Agreement Pattern Across Examples')\n",
    "ax2.axhline(y=agreement_stats[\"agreement_rate\"], color='r', \n",
    "            linestyle='--', label=f'Avg: {agreement_stats[\"agreement_rate\"]:.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, we've explored the fundamentals of LLM-as-Judge evaluation:\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Basic Implementation**: How to use LLMs to evaluate other model outputs through pairwise comparison\n",
    "2. **Prompt Engineering**: The importance of well-designed evaluation prompts for consistent results\n",
    "3. **Batch Processing**: Efficient evaluation of multiple examples in single API calls\n",
    "4. **Common Biases**: \n",
    "   - Length bias (preference for longer outputs)\n",
    "   - Position bias (order affects judgment)\n",
    "   - Self-preference bias (models prefer their own style)\n",
    "5. **Mitigation Strategies**: Length-controlled evaluation using confidence scores\n",
    "6. **Practical Applications**: When to use LLM vs human evaluation\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **LLM evaluation correlates highly with human judgment** (0.98 for AlpacaEval with ChatBot Arena)\n",
    "- **Biases are systematic and predictable**, allowing for correction\n",
    "- **Hybrid approaches work best**: Use LLM evaluation for development, human evaluation for validation\n",
    "- **Length control is crucial** for fair evaluation of concise vs verbose models\n",
    "\n",
    "### Best Practices for LLM-as-Judge Evaluationi:\n",
    "\n",
    "1. Always test for position bias by evaluating both orders\n",
    "2. Validate LLM judgments with human evaluation on a subset\n",
    "3. Monitor for drift - LLM judges can change behavior over time\n",
    "4. Use multiple judges when possible to reduce single-model bias\n",
    "5. Use length-controlled metrics when comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "For further exploration of LLM-as-Judge evaluation:\n",
    "\n",
    "### Papers and Research:\n",
    "- **AlpacaEval GitHub**: [\"AlpacaEval: An Automatic Evaluator for Instruction-following Language Models\"](https://github.com/tatsu-lab/alpaca_eval)\n",
    "- **Length-Controlled AlpacaEval**: Introduces techniques to mitigate length bias\n",
    "- **ChatBot Arena**: Human evaluation platform that validates LLM judge correlations\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different judge models (GPT-4, Claude, etc.)\n",
    "- Try evaluating your own model outputs\n",
    "- Implement more sophisticated bias mitigation techniques\n",
    "- Explore multi-turn dialogue evaluation\n",
    "- Build domain-specific evaluators for your use cases\n",
    "\n",
    "Remember: LLM-as-Judge is a powerful tool, but it's not a complete replacement for human evaluation. Use it wisely as part of a comprehensive evaluation strategy!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
