{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Methods for Language Models\n",
    "\n",
    "In this notebook, we'll explore different sampling strategies for decoder-only transformer models. These strategies include:\n",
    "\n",
    "1. Greedy Sampling\n",
    "2. Temperature Sampling\n",
    "3. Top-K Sampling\n",
    "4. Top-p (Nucleus) Sampling\n",
    "5. Beam Search\n",
    "6. Repetition Penalty\n",
    "\n",
    "We'll use toy examples with logit calculations from decoder-only transformer models to illustrate how each sampling method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example Setup: Decoder-Only Transformer\n",
    "\n",
    "To illustrate sampling methods, we'll create a simplified setup that represents the output logits from a decoder-only transformer. We'll use a small vocabulary to make visualization easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a small vocabulary for our toy examples\n",
    "vocab = ['the', 'cat', 'dog', 'sat', 'on', 'mat', 'ran', 'fast', 'slow', 'jumped']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Function to convert tokens to IDs and vice versa\n",
    "def token_to_id(token: str) -> int:\n",
    "    return vocab.index(token) if token in vocab else -1\n",
    "\n",
    "def id_to_token(id: int) -> str:\n",
    "    return vocab[id] if 0 <= id < len(vocab) else \"<UNK>\"\n",
    "\n",
    "# Generate sample logits as if they were from the last layer of a transformer model\n",
    "# These represent unnormalized log probabilities for the next token\n",
    "def generate_sample_logits() -> torch.Tensor:\n",
    "    # Create logits with different distributions for demonstration\n",
    "    logits_1 = torch.tensor([2.0, 3.5, 3.0, 1.0, 2.5, 1.5, 4.0, 0.5, 0.8, 2.2])  # Peaked at 'ran'\n",
    "    logits_2 = torch.tensor([1.0, 2.0, 5.0, 1.8, 2.2, 1.5, 1.7, 1.9, 1.4, 1.6])  # Peaked at 'dog'\n",
    "    logits_3 = torch.tensor([3.0, 2.8, 2.9, 2.7, 3.1, 2.5, 2.6, 2.4, 2.3, 3.2])  # More uniform\n",
    "    \n",
    "    return {\"peaked_at_ran\": logits_1, \n",
    "            \"peaked_at_dog\": logits_2, \n",
    "            \"uniform\": logits_3}\n",
    "\n",
    "# Get sample logits\n",
    "sample_logits = generate_sample_logits()\n",
    "\n",
    "# Function to visualize the probability distribution\n",
    "def visualize_distribution(distribution, title=\"Probability Distribution\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(vocab, distribution.numpy())\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert logits to probabilities using softmax\n",
    "def logits_to_probs(logits):\n",
    "    return F.softmax(logits, dim=-1)\n",
    "\n",
    "# Display the base distributions\n",
    "for name, logits in sample_logits.items():\n",
    "    probs = logits_to_probs(logits)\n",
    "    visualize_distribution(probs, f\"Probability Distribution - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention and Logit Generation in Transformer Models\n",
    "\n",
    "Before diving into sampling methods, let's briefly review how logits are generated in a decoder-only transformer model using self-attention.\n",
    "\n",
    "In a transformer decoder, each token attends to all previous tokens through self-attention. The final layer produces logits which represent the model's prediction for the next token. Here's a simplified implementation of self-attention and the process that generates logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SimplifiedSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.q_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply causal mask (for decoder-only models)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        context = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.out_linear(context)\n",
    "        return output, attention_weights\n",
    "\n",
    "class SimplifiedTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.self_attention = SimplifiedSelfAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attention(x, mask)\n",
    "        x = self.norm1(x + attn_output)  # Add & Norm\n",
    "        \n",
    "        # Feed-forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_output)  # Add & Norm\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ToyDecoderOnlyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(100, d_model)  # Max 100 positions for simplicity\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [SimplifiedTransformerLayer(d_model, n_heads, d_ff) for _ in range(n_layers)]\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)  # Linear layer to produce logits\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(0, seq_len).expand(batch_size, seq_len).to(x.device)\n",
    "        \n",
    "        # Create embeddings\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        mask = ~mask  # Invert to get the causal mask (1 for positions to attend to)\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Project to vocabulary size to get logits\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# For demonstration, we'll just use our pre-computed logits\n",
    "# But this shows the process of how those logits would be generated in a real model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Greedy Sampling\n",
    "\n",
    "Greedy sampling is the simplest approach: always select the token with the highest probability at each step. This is deterministic but often leads to repetitive and less diverse outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def greedy_sampling(logits: torch.Tensor) -> int:\n",
    "    \"\"\"Select the token with the highest probability\"\"\"\n",
    "    return torch.argmax(logits).item()\n",
    "\n",
    "# Demonstrate greedy sampling with our sample logits\n",
    "print(\"Greedy Sampling Results:\")\n",
    "for name, logits in sample_logits.items():\n",
    "    token_id = greedy_sampling(logits)\n",
    "    token = id_to_token(token_id)\n",
    "    print(f\"  Distribution '{name}': Selected token '{token}' (ID: {token_id})\")\n",
    "    \n",
    "    # Visualize the selection\n",
    "    probs = logits_to_probs(logits)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(vocab, probs.numpy())\n",
    "    bars[token_id].set_color('red')  # Highlight the selected token\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f\"Greedy Sampling - Distribution '{name}'\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Sampling Analysis\n",
    "\n",
    "As we can see from the results, greedy sampling always selects the token with the highest probability. This makes the generation process deterministic - given the same input, it will always produce the same output.\n",
    "\n",
    "**Pros of Greedy Sampling:**\n",
    "- Simple to implement\n",
    "- Deterministic\n",
    "- Works well for tasks where there's a clear \"right answer\" (like arithmetic)\n",
    "\n",
    "**Cons of Greedy Sampling:**\n",
    "- Can get stuck in repetition loops\n",
    "- Produces less diverse and creative text\n",
    "- Cannot recover from mistakes\n",
    "\n",
    "Let's see a step-by-step example of how greedy sampling works in a sequence:\n",
    "\n",
    "1. We have an input sequence \"The cat\"\n",
    "2. The model generates logits for the next token\n",
    "3. We convert logits to probabilities with softmax\n",
    "4. Select the token with highest probability (e.g., \"sat\")\n",
    "5. Add \"sat\" to our sequence: \"The cat sat\"\n",
    "6. Generate logits for the next token after \"The cat sat\"\n",
    "7. Again select the highest probability token (e.g., \"on\")\n",
    "8. Repeat until we decide to stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temperature Sampling\n",
    "\n",
    "Temperature sampling introduces randomness by scaling the logits before applying softmax. A higher temperature leads to more uniform probabilities (more random), while a lower temperature makes the distribution more peaked (less random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def temperature_sampling(logits: torch.Tensor, temperature: float = 1.0) -> int:\n",
    "    \"\"\"Sample from the distribution after applying temperature scaling\"\"\"\n",
    "    if temperature == 0:  # Temperature of 0 is equivalent to greedy sampling\n",
    "        return greedy_sampling(logits)\n",
    "    \n",
    "    # Scale logits by temperature\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    # Sample from the distribution\n",
    "    return torch.multinomial(probs, 1).item()\n",
    "\n",
    "# Demonstrate temperature sampling with various temperatures\n",
    "temperatures = [0.5, 1.0, 2.0]\n",
    "logits_to_use = sample_logits[\"peaked_at_ran\"]  # Use one distribution for clarity\n",
    "\n",
    "# Set up multi-plot figure\n",
    "fig, axes = plt.subplots(len(temperatures), 1, figsize=(12, 4*len(temperatures)))\n",
    "fig.suptitle(\"Effect of Temperature on Token Probabilities\", fontsize=16)\n",
    "\n",
    "print(\"Temperature Sampling:\")\n",
    "for i, temp in enumerate(temperatures):\n",
    "    # Scale logits and convert to probabilities\n",
    "    scaled_logits = logits_to_use / temp\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    # Sample a token\n",
    "    token_id = temperature_sampling(logits_to_use, temp)\n",
    "    token = id_to_token(token_id)\n",
    "    print(f\"  Temperature {temp}: Selected token '{token}' (ID: {token_id})\")\n",
    "    \n",
    "    # Plot the probability distribution\n",
    "    ax = axes[i]\n",
    "    bars = ax.bar(vocab, probs.numpy())\n",
    "    bars[token_id].set_color('red')  # Highlight the selected token\n",
    "    ax.set_xlabel('Tokens')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f\"Temperature = {temp}\")\n",
    "    ax.set_xticks(range(len(vocab)))\n",
    "    ax.set_xticklabels(vocab, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)  # Adjust for the suptitle\n",
    "plt.show()\n",
    "\n",
    "# Let's demonstrate multiple samples at the same temperature to show randomness\n",
    "selected_temp = 1.0\n",
    "num_samples = 10\n",
    "\n",
    "print(f\"\\nMultiple samples at Temperature {selected_temp}:\")\n",
    "for i in range(num_samples):\n",
    "    token_id = temperature_sampling(logits_to_use, selected_temp)\n",
    "    token = id_to_token(token_id)\n",
    "    print(f\"  Sample {i+1}: '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Sampling Analysis\n",
    "\n",
    "Temperature sampling introduces controllable randomness to the generation process. The temperature parameter (T) directly impacts how the model distributes probability mass across the vocabulary:\n",
    "\n",
    "- **T < 1.0 (Low temperature)**: Makes the distribution more peaked, emphasizing high-probability tokens. As T approaches 0, it becomes equivalent to greedy sampling.\n",
    "- **T = 1.0**: No modification to the original distribution.\n",
    "- **T > 1.0 (High temperature)**: Makes the distribution more uniform, giving lower-probability tokens a better chance of being selected.\n",
    "\n",
    "The formula for temperature scaling is:\n",
    "\n",
    "$$p_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}$$\n",
    "\n",
    "Where:\n",
    "- $z_i$ is the logit for token $i$\n",
    "- $T$ is the temperature\n",
    "- $p_i$ is the resulting probability for token $i$\n",
    "\n",
    "**Pros of Temperature Sampling:**\n",
    "- Provides a simple way to control randomness/creativity\n",
    "- Can produce more diverse outputs than greedy sampling\n",
    "- Preserves the relative ordering of token probabilities\n",
    "\n",
    "**Cons of Temperature Sampling:**\n",
    "- At high temperatures, can generate low-quality or nonsensical text\n",
    "- At low temperatures, can still suffer from repetition issues\n",
    "- Difficult to find the \"right\" temperature for every generation scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top-K Sampling\n",
    "\n",
    "Top-K sampling restricts token selection to only the K most likely candidates. This prevents the model from selecting tokens with very low probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def top_k_sampling(logits: torch.Tensor, k: int = 5, temperature: float = 1.0) -> int:\n",
    "    \"\"\"Sample from the k most likely tokens\"\"\"\n",
    "    # Apply temperature scaling first\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Find the top-k tokens\n",
    "    top_k_logits, top_k_indices = torch.topk(scaled_logits, k)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
    "    \n",
    "    # Sample from the top-k distribution\n",
    "    sampled_index = torch.multinomial(top_k_probs, 1).item()\n",
    "    \n",
    "    # Map back to the original vocabulary index\n",
    "    return top_k_indices[sampled_index].item()\n",
    "\n",
    "# Demonstrate top-k sampling with various k values\n",
    "k_values = [3, 5, 8]\n",
    "logits_to_use = sample_logits[\"uniform\"]  # Use more uniform distribution to see effect\n",
    "\n",
    "# Initialize plot\n",
    "fig, axes = plt.subplots(len(k_values), 1, figsize=(12, 4*len(k_values)))\n",
    "fig.suptitle(\"Effect of k in Top-K Sampling\", fontsize=16)\n",
    "\n",
    "print(\"Top-K Sampling:\")\n",
    "for i, k in enumerate(k_values):\n",
    "    # Get original probabilities\n",
    "    original_probs = F.softmax(logits_to_use, dim=-1)\n",
    "    \n",
    "    # Find top-k tokens and their probabilities\n",
    "    top_k_values, top_k_indices = torch.topk(logits_to_use, k)\n",
    "    top_k_probs = F.softmax(top_k_values, dim=-1)\n",
    "    \n",
    "    # Sample token\n",
    "    token_id = top_k_sampling(logits_to_use, k)\n",
    "    token = id_to_token(token_id)\n",
    "    print(f\"  k={k}: Selected token '{token}' (ID: {token_id})\")\n",
    "    \n",
    "    # Create new distribution for visualization (all mass on top-k tokens)\n",
    "    modified_probs = torch.zeros_like(logits_to_use)\n",
    "    for j, idx in enumerate(top_k_indices):\n",
    "        modified_probs[idx] = top_k_probs[j]\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot original distribution (transparent)\n",
    "    ax.bar(vocab, original_probs.numpy(), alpha=0.3, label='Original probabilities')\n",
    "    \n",
    "    # Plot top-k distribution\n",
    "    bars = ax.bar(vocab, modified_probs.numpy(), label='Top-K probabilities')\n",
    "    \n",
    "    # Highlight selected token\n",
    "    if token_id in top_k_indices:\n",
    "        bars[token_id].set_color('red')\n",
    "    \n",
    "    ax.set_xlabel('Tokens')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f\"Top-{k} Sampling\")\n",
    "    ax.set_xticks(range(len(vocab)))\n",
    "    ax.set_xticklabels(vocab, rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)  # Adjust for the suptitle\n",
    "plt.show()\n",
    "\n",
    "# Let's demonstrate multiple samples with top-k to show variation\n",
    "selected_k = 5\n",
    "num_samples = 10\n",
    "\n",
    "print(f\"\\nMultiple samples with Top-{selected_k}:\")\n",
    "for i in range(num_samples):\n",
    "    token_id = top_k_sampling(logits_to_use, selected_k)\n",
    "    token = id_to_token(token_id)\n",
    "    print(f\"  Sample {i+1}: '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K Sampling Analysis\n",
    "\n",
    "Top-K sampling addresses a key limitation of temperature sampling: it prevents the model from sampling very unlikely tokens. The algorithm works as follows:\n",
    "\n",
    "1. Identify the K tokens with the highest probabilities\n",
    "2. Set the probabilities of all other tokens to zero\n",
    "3. Renormalize the probability distribution across the K tokens\n",
    "4. Sample from this truncated distribution\n",
    "\n",
    "This approach ensures that the model only considers the K most likely continuations, which can prevent nonsensical or out-of-distribution outputs.\n",
    "\n",
    "**Pros of Top-K Sampling:**\n",
    "- Prevents selection of very low probability tokens\n",
    "- Generally produces more coherent text than high-temperature sampling\n",
    "- Simple to implement and computationally efficient\n",
    "\n",
    "**Cons of Top-K Sampling:**\n",
    "- Fixed K is not optimal for all contexts\n",
    "- In highly-peaked distributions, K might be too restrictive\n",
    "- In flat distributions, K might allow too many low-quality options\n",
    "\n",
    "The optimal value of K often depends on the specific use case and model. Common values range from 10 to 50 in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-p (Nucleus) Sampling\n",
    "\n",
    "Top-p sampling (also called nucleus sampling) dynamically selects the smallest set of tokens whose cumulative probability exceeds a threshold p. This adapts to the confidence of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def top_p_sampling(logits: torch.Tensor, p: float = 0.9, temperature: float = 1.0) -> int:\n",
    "    \"\"\"Sample from the smallest set of tokens with cumulative probability >= p\"\"\"\n",
    "    # Apply temperature scaling\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    # Sort probabilities in descending order\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    \n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # Find tokens within the nucleus (where cumulative prob >= p)\n",
    "    nucleus_indices = cumulative_probs <= p\n",
    "    nucleus_indices[0] = True  # Always include the most likely token\n",
    "    \n",
    "    # Select the tokens within the nucleus\n",
    "    nucleus_probs = sorted_probs[nucleus_indices]\n",
    "    nucleus_indices = sorted_indices[nucleus_indices]\n",
    "    \n",
    "    # Sample from the nucleus\n",
    "    sampled_index = torch.multinomial(nucleus_probs, 1).item()\n",
    "    \n",
    "    # Map back to the original vocabulary index\n",
    "    return nucleus_indices[sampled_index].item()\n",
    "\n",
    "# Demonstrate top-p sampling with various p values\n",
    "p_values = [0.5, 0.7, 0.9]\n",
    "\n",
    "# Test on two different distributions\n",
    "distributions = [\"peaked_at_ran\", \"uniform\"]\n",
    "\n",
    "for dist_name in distributions:\n",
    "    logits_to_use = sample_logits[dist_name]\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, axes = plt.subplots(len(p_values), 1, figsize=(12, 4*len(p_values)))\n",
    "    fig.suptitle(f\"Effect of p in Top-p Sampling - {dist_name} distribution\", fontsize=16)\n",
    "    \n",
    "    print(f\"\\nTop-p Sampling on {dist_name} distribution:\")\n",
    "    for i, p in enumerate(p_values):\n",
    "        # Get original probabilities\n",
    "        probs = F.softmax(logits_to_use, dim=-1)\n",
    "        \n",
    "        # Sort probabilities for visualization\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        nucleus_mask = cumulative_probs <= p\n",
    "        nucleus_mask[0] = True  # Always include the most likely token\n",
    "        \n",
    "        # Count tokens in the nucleus\n",
    "        nucleus_size = nucleus_mask.sum().item()\n",
    "        \n",
    "        # Sample token\n",
    "        token_id = top_p_sampling(logits_to_use, p)\n",
    "        token = id_to_token(token_id)\n",
    "        print(f\"  p={p}: Selected token '{token}' (ID: {token_id}), Nucleus size: {nucleus_size}\")\n",
    "        \n",
    "        # Create modified distribution for visualization\n",
    "        modified_probs = torch.zeros_like(probs)\n",
    "        for j, idx in enumerate(sorted_indices):\n",
    "            if j < nucleus_size:\n",
    "                modified_probs[idx] = probs[idx]\n",
    "        \n",
    "        # Normalize the modified distribution\n",
    "        if modified_probs.sum() > 0:\n",
    "            modified_probs = modified_probs / modified_probs.sum()\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot original distribution (transparent)\n",
    "        ax.bar(vocab, probs.numpy(), alpha=0.3, label='Original probabilities')\n",
    "        \n",
    "        # Plot nucleus distribution\n",
    "        bars = ax.bar(vocab, modified_probs.numpy(), label=f'Top-p ({nucleus_size} tokens)')\n",
    "        \n",
    "        # Highlight selected token\n",
    "        bars[token_id].set_color('red')\n",
    "        \n",
    "        ax.set_xlabel('Tokens')\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.set_title(f\"Top-p (p={p}) Sampling\")\n",
    "        ax.set_xticks(range(len(vocab)))\n",
    "        ax.set_xticklabels(vocab, rotation=45)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Adjust for the suptitle\n",
    "    plt.show()\n",
    "\n",
    "# Let's demonstrate multiple samples with top-p to show variation\n",
    "selected_p = 0.9\n",
    "num_samples = 10\n",
    "logits_to_use = sample_logits[\"uniform\"]\n",
    "\n",
    "print(f\"\\nMultiple samples with Top-p (p={selected_p}):\")\n",
    "for i in range(num_samples):\n",
    "    token_id = top_p_sampling(logits_to_use, selected_p)\n",
    "    token = id_to_token(token_id)\n",
    "    print(f\"  Sample {i+1}: '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-p (Nucleus) Sampling Analysis\n",
    "\n",
    "Top-p sampling, introduced in the paper \"The Curious Case of Neural Text Degeneration\" (Holtzman et al., 2019), adapts to the confidence of the model. It selects the smallest set of tokens whose cumulative probability exceeds a threshold p (the \"nucleus\").\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. Sort tokens by probability in descending order\n",
    "2. Calculate cumulative probabilities\n",
    "3. Select the smallest set of tokens such that their cumulative probability e p\n",
    "4. Renormalize the probability distribution across these tokens\n",
    "5. Sample from this truncated distribution\n",
    "\n",
    "Notice how the size of the nucleus changes based on the distribution:\n",
    "- For peaked distributions, top-p selects fewer tokens\n",
    "- For flat distributions, top-p selects more tokens\n",
    "\n",
    "This adaptive behavior is the key advantage of top-p over top-k sampling.\n",
    "\n",
    "**Pros of Top-p Sampling:**\n",
    "- Adapts to the confidence of the model in different contexts\n",
    "- Works well across various types of distributions\n",
    "- Often produces more natural-sounding text than fixed top-k\n",
    "\n",
    "**Cons of Top-p Sampling:**\n",
    "- Slightly more complex to implement than top-k\n",
    "- May still include low-probability tokens in very flat distributions\n",
    "- Finding the optimal p value can require tuning\n",
    "\n",
    "Common values for p range from 0.9 to 0.95 in practice. Many state-of-the-art systems use top-p sampling, sometimes in combination with temperature sampling or top-k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Beam Search\n",
    "\n",
    "Beam search maintains multiple candidate sequences (the beam) and selects the one with the highest cumulative probability. This is useful for finding high-probability sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class BeamSearchState:\n",
    "    def __init__(self, sequence, log_prob):\n",
    "        self.sequence = sequence  # List of token IDs\n",
    "        self.log_prob = log_prob  # Log probability of the sequence\n",
    "\n",
    "def beam_search_step(model, current_beams, beam_width, max_seq_len):\n",
    "    \"\"\"Perform one step of beam search.\"\"\"\n",
    "    all_candidates = []\n",
    "    \n",
    "    # Expand each beam\n",
    "    for beam in current_beams:\n",
    "        # If beam has reached max length or end token, keep as is\n",
    "        if len(beam.sequence) >= max_seq_len:\n",
    "            all_candidates.append(beam)\n",
    "            continue\n",
    "        \n",
    "        # For simulation, we'll just use pre-computed logits\n",
    "        # In reality, we would run the model to get next-token logits\n",
    "        # logits = model(torch.tensor([beam.sequence]))[0, -1, :]\n",
    "        \n",
    "        # For demo, we'll use a random distribution\n",
    "        logits_name = list(sample_logits.keys())[len(beam.sequence) % len(sample_logits)]\n",
    "        logits = sample_logits[logits_name]\n",
    "        \n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get top-k next tokens\n",
    "        top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "        \n",
    "        # Create a new candidate beam for each top token\n",
    "        for i in range(beam_width):\n",
    "            next_token = top_indices[i].item()\n",
    "            next_log_prob = top_log_probs[i].item()\n",
    "            \n",
    "            new_sequence = beam.sequence + [next_token]\n",
    "            new_log_prob = beam.log_prob + next_log_prob\n",
    "            \n",
    "            all_candidates.append(BeamSearchState(new_sequence, new_log_prob))\n",
    "    \n",
    "    # Sort candidates by log probability and keep top beam_width\n",
    "    all_candidates.sort(key=lambda x: x.log_prob, reverse=True)\n",
    "    return all_candidates[:beam_width]\n",
    "\n",
    "def run_beam_search(model, initial_sequence, beam_width, max_seq_len):\n",
    "    \"\"\"Run beam search to generate a sequence\"\"\"\n",
    "    # Initialize beam with the initial sequence\n",
    "    current_beams = [BeamSearchState(initial_sequence, 0.0)]\n",
    "    \n",
    "    # Run beam search for max_seq_len steps\n",
    "    for _ in range(max_seq_len):\n",
    "        current_beams = beam_search_step(model, current_beams, beam_width, max_seq_len)\n",
    "    \n",
    "    # Return the highest probability sequence\n",
    "    return current_beams[0].sequence\n",
    "\n",
    "# Demonstrate beam search\n",
    "beam_widths = [1, 2, 3]\n",
    "initial_sequence = [0]  # Start with token \"the\" (ID: 0)\n",
    "max_seq_len = 5\n",
    "\n",
    "print(\"Beam Search Results:\")\n",
    "for beam_width in beam_widths:\n",
    "    # Use None as model since we're using pre-computed logits\n",
    "    output_sequence = run_beam_search(None, initial_sequence, beam_width, max_seq_len)\n",
    "    output_tokens = [id_to_token(idx) for idx in output_sequence]\n",
    "    \n",
    "    print(f\"  Beam width {beam_width}: {' '.join(output_tokens)}\")\n",
    "    \n",
    "    # Let's visualize the beam search process for beam_width=2\n",
    "    if beam_width == 2:\n",
    "        # Re-run with visualization\n",
    "        current_beams = [BeamSearchState(initial_sequence, 0.0)]\n",
    "        print(\"\\nStep-by-step Beam Search Process (beam_width=2):\")\n",
    "        \n",
    "        for step in range(max_seq_len):\n",
    "            print(f\"\\nStep {step+1}:\")\n",
    "            print(\"  Current beams:\")\n",
    "            for i, beam in enumerate(current_beams):\n",
    "                beam_tokens = [id_to_token(idx) for idx in beam.sequence]\n",
    "                print(f\"    Beam {i+1}: {' '.join(beam_tokens)} (log prob: {beam.log_prob:.2f})\")\n",
    "            \n",
    "            current_beams = beam_search_step(None, current_beams, beam_width, max_seq_len)\n",
    "            \n",
    "            print(\"  New beams after expansion:\")\n",
    "            for i, beam in enumerate(current_beams):\n",
    "                beam_tokens = [id_to_token(idx) for idx in beam.sequence]\n",
    "                print(f\"    Beam {i+1}: {' '.join(beam_tokens)} (log prob: {beam.log_prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Analysis\n",
    "\n",
    "Beam search is a breadth-first search algorithm that explores multiple possible continuation paths simultaneously. Unlike the sampling methods we've seen so far, beam search is designed to find the most likely sequence overall, not just at each step.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. Start with an initial sequence and maintain a list of the top-B sequences (where B is the beam width)\n",
    "2. For each sequence in the beam, compute the next-token probabilities\n",
    "3. For each sequence, consider all possible next tokens\n",
    "4. From these B×(vocab size) possible continuations, select the top-B sequences with the highest cumulative log-probability\n",
    "5. Repeat steps 2-4 until a stopping criterion is met\n",
    "6. Return the highest-scoring complete sequence\n",
    "\n",
    "Note that beam search uses log-probabilities to prevent numerical underflow when multiplying many probabilities together. The cumulative score for a sequence is the sum of the log-probabilities of each token.\n",
    "\n",
    "**Pros of Beam Search:**\n",
    "- Often finds sequences with higher overall probability\n",
    "- Helpful for tasks requiring precision, like translation or summarization\n",
    "- Can compare multiple high-scoring candidate sequences\n",
    "\n",
    "**Cons of Beam Search:**\n",
    "- Computationally expensive (scales with beam width)\n",
    "- Tends to produce generic, \"safe\" text\n",
    "- Can get stuck in repetition loops\n",
    "- Not suitable for creative text generation\n",
    "\n",
    "Special note: Beam search with beam width B=1 is equivalent to greedy search. As the beam width increases, beam search explores more alternative continuations, potentially finding higher-probability sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Repetition Penalty\n",
    "\n",
    "Repetition penalty discourages the model from generating the same tokens multiple times by penalizing tokens that have already appeared in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_repetition_penalty(logits: torch.Tensor, generated_tokens: List[int], penalty: float = 1.2) -> torch.Tensor:\n",
    "    \"\"\"Apply repetition penalty to logits based on tokens that have already been generated\"\"\"\n",
    "    # Make a copy of the logits to avoid modifying the original\n",
    "    penalized_logits = logits.clone()\n",
    "    \n",
    "    # Apply penalty to tokens that have already been generated\n",
    "    for token_id in set(generated_tokens):  # Use set to handle each token once\n",
    "        if penalized_logits[token_id] > 0:  # If logit is positive\n",
    "            penalized_logits[token_id] /= penalty\n",
    "        else:  # If logit is negative\n",
    "            penalized_logits[token_id] *= penalty\n",
    "    \n",
    "    return penalized_logits\n",
    "\n",
    "# Let's demonstrate repetition penalty on a token sequence\n",
    "def sample_with_repetition_penalty(logits: torch.Tensor, generated_tokens: List[int], \n",
    "                                  penalty: float = 1.2, temperature: float = 1.0) -> int:\n",
    "    \"\"\"Sample from the distribution after applying repetition penalty\"\"\"\n",
    "    # Apply repetition penalty first\n",
    "    penalized_logits = apply_repetition_penalty(logits, generated_tokens, penalty)\n",
    "    \n",
    "    # Then apply temperature\n",
    "    scaled_logits = penalized_logits / temperature\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    # Sample from the distribution\n",
    "    return torch.multinomial(probs, 1).item()\n",
    "\n",
    "# Simulate a sequence with repetitive tokens to show the effect of repetition penalty\n",
    "example_sequence = [0, 1, 1, 2, 1]  # \"the cat cat dog cat\"\n",
    "print(f\"Example sequence: {' '.join([id_to_token(idx) for idx in example_sequence])}\")\n",
    "\n",
    "penalty_values = [1.0, 1.5, 3.0]  # 1.0 = no penalty\n",
    "logits_to_use = sample_logits[\"peaked_at_dog\"]  # Distribution peaked at 'dog' (token_id: 2)\n",
    "\n",
    "# Initialize plot\n",
    "fig, axes = plt.subplots(len(penalty_values), 1, figsize=(12, 4*len(penalty_values)))\n",
    "fig.suptitle(\"Effect of Repetition Penalty\", fontsize=16)\n",
    "\n",
    "print(\"\\nRepetition Penalty Sampling:\")\n",
    "for i, penalty in enumerate(penalty_values):\n",
    "    # Apply repetition penalty\n",
    "    penalized_logits = apply_repetition_penalty(logits_to_use, example_sequence, penalty)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    original_probs = F.softmax(logits_to_use, dim=-1)\n",
    "    penalized_probs = F.softmax(penalized_logits, dim=-1)\n",
    "    \n",
    "    # Sample token\n",
    "    token_id = sample_with_repetition_penalty(logits_to_use, example_sequence, penalty)\n",
    "    token = id_to_token(token_id)\n",
    "    print(f\"  Penalty {penalty}: Selected token '{token}' (ID: {token_id})\")\n",
    "    \n",
    "    # Find tokens that were penalized (those in the sequence)\n",
    "    penalized_tokens = list(set(example_sequence))\n",
    "    print(f\"    Penalized tokens: {[id_to_token(idx) for idx in penalized_tokens]}\")\n",
    "    \n",
    "    # Calculate change in probability for each token\n",
    "    prob_change = penalized_probs - original_probs\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot original probabilities\n",
    "    ax.bar(vocab, original_probs.numpy(), alpha=0.3, label='Original probabilities')\n",
    "    \n",
    "    # Plot penalized probabilities\n",
    "    bars = ax.bar(vocab, penalized_probs.numpy(), label='After repetition penalty')\n",
    "    \n",
    "    # Highlight penalized tokens\n",
    "    for idx in penalized_tokens:\n",
    "        bars[idx].set_color('orange')\n",
    "    \n",
    "    # Highlight selected token\n",
    "    bars[token_id].set_color('red')\n",
    "    \n",
    "    ax.set_xlabel('Tokens')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f\"Repetition Penalty = {penalty}\")\n",
    "    ax.set_xticks(range(len(vocab)))\n",
    "    ax.set_xticklabels(vocab, rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)  # Adjust for the suptitle\n",
    "plt.show()\n",
    "\n",
    "# Let's simulate a sequence generation with repetition penalty to show its effect\n",
    "def generate_sequence(logits_dict, max_length=10, penalty=1.5, temperature=1.0):\n",
    "    sequence = [0]  # Start with \"the\"\n",
    "    \n",
    "    for i in range(max_length - 1):\n",
    "        # In reality, we would run the model to get logits\n",
    "        # For simulation, alternate between our sample distributions\n",
    "        logits_name = list(logits_dict.keys())[i % len(logits_dict)]\n",
    "        logits = logits_dict[logits_name]\n",
    "        \n",
    "        # Apply repetition penalty and sample\n",
    "        token_id = sample_with_repetition_penalty(logits, sequence, penalty, temperature)\n",
    "        sequence.append(token_id)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "penalty_values = [1.0, 1.5, 2.5]  # 1.0 = no penalty\n",
    "print(\"\\nGenerated sequences with different repetition penalties:\")\n",
    "for penalty in penalty_values:\n",
    "    sequence = generate_sequence(sample_logits, penalty=penalty)\n",
    "    tokens = [id_to_token(idx) for idx in sequence]\n",
    "    print(f\"  Penalty {penalty}: {' '.join(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetition Penalty Analysis\n",
    "\n",
    "Repetition penalty is a simple but effective technique to prevent the model from generating repetitive text. It works by reducing the probability of tokens that have already appeared in the generated sequence.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. Identify the set of tokens that have already been generated\n",
    "2. For each such token:\n",
    "   - If its logit is positive, divide it by the penalty factor\n",
    "   - If its logit is negative, multiply it by the penalty factor\n",
    "3. Apply other sampling methods (e.g., temperature, top-k, top-p) on these adjusted logits\n",
    "\n",
    "The effect of the penalty is that it directly reduces the likelihood of generating the same token multiple times, which leads to more diverse text.\n",
    "\n",
    "The formula for repetition penalty is:\n",
    "\n",
    "$$\\text{If } z_i > 0:\\quad z_i' = \\frac{z_i}{\\text{penalty}}$$\n",
    "$$\\text{If } z_i \\leq 0:\\quad z_i' = z_i \\times \\text{penalty}$$\n",
    "\n",
    "Where:\n",
    "- $z_i$ is the logit for token $i$\n",
    "- $z_i'$ is the adjusted logit\n",
    "- $\\text{penalty}$ is the repetition penalty factor (typically > 1.0)\n",
    "\n",
    "**Pros of Repetition Penalty:**\n",
    "- Simple and effective way to reduce repetition\n",
    "- Can be combined with any other sampling method\n",
    "- Computationally inexpensive\n",
    "\n",
    "**Cons of Repetition Penalty:**\n",
    "- Too high a penalty can prevent intentional repetition (like \"very, very\")\n",
    "- May not distinguish between good and bad repetition\n",
    "- Doesn't capture longer-range dependencies or patterns\n",
    "\n",
    "Common values for repetition penalty range from 1.0 (no penalty) to 2.0, with 1.2-1.5 being typical choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Sampling Methods\n",
    "\n",
    "In practice, these sampling methods are often combined to get the best results. A common approach is to use temperature scaling with top-p or top-k sampling, and apply a repetition penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def combined_sampling(logits: torch.Tensor, generated_tokens: List[int] = None,\n",
    "                      temperature: float = 1.0, top_k: int = None, top_p: float = None,\n",
    "                      repetition_penalty: float = 1.0) -> int:\n",
    "    \"\"\"Combined sampling with multiple methods\"\"\"\n",
    "    # Start with original logits\n",
    "    working_logits = logits.clone()\n",
    "    \n",
    "    # Apply repetition penalty if applicable\n",
    "    if generated_tokens is not None and repetition_penalty > 1.0:\n",
    "        working_logits = apply_repetition_penalty(working_logits, generated_tokens, repetition_penalty)\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        working_logits = working_logits / temperature\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(working_logits, dim=-1)\n",
    "    \n",
    "    # Apply top-k if specified\n",
    "    if top_k is not None:\n",
    "        # Set probabilities outside top-k to zero\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, min(top_k, len(probs)))\n",
    "        new_probs = torch.zeros_like(probs)\n",
    "        new_probs[top_k_indices] = top_k_probs\n",
    "        probs = new_probs\n",
    "        \n",
    "        # Renormalize if needed\n",
    "        if probs.sum() > 0:\n",
    "            probs = probs / probs.sum()\n",
    "    \n",
    "    # Apply top-p if specified\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        # Sort probabilities\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        \n",
    "        # Calculate cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        \n",
    "        # Find cutoff point for nucleus\n",
    "        nucleus_indices = cumulative_probs <= top_p\n",
    "        nucleus_indices[0] = True  # Always include the most likely token\n",
    "        \n",
    "        # Set probabilities outside nucleus to zero\n",
    "        sorted_probs[~nucleus_indices] = 0\n",
    "        \n",
    "        # Rearrange back to original order\n",
    "        new_probs = torch.zeros_like(probs)\n",
    "        for i, idx in enumerate(sorted_indices):\n",
    "            new_probs[idx] = sorted_probs[i]\n",
    "        probs = new_probs\n",
    "        \n",
    "        # Renormalize if needed\n",
    "        if probs.sum() > 0:\n",
    "            probs = probs / probs.sum()\n",
    "    \n",
    "    # If all probabilities are zero (can happen in extreme cases), back off to uniform\n",
    "    if probs.sum() == 0:\n",
    "        probs = torch.ones_like(probs) / len(probs)\n",
    "    \n",
    "    # Sample from the final distribution\n",
    "    return torch.multinomial(probs, 1).item()\n",
    "\n",
    "# Demonstrate a typical combined approach: temperature + top-p + repetition penalty\n",
    "logits_to_use = sample_logits[\"peaked_at_dog\"]\n",
    "example_sequence = [0, 1, 2, 2]  # \"the cat dog dog\"\n",
    "\n",
    "# Parameters for combined sampling\n",
    "temp = 0.8\n",
    "p = 0.9\n",
    "rep_penalty = 1.5\n",
    "\n",
    "print(\"Combined Sampling Example:\")\n",
    "print(f\"Original sequence: {' '.join([id_to_token(idx) for idx in example_sequence])}\")\n",
    "print(f\"Parameters: temperature={temp}, top_p={p}, repetition_penalty={rep_penalty}\")\n",
    "\n",
    "# Generate 5 next tokens with combined sampling\n",
    "sequence = example_sequence.copy()\n",
    "generated = []\n",
    "\n",
    "for i in range(5):\n",
    "    next_token_id = combined_sampling(logits_to_use, sequence, \n",
    "                                     temperature=temp, top_p=p, repetition_penalty=rep_penalty)\n",
    "    sequence.append(next_token_id)\n",
    "    generated.append(id_to_token(next_token_id))\n",
    "\n",
    "print(f\"Generated next tokens: {' '.join(generated)}\")\n",
    "print(f\"Full sequence: {' '.join([id_to_token(idx) for idx in sequence])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Best Practices\n",
    "\n",
    "In this notebook, we've explored various sampling methods for language models:\n",
    "\n",
    "1. **Greedy Sampling**: Always selects the most probable token. Deterministic but can lead to repetition and lack of diversity.\n",
    "\n",
    "2. **Temperature Sampling**: Scales the logits before softmax. Higher temperature (>1.0) leads to more randomness, lower temperature (<1.0) makes the distribution more peaked.\n",
    "\n",
    "3. **Top-K Sampling**: Restricts selection to only the K most likely tokens. Prevents the model from choosing very unlikely tokens.\n",
    "\n",
    "4. **Top-p (Nucleus) Sampling**: Dynamically selects the smallest set of tokens whose cumulative probability exceeds threshold p. Adapts to the confidence of the model.\n",
    "\n",
    "5. **Beam Search**: Maintains multiple candidate sequences and selects the one with highest cumulative probability. Good for finding the most likely sequence overall.\n",
    "\n",
    "6. **Repetition Penalty**: Discourages the model from generating the same tokens multiple times by penalizing tokens that have already appeared.\n",
    "\n",
    "### Best Practices and Recommendations\n",
    "\n",
    "Different tasks benefit from different sampling strategies:\n",
    "\n",
    "- **Creative writing, casual chat**: Temperature 0.7-0.9 with top-p=0.9 and moderate repetition penalty (1.1-1.3)\n",
    "- **Factual responses, coding**: Lower temperature (0.3-0.7) with top-p=0.95\n",
    "- **Translation, summarization**: Beam search with beam width 4-8\n",
    "- **Question answering**: Greedy or beam search with small beam width\n",
    "\n",
    "Many modern systems use a combination of these methods. For example, the output of GPT models typically uses temperature scaling + top-p sampling + repetition penalty.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The choice of sampling method significantly impacts the quality and diversity of generated text. There's no one-size-fits-all approach, and the optimal strategy depends on the specific task and desired characteristics of the output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}