{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM: High-Performance Inference Engine for LLMs\n",
    "\n",
    "In this notebook, we'll explore vLLM, an advanced inference engine designed to maximize the performance of Large Language Models (LLMs) on GPU hardware. vLLM addresses key performance bottlenecks in traditional inference systems, achieving substantially higher throughput while maintaining low latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Inference Engines\n",
    "\n",
    "### Why LLM Inference is Challenging\n",
    "\n",
    "Deploying LLMs efficiently presents several challenges:\n",
    "\n",
    "1. **Memory Constraints**: LLMs have billions of parameters that must be loaded into GPU memory\n",
    "2. **Attention Computation**: Quadratic scaling with sequence length makes attention expensive\n",
    "3. **Sequential Generation**: Auto-regressive generation is inherently sequential\n",
    "4. **Dynamic Batch Sizes**: Variable-length inputs and outputs make static batching inefficient\n",
    "5. **GPU Utilization**: Traditional inference pipelines often leave GPUs underutilized\n",
    "\n",
    "Specialized inference engines like vLLM are designed to address these challenges through innovative techniques and optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation Requirements\n",
    "\n",
    "Before we begin, here are the requirements for installing vLLM:\n",
    "\n",
    "```bash\n",
    "# Standard installation (with compatible CUDA version)\n",
    "pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n",
    "\n",
    "# Or via Docker (recommended for CUDA compatibility issues)\n",
    "docker run --gpus all -it --rm --ipc=host nvcr.io/nvidia/pytorch:23.10-py3\n",
    "```\n",
    "\n",
    "vLLM requires:\n",
    "- NVIDIA GPU with compute capability 7.0+ (V100, T4, A100, H100, etc.)\n",
    "- CUDA 11.8+ (best with CUDA 12.1)\n",
    "- NVIDIA drivers (tested with Driver 535+)\n",
    "- PyTorch 2.5.1 (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Value Propositions of vLLM\n",
    "\n",
    "vLLM offers several key advantages for LLM inference:\n",
    "\n",
    "1. **PagedAttention**: Efficient KV cache management that eliminates waste and reduces memory fragmentation\n",
    "2. **Continuous Batching**: Dynamic handling of requests to maximize GPU utilization \n",
    "3. **Optimized CUDA Kernels**: Highly optimized implementations of computational bottlenecks\n",
    "4. **Tensor Parallelism**: Ability to split model across multiple GPUs\n",
    "5. **Quantization Support**: Precision reduction (INT8, INT4) with minimal accuracy loss\n",
    "\n",
    "These innovations allow vLLM to achieve 2-18x higher throughput than other inference engines while maintaining low latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PagedAttention: Memory-Efficient KV Caching\n",
    "\n",
    "### The KV Cache Problem\n",
    "\n",
    "During LLM inference, the key-value (KV) pairs from previous tokens must be stored to avoid recomputation. Traditional approaches allocate a fixed-size cache for each request based on the maximum possible sequence length, leading to significant memory waste when:\n",
    "\n",
    "1. Actual sequences are shorter than the maximum length\n",
    "2. Each sequence in a batch has different lengths\n",
    "\n",
    "This problem becomes more severe with multiple simultaneous requests and longer context windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How PagedAttention Works\n",
    "\n",
    "PagedAttention applies virtual memory concepts to KV cache management:\n",
    "\n",
    "1. **Blocks Instead of Sequences**: PagedAttention divides the KV cache into fixed-size memory blocks (e.g., 16 tokens per block)\n",
    "2. **Physical and Logical Separation**: Maintains logical sequences using a block table that maps sequence positions to physical memory blocks\n",
    "3. **On-Demand Allocation**: Only allocates blocks when needed, rather than preallocating for maximum length\n",
    "4. **Memory Reuse**: Freed blocks from completed sequences can be immediately reused for new requests\n",
    "\n",
    "Let's see a basic example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization helper for PagedAttention concept\n",
    "def visualize_paged_attention(block_size=4, num_blocks=10, sequences=None):\n",
    "    if sequences is None:\n",
    "        sequences = [\n",
    "            {'name': 'Seq 1', 'length': 7, 'start_block': 0},\n",
    "            {'name': 'Seq 2', 'length': 5, 'start_block': 2},\n",
    "            {'name': 'Seq 3', 'length': 11, 'start_block': 4},\n",
    "        ]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Physical memory blocks\n",
    "    blocks = np.zeros(num_blocks * block_size)\n",
    "    colors = ['#ff9999', '#99ff99', '#9999ff']\n",
    "    \n",
    "    # Fill blocks based on sequences\n",
    "    for i, seq in enumerate(sequences):\n",
    "        start_idx = seq['start_block'] * block_size\n",
    "        for j in range(min(seq['length'], (num_blocks - seq['start_block']) * block_size)):\n",
    "            blocks[start_idx + j] = i + 1\n",
    "    \n",
    "    # Display physical memory\n",
    "    ax1.set_title('Physical Memory: KV Cache Blocks')\n",
    "    for i in range(num_blocks):\n",
    "        ax1.axvline(x=i*block_size - 0.5, color='black', linestyle='-', alpha=0.3)\n",
    "        ax1.text(i*block_size + block_size/2 - 0.5, -0.8, f'Block {i}', ha='center')\n",
    "    \n",
    "    cmap = plt.cm.colors.ListedColormap(['white'] + colors)\n",
    "    ax1.imshow(blocks.reshape(1, -1), aspect='auto', cmap=cmap, vmin=0, vmax=len(sequences))\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks(np.arange(0, num_blocks*block_size, block_size))\n",
    "    ax1.set_xticklabels([])\n",
    "    \n",
    "    # Display logical sequences\n",
    "    ax2.set_title('Logical Sequences: How PagedAttention Maps Requests')\n",
    "    for i, seq in enumerate(sequences):\n",
    "        logical_seq = np.zeros(num_blocks * block_size)\n",
    "        for j in range(seq['length']):\n",
    "            block_idx = seq['start_block'] + j // block_size\n",
    "            if block_idx < num_blocks:\n",
    "                token_idx = (block_idx * block_size) + (j % block_size)\n",
    "                logical_seq[j] = token_idx + 1  # Where this token is stored in physical memory\n",
    "                \n",
    "        ax2.scatter(range(seq['length']), [i]*seq['length'], \n",
    "                   c=[colors[i]]*seq['length'], label=seq['name'])\n",
    "        for j in range(seq['length']):\n",
    "            if logical_seq[j] > 0:\n",
    "                ax2.text(j, i, f'{int(logical_seq[j]-1)}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    ax2.set_xlim(-0.5, num_blocks*block_size - 0.5)\n",
    "    ax2.set_ylim(-0.5, len(sequences) - 0.5)\n",
    "    ax2.set_yticks(range(len(sequences)))\n",
    "    ax2.set_yticklabels([s['name'] for s in sequences])\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_xlabel('Logical Token Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "visualize_paged_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of PagedAttention\n",
    "\n",
    "PagedAttention provides several key advantages:\n",
    "\n",
    "1. **Memory Efficiency**: Uses up to 65% less memory than traditional KV caching\n",
    "2. **Elimination of Memory Fragmentation**: Blocks can be allocated and released dynamically\n",
    "3. **Support for Variable-Length Sequences**: Efficiently handles sequences of any length\n",
    "4. **Higher Throughput**: More requests can be processed simultaneously with the same memory\n",
    "\n",
    "The CUDA implementation of PagedAttention in vLLM includes highly optimized attention kernels that maintain computational efficiency while providing these memory benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous Batching for Maximum GPU Utilization\n",
    "\n",
    "### Static vs. Continuous Batching\n",
    "\n",
    "Traditional inference systems use static batching, where:\n",
    "1. A fixed number of requests are grouped into a batch\n",
    "2. The entire batch is processed together\n",
    "3. No new requests can join until the batch completes\n",
    "4. All sequences in the batch must wait for the longest sequence to finish\n",
    "\n",
    "This approach leads to poor GPU utilization because:\n",
    "- GPUs are idle while waiting for new batches\n",
    "- Shorter sequences waste compute waiting for longer ones\n",
    "- Batch size must be small to maintain reasonable latency\n",
    "\n",
    "Continuous batching solves these problems by dynamically adding and removing sequences from the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Continuous Batching Works\n",
    "\n",
    "vLLM implements continuous batching through a scheduling algorithm that:\n",
    "\n",
    "1. **Dynamically Adds Requests**: New requests join the batch as soon as they arrive\n",
    "2. **Independently Manages Sequences**: Each sequence progresses at its own pace\n",
    "3. **Immediately Removes Completed Sequences**: Frees resources as soon as generation completes\n",
    "4. **Prioritizes Requests**: Can implement strategies like FIFO, round-robin, or priority queuing\n",
    "\n",
    "Let's look at a comparison between static and continuous batching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Visualizing static vs continuous batching\n",
    "def compare_batching_methods():\n",
    "    # Setup\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Define request data: [arrival_time, tokens_to_generate]\n",
    "    requests = [\n",
    "        [0, 10],   # Request 1: arrives at t=0, needs 10 tokens\n",
    "        [1, 5],    # Request 2: arrives at t=1, needs 5 tokens\n",
    "        [3, 15],   # Request 3: arrives at t=3, needs 15 tokens\n",
    "        [5, 3],    # Request 4: arrives at t=5, needs 3 tokens\n",
    "        [6, 7],    # Request 5: arrives at t=6, needs 7 tokens\n",
    "    ]\n",
    "    \n",
    "    # Colors for each request\n",
    "    colors = ['#ff9999', '#99ff99', '#9999ff', '#ffcc99', '#cc99ff']\n",
    "    \n",
    "    # Static batching (batch size = 2)\n",
    "    ax1.set_title('Static Batching (batch size = 2)')\n",
    "    batch_size = 2\n",
    "    current_time = 0\n",
    "    batch_queue = []\n",
    "    processing_batch = False\n",
    "    batch_end_time = 0\n",
    "    \n",
    "    for t in range(30):  # Simulate 30 time steps\n",
    "        # Add new arrivals to queue\n",
    "        for i, req in enumerate(requests):\n",
    "            if req[0] == t:\n",
    "                batch_queue.append((i, req[1]))\n",
    "        \n",
    "        # Start new batch if we're not processing and have enough requests\n",
    "        if not processing_batch and len(batch_queue) >= batch_size:\n",
    "            processing_batch = True\n",
    "            current_batch = batch_queue[:batch_size]\n",
    "            batch_queue = batch_queue[batch_size:]\n",
    "            \n",
    "            # Determine longest sequence in batch\n",
    "            max_tokens = max([r[1] for r in current_batch])\n",
    "            batch_end_time = t + max_tokens\n",
    "            \n",
    "            # Draw rectangles for each request in this batch\n",
    "            for idx, (req_idx, tokens) in enumerate(current_batch):\n",
    "                rect = Rectangle((t, req_idx), max_tokens, 0.7, color=colors[req_idx], alpha=0.7)\n",
    "                ax1.add_patch(rect)\n",
    "                \n",
    "                # Add diagonal lines for actual token generation\n",
    "                for token in range(tokens):\n",
    "                    ax1.plot([t+token, t+token+1], [req_idx+0.3, req_idx+0.3], color='black', alpha=0.5)\n",
    "                \n",
    "                # Add idle time (waiting for longest sequence)\n",
    "                if tokens < max_tokens:\n",
    "                    rect = Rectangle((t+tokens, req_idx), max_tokens-tokens, 0.7, \n",
    "                                    color='lightgray', alpha=0.5, hatch='//')\n",
    "                    ax1.add_patch(rect)\n",
    "        \n",
    "        # Check if current batch is done\n",
    "        if processing_batch and t == batch_end_time:\n",
    "            processing_batch = False\n",
    "    \n",
    "    # Continuous batching\n",
    "    ax2.set_title('Continuous Batching')\n",
    "    active_requests = {}\n",
    "    \n",
    "    for t in range(30):  # Simulate 30 time steps\n",
    "        # Add new arrivals to active requests\n",
    "        for i, req in enumerate(requests):\n",
    "            if req[0] == t:\n",
    "                active_requests[i] = {'start': t, 'tokens_left': req[1]}\n",
    "        \n",
    "        # Process all active requests\n",
    "        finished = []\n",
    "        for req_idx, req_data in active_requests.items():\n",
    "            if req_data['tokens_left'] > 0:\n",
    "                # Draw token generation step\n",
    "                token_num = req_data['start'] + req[1] - req_data['tokens_left']\n",
    "                ax2.plot([t, t+1], [req_idx+0.3, req_idx+0.3], color='black', alpha=0.5)\n",
    "                \n",
    "                # Draw rectangle for this time step\n",
    "                rect = Rectangle((t, req_idx), 1, 0.7, color=colors[req_idx], alpha=0.7)\n",
    "                ax2.add_patch(rect)\n",
    "                \n",
    "                # Decrement tokens left\n",
    "                req_data['tokens_left'] -= 1\n",
    "                \n",
    "                # Check if request is finished\n",
    "                if req_data['tokens_left'] == 0:\n",
    "                    finished.append(req_idx)\n",
    "        \n",
    "        # Remove finished requests\n",
    "        for req_idx in finished:\n",
    "            del active_requests[req_idx]\n",
    "    \n",
    "    # Formatting\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_xlim(0, 30)\n",
    "        ax.set_ylim(-0.5, len(requests) - 0.5)\n",
    "        ax.set_yticks(range(len(requests)))\n",
    "        ax.set_yticklabels([f'Request {i+1}' for i in range(len(requests))])\n",
    "        ax.set_xlabel('Time steps')\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    handles = [Rectangle((0,0), 1, 1, color=colors[i]) for i in range(len(requests))]\n",
    "    handles.append(Rectangle((0,0), 1, 1, color='lightgray', hatch='//'))\n",
    "    labels = [f'Request {i+1}' for i in range(len(requests))]\n",
    "    labels.append('Idle (waiting for batch)')\n",
    "    \n",
    "    plt.figlegend(handles, labels, loc='lower center', ncol=len(handles), bbox_to_anchor=(0.5, 0))\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    \n",
    "compare_batching_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Continuous Batching\n",
    "\n",
    "Continuous batching provides several critical advantages:\n",
    "\n",
    "1. **Higher GPU Utilization**: Keeps the GPU busy by continuously adding new requests\n",
    "2. **Lower Latency**: No waiting for batch formation or for longer sequences to complete\n",
    "3. **Higher Throughput**: Processes more tokens per second by eliminating idle time\n",
    "4. **Better QoS**: Can implement prioritization for critical requests\n",
    "\n",
    "When combined with PagedAttention, continuous batching allows vLLM to achieve much higher throughput than traditional inference engines, especially under high load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Flash Attention for Compute Optimization\n",
    "\n",
    "While PagedAttention optimizes memory usage and continuous batching improves scheduling, the actual attention computation remains a bottleneck. Flash Attention is an algorithm that optimizes attention computation by reducing memory I/O between GPU high-bandwidth memory (HBM) and on-chip SRAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Flash Attention Works\n",
    "\n",
    "Standard attention implementation has three key inefficiencies:\n",
    "\n",
    "1. **Multiple HBM Accesses**: Reading Q, K, V matrices multiple times\n",
    "2. **Storing Large Attention Matrices**: O(N²) memory for sequence length N\n",
    "3. **Softmax Stability Tricks**: Extra passes to compute max values\n",
    "\n",
    "Flash Attention addresses these issues by:\n",
    "\n",
    "1. **Tiled Computation**: Breaking large matrices into smaller blocks that fit in SRAM\n",
    "2. **Online Softmax Algorithm**: Computing softmax incrementally without storing the full attention matrix\n",
    "3. **Fused Operations**: Combining multiple operations to reduce memory I/O\n",
    "\n",
    "The algorithm time complexity remains O(N²) but with drastically reduced memory I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM Integration with Flash Attention\n",
    "\n",
    "vLLM integrates FlashAttention (via FlashInfer) as its attention backend to further optimize inference performance:\n",
    "\n",
    "1. **Reduced Memory Bandwidth**: Lower memory I/O means faster attention computation\n",
    "2. **Lower GPU Memory Usage**: No need to materialize the full attention matrix\n",
    "3. **Faster Backward Pass**: More efficient gradient computation (when using vLLM for fine-tuning)\n",
    "4. **Optimized for Modern GPUs**: Tailored for NVIDIA architectures (Ampere, Hopper, etc.)\n",
    "\n",
    "The combination of PagedAttention memory efficiency and FlashAttention compute efficiency gives vLLM exceptional performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing Efficient Batch Inference with vLLM\n",
    "\n",
    "Now let's see how to use vLLM in practice. We'll first look at a simple example of how to load a model and generate completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vllm\n",
    "# Basic vLLM usage example\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the model\n",
    "# This loads the model onto the GPU(s)\n",
    "llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Set sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "# Single prompt generation\n",
    "prompts = [\"Write a short story about a robot learning to paint:\"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the generated text\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing with Thread Concurrency\n",
    "\n",
    "Now let's implement a more complex example that demonstrates how vLLM handles concurrent requests efficiently. We'll create multiple threads to simulate concurrent users and see how vLLM processes them using continuous batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import queue\n",
    "import random\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Create a thread-safe queue for results\n",
    "result_queue = queue.Queue()\n",
    "\n",
    "# Define some varied prompts with different expected lengths\n",
    "prompts = [\n",
    "    \"Explain quantum computing in one sentence:\",  # Short\n",
    "    \"Write a haiku about programming:\",  # Short\n",
    "    \"List 5 tips for productive coding sessions:\",  # Medium\n",
    "    \"Explain the difference between supervised and unsupervised learning:\",  # Medium\n",
    "    \"Write a short story about a programmer who discovers a bug that leads to an adventure:\",  # Long\n",
    "    \"Describe in detail how transformers work in deep learning:\",  # Long\n",
    "]\n",
    "\n",
    "# Function to generate text for a given prompt\n",
    "def generate_text(llm, prompt_id, prompt, max_tokens):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Configure sampling parameters - vary them to simulate different request types\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    # Generate the text\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Add result to queue\n",
    "    result_queue.put({\n",
    "        'prompt_id': prompt_id,\n",
    "        'prompt': prompt,\n",
    "        'text': generated_text,\n",
    "        'max_tokens': max_tokens,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'tokens_per_second': len(generated_text.split()) / elapsed_time\n",
    "    })\n",
    "\n",
    "# Main function to simulate concurrent requests\n",
    "def run_concurrent_inference():\n",
    "    # Initialize the model once (shared across all threads)\n",
    "    print(\"Loading model...\")\n",
    "    llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "    print(\"Model loaded!\")\n",
    "    \n",
    "    # Create threads for concurrent requests\n",
    "    threads = []\n",
    "    num_requests = 10  # Total number of requests to process\n",
    "    \n",
    "    # Staggered launch of requests to simulate real-world scenario\n",
    "    for i in range(num_requests):\n",
    "        # Select a random prompt from our list\n",
    "        prompt = random.choice(prompts)\n",
    "        \n",
    "        # Vary max tokens to simulate different response lengths\n",
    "        max_tokens = random.choice([32, 64, 128, 256])\n",
    "        \n",
    "        # Create a thread for this request\n",
    "        thread = threading.Thread(\n",
    "            target=generate_text,\n",
    "            args=(llm, i, prompt, max_tokens)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "    \n",
    "    # Start all threads with a small delay between them\n",
    "    print(f\"Starting {num_requests} concurrent requests...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "        # Small random delay to simulate staggered arrivals\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "    \n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"All requests completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Process and display results\n",
    "    results = []\n",
    "    while not result_queue.empty():\n",
    "        results.append(result_queue.get())\n",
    "    \n",
    "    # Sort results by completion time\n",
    "    results.sort(key=lambda x: x['elapsed_time'])\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\nRequest Statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'ID':^5} | {'Max Tokens':^10} | {'Time (s)':^10} | {'Tokens/s':^10} | {'Prompt':30}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for result in results:\n",
    "        prompt_short = result['prompt'][:30] + \"...\" if len(result['prompt']) > 30 else result['prompt']\n",
    "        print(f\"{result['prompt_id']:^5} | {result['max_tokens']:^10} | {result['elapsed_time']:.2f}s | {result['tokens_per_second']:.2f} | {prompt_short}\")\n",
    "    \n",
    "    # Calculate aggregate statistics\n",
    "    avg_time = sum(r['elapsed_time'] for r in results) / len(results)\n",
    "    total_tokens = sum(r['max_tokens'] for r in results)\n",
    "    overall_tokens_per_sec = total_tokens / total_time\n",
    "    \n",
    "    print(\"\\nAggregate Statistics:\")\n",
    "    print(f\"Average request time: {avg_time:.2f}s\")\n",
    "    print(f\"Total tokens generated: {total_tokens}\")\n",
    "    print(f\"Overall tokens per second: {overall_tokens_per_sec:.2f}\")\n",
    "    print(f\"Throughput: {len(results) / total_time:.2f} requests per second\")\n",
    "    \n",
    "    # Return results for additional analysis if needed\n",
    "    return results\n",
    "\n",
    "# Run the concurrent inference test\n",
    "results = run_concurrent_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a vLLM Server\n",
    "\n",
    "For production deployment, vLLM offers a server mode with a REST API compatible with the OpenAI API. This makes it easy to integrate into existing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command to start a vLLM server\n",
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can query the server using standard HTTP requests that mimic the OpenAI API format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Example of calling a vLLM server with OpenAI-compatible API\n",
    "def query_vllm_server(prompt, max_tokens=100):\n",
    "    url = \"http://localhost:8000/v1/completions\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example usage (if server is running)\n",
    "# result = query_vllm_server(\"Write a haiku about artificial intelligence:\")\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced vLLM Configurations\n",
    "\n",
    "vLLM supports several advanced configurations to further optimize performance:\n",
    "\n",
    "### Tensor Parallelism\n",
    "\n",
    "For large models that don't fit on a single GPU, vLLM supports tensor parallelism to split the model across multiple GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using tensor parallelism across multiple GPUs\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load a larger model with tensor parallelism across GPUs\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    tensor_parallel_size=4,  # Use 4 GPUs\n",
    "    gpu_memory_utilization=0.85,  # Control memory usage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "vLLM supports various quantization methods to reduce memory footprint with minimal impact on accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a quantized model\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "llm = LLM(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    quantization=\"awq\",  # Activation-aware Weight Quantization\n",
    "    dtype=\"half\"  # FP16 for non-quantized parts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KV Cache Size\n",
    "\n",
    "You can control the amount of memory allocated to KV cache to balance between memory usage and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of controlling KV cache size\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Adjust block size and max number of batched tokens\n",
    "llm = LLM(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    block_size=16,  # Size of each memory block in tokens\n",
    "    max_num_batched_tokens=4096,  # Max tokens across all requests\n",
    "    max_num_seqs=256  # Max number of concurrent sequences\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "vLLM represents a significant advancement in LLM inference optimization, addressing key bottlenecks through innovations like:\n",
    "\n",
    "1. **PagedAttention**: Efficient memory management of KV cache\n",
    "2. **Continuous Batching**: Dynamic handling of requests for maximum GPU utilization\n",
    "3. **Flash Attention Integration**: Optimized attention computation\n",
    "4. **Distributed Inference**: Support for multi-GPU deployment\n",
    "\n",
    "These optimizations together enable vLLM to deliver 2-18x higher throughput compared to other inference engines while maintaining low latency. As LLMs continue to grow in size and importance, inference engines like vLLM will be critical for making these models practically deployable in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [PagedAttention: Paging KV Cache for Unlimited Context in Large Language Models](https://arxiv.org/abs/2309.06180)\n",
    "- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)\n",
    "- [Deploying vLLM: A Step-by-Step Guide](https://ploomber.io/blog/vllm-deploy/)\n",
    "- [vLLM GitHub Repository](https://github.com/vllm-project/vllm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}