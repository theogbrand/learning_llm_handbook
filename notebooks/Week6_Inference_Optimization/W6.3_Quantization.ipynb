{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W6.3: Model Quantization for Training and Inference\n",
    "\n",
    "In this notebook, we'll explore model quantization techniques for both training and inference. Quantization reduces the precision of model weights, activations, and gradients, enabling:\n",
    "\n",
    "- Smaller memory footprints\n",
    "- Faster inference and training\n",
    "- Deployment on resource-constrained devices\n",
    "- Ability to run larger models on smaller GPUs\n",
    "\n",
    "We'll cover:\n",
    "1. Quantization fundamentals\n",
    "2. Training with quantization using bitsandbytes\n",
    "3. Inference quantization using GGUF and llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### The Mathematics of Quantization\n\nBefore diving deeper into quantization techniques, let's understand the mathematical operations that happen during the quantization process with a concrete example.\n\nQuantization involves mapping a continuous set of values (like 32-bit floating point weights) to a discrete set of values (like 8-bit integers). The two most common approaches are:\n\n1. **Absmax Quantization**\n2. **Zero-point Quantization**\n\nLet's see how they work mathematically:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to perform absmax quantization\ndef absmax_quantize(X, bits=8):\n    \"\"\"\n    Quantize tensor using absolute maximum method\n    \n    Parameters:\n    - X: Input tensor\n    - bits: Number of bits for quantization (default: 8)\n    \n    Returns:\n    - Quantized tensor (in int format)\n    - Dequantized tensor (reconstructed to original format)\n    - Scale factor\n    \"\"\"\n    # Calculate max range for target bits\n    max_val = 2**(bits-1) - 1  # For 8 bits: 127\n    \n    # Calculate scale (maps max absolute value to max range)\n    abs_max = torch.max(torch.abs(X)).item()\n    scale = max_val / abs_max\n    \n    # Quantize: scale and round\n    X_quant = torch.round(X * scale)\n    \n    # Clip values to ensure they fit in the target range\n    X_quant = torch.clamp(X_quant, -max_val, max_val)\n    \n    # Dequantize: divide by scale to return to original range\n    X_dequant = X_quant / scale\n    \n    return X_quant.to(torch.int8), X_dequant, scale\n\n# Function to perform zero-point quantization\ndef zeropoint_quantize(X, bits=8):\n    \"\"\"\n    Quantize tensor using zero-point method\n    \n    Parameters:\n    - X: Input tensor\n    - bits: Number of bits for quantization (default: 8)\n    \n    Returns:\n    - Quantized tensor (in int format)\n    - Dequantized tensor (reconstructed to original format)\n    - Scale factor\n    - Zero point\n    \"\"\"\n    # Calculate range for target bits\n    max_val = 2**(bits-1) - 1  # For 8 bits: 127\n    \n    # Get min and max values from tensor\n    x_min = torch.min(X).item()\n    x_max = torch.max(X).item()\n    \n    # Calculate value range (denominator)\n    x_range = x_max - x_min\n    x_range = 1.0 if x_range == 0 else x_range\n    \n    # Calculate scale\n    scale = (2*max_val) / x_range\n    \n    # Calculate zero point (maps x_min to -max_val)\n    zero_point = torch.round(-scale * x_min - max_val)\n    \n    # Quantize: scale, add zero point, and round\n    X_quant = torch.round(X * scale + zero_point)\n    \n    # Clip values to ensure they fit in the target range\n    X_quant = torch.clamp(X_quant, -max_val, max_val)\n    \n    # Dequantize: subtract zero point and divide by scale\n    X_dequant = (X_quant - zero_point) / scale\n    \n    return X_quant.to(torch.int8), X_dequant, scale, zero_point",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Let's demonstrate quantization with a concrete example\n# Create a small tensor with some weights (simulating a model's weight matrix)\nweights = torch.tensor([\n    [-0.8543, -0.4328,  0.2731,  0.9126],\n    [ 0.1892,  0.7253, -0.6105, -0.3217],\n    [-0.2944,  0.5312,  0.0891, -0.1734]\n], dtype=torch.float32)\n\nprint(\"Original weights (FP32):\")\nprint(weights)\nprint(f\"Memory usage: {weights.nelement() * weights.element_size()} bytes\")\n\n# Absmax quantization\nweights_absmax_quant, weights_absmax_dequant, scale_absmax = absmax_quantize(weights)\n\nprint(\"\\nAbsmax quantized weights (INT8):\")\nprint(weights_absmax_quant)\nprint(f\"Memory usage: {weights_absmax_quant.nelement() * weights_absmax_quant.element_size()} bytes\")\nprint(f\"Scale factor: {scale_absmax:.6f}\")\n\nprint(\"\\nAbsmax dequantized weights (approximated back to float):\")\nprint(weights_absmax_dequant)\n\n# Calculate quantization error for absmax\nabsmax_error = torch.abs(weights - weights_absmax_dequant)\nprint(\"\\nAbsmax quantization absolute error:\")\nprint(absmax_error)\nprint(f\"Mean absolute error: {torch.mean(absmax_error):.6f}\")\n\n# Zero-point quantization\nweights_zp_quant, weights_zp_dequant, scale_zp, zero_point = zeropoint_quantize(weights)\n\nprint(\"\\nZero-point quantized weights (INT8):\")\nprint(weights_zp_quant)\nprint(f\"Memory usage: {weights_zp_quant.nelement() * weights_zp_quant.element_size()} bytes\")\nprint(f\"Scale factor: {scale_zp:.6f}\")\nprint(f\"Zero point: {zero_point:.2f}\")\n\nprint(\"\\nZero-point dequantized weights (approximated back to float):\")\nprint(weights_zp_dequant)\n\n# Calculate quantization error for zero-point\nzp_error = torch.abs(weights - weights_zp_dequant)\nprint(\"\\nZero-point quantization absolute error:\")\nprint(zp_error)\nprint(f\"Mean absolute error: {torch.mean(zp_error):.6f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize the weights before and after quantization\nplt.figure(figsize=(15, 8))\n\n# Original weights\nplt.subplot(2, 2, 1)\nplt.imshow(weights, cmap='coolwarm')\nplt.colorbar()\nplt.title('Original Weights (FP32)')\n\n# Absmax quantized\nplt.subplot(2, 2, 2)\nplt.imshow(weights_absmax_dequant, cmap='coolwarm')\nplt.colorbar()\nplt.title('Absmax Quantized/Dequantized Weights')\n\n# Absmax error\nplt.subplot(2, 2, 3)\nplt.imshow(absmax_error, cmap='Reds')\nplt.colorbar()\nplt.title('Absmax Quantization Error')\n\n# Zero-point error\nplt.subplot(2, 2, 4)\nplt.imshow(zp_error, cmap='Reds')\nplt.colorbar()\nplt.title('Zero-point Quantization Error')\n\nplt.tight_layout()\nplt.show()\n\n# Show value distributions\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.hist(weights.numpy().flatten(), bins=30, alpha=0.7, color='blue')\nplt.title('Original Weight Distribution')\nplt.xlabel('Weight Value')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 3, 2)\nplt.hist(weights_absmax_dequant.numpy().flatten(), bins=30, alpha=0.7, color='green')\nplt.title('Absmax Quantized Weight Distribution')\nplt.xlabel('Weight Value')\n\nplt.subplot(1, 3, 3)\nplt.hist(weights_zp_dequant.numpy().flatten(), bins=30, alpha=0.7, color='orange')\nplt.title('Zero-point Quantized Weight Distribution')\nplt.xlabel('Weight Value')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding the Quantization Process\n\nThe example above demonstrates the two main approaches to weight quantization:\n\n#### **1. Absmax Quantization**\n\nAbsmax quantization scales values based on the absolute maximum value in the tensor:\n\n1. Find the maximum absolute value in the tensor: `abs_max = max(abs(weights))`\n2. Calculate the scale factor: `scale = 127 / abs_max` (for 8-bit quantization)\n3. Quantize by scaling and rounding: `weights_quant = round(weights * scale)`\n4. Dequantize by dividing by the scale: `weights_dequant = weights_quant / scale`\n\nThis method centers around zero and uses symmetric quantization, which works well for weights that are roughly centered around zero.\n\n#### **2. Zero-point Quantization**\n\nZero-point quantization uses both a scale and an offset (zero point) to better utilize the quantized range:\n\n1. Find the min and max values: `x_min = min(weights)`, `x_max = max(weights)`\n2. Calculate the scale: `scale = 255 / (x_max - x_min)` (for 8-bit)\n3. Calculate the zero point: `zero_point = round(-scale * x_min - 128)`\n4. Quantize: `weights_quant = round(weights * scale + zero_point)`\n5. Dequantize: `weights_dequant = (weights_quant - zero_point) / scale`\n\nThis method is especially valuable for activations or when weights are not centered around zero.\n\n#### **Key Observations**\n\n- **Memory Savings**: As demonstrated, 8-bit weights use 1/4 the memory of 32-bit weights\n- **Quantization Error**: The process introduces small errors that can accumulate in deep networks\n- **Distribution Preservation**: Both methods preserve the overall distribution shape, but with discretization\n- **Trade-offs**: Zero-point quantization can utilize the full range better but requires storing an additional parameter (the zero point)\n\n#### **4-bit Quantization**\n\nFor even more aggressive compression (as used in the bitsandbytes library's 4-bit methods), the same principles apply but with smaller ranges:\n\n- For INT4, the range is from -8 to 7 (16 values total)\n- Special quantization schemes like NF4 (normalized float 4) are optimized for the statistical distribution of LLM weights\n\nAt 4-bit precision, careful calibration and specialized quantization schemes become increasingly important to preserve model accuracy.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Let's demonstrate 4-bit quantization effects\ndef int4_quantize(X):\n    \"\"\"Simple 4-bit quantization demonstration\"\"\"\n    # For 4-bit, range is -8 to 7\n    max_val = 7\n    \n    # Calculate scale (maps max absolute value to max range)\n    abs_max = torch.max(torch.abs(X)).item()\n    scale = max_val / abs_max\n    \n    # Quantize: scale and round\n    X_quant = torch.round(X * scale)\n    \n    # Clip values to ensure they fit in the 4-bit range\n    X_quant = torch.clamp(X_quant, -8, 7)\n    \n    # Dequantize: divide by scale to return to original range\n    X_dequant = X_quant / scale\n    \n    return X_quant.to(torch.int8), X_dequant, scale\n\n# Apply 4-bit quantization to our weights\nweights_4bit_quant, weights_4bit_dequant, scale_4bit = int4_quantize(weights)\n\nprint(\"Original weights (FP32):\")\nprint(weights)\nprint(f\"Memory usage: {weights.nelement() * weights.element_size()} bytes\")\n\nprint(\"\\n4-bit quantized weights:\")\nprint(weights_4bit_quant)\n\nprint(\"\\n4-bit dequantized weights:\")\nprint(weights_4bit_dequant)\n\n# Calculate quantization error for 4-bit\nerror_4bit = torch.abs(weights - weights_4bit_dequant)\nprint(\"\\n4-bit quantization absolute error:\")\nprint(error_4bit)\nprint(f\"Mean absolute error: {torch.mean(error_4bit):.6f}\")\n\n# Compare to 8-bit error\nprint(f\"\\n8-bit mean absolute error: {torch.mean(absmax_error):.6f}\")\nprint(f\"4-bit mean absolute error: {torch.mean(error_4bit):.6f}\")\nprint(f\"Error increase: {torch.mean(error_4bit)/torch.mean(absmax_error):.2f}x\")\n\n# Visualize 4-bit quantization \nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(weights, cmap='coolwarm')\nplt.colorbar()\nplt.title('Original Weights (FP32)')\n\nplt.subplot(1, 3, 2)\nplt.imshow(weights_absmax_dequant, cmap='coolwarm')\nplt.colorbar()\nplt.title('8-bit Quantized Weights')\n\nplt.subplot(1, 3, 3)\nplt.imshow(weights_4bit_dequant, cmap='coolwarm')\nplt.colorbar()\nplt.title('4-bit Quantized Weights')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impact on Model Size and Performance\n\nTo understand how quantization affects real models, let's calculate the theoretical memory requirements for different quantization levels of a large language model:\n\n| Model Size | FP32 (32-bit) | FP16 (16-bit) | INT8 (8-bit) | INT4 (4-bit) |\n|------------|--------------|--------------|------------|------------|\n| 7B params | 28 GB | 14 GB | 7 GB | 3.5 GB |\n| 13B params | 52 GB | 26 GB | 13 GB | 6.5 GB |\n| 70B params | 280 GB | 140 GB | 70 GB | 35 GB |\n\nAs you can see, a 70B parameter model that would require 280GB in FP32 precision can be reduced to just 35GB in 4-bit precision, making it possible to run on consumer hardware.\n\nNow that we understand the mathematics behind quantization, let's explore how to apply these techniques in practice using specialized libraries.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quantization Fundamentals\n",
    "\n",
    "### What is Quantization?\n",
    "\n",
    "Quantization is the process of reducing the precision of numbers used in a model. Modern deep learning models typically use 32-bit floating-point (FP32) numbers by default, but we can represent these values with fewer bits using various quantization techniques.\n",
    "\n",
    "### Common Quantization Formats\n",
    "\n",
    "- **FP32 (32-bit floating point)**: Standard full precision format\n",
    "- **FP16 (16-bit floating point)**: Half precision, 1 sign bit, 5 exponent bits, 10 mantissa bits\n",
    "- **BF16 (Brain Floating Point)**: 1 sign bit, 8 exponent bits, 7 mantissa bits\n",
    "- **INT8 (8-bit integer)**: Integer-only quantization, often with scaling factors\n",
    "- **INT4 (4-bit integer)**: Extreme quantization for inference only\n",
    "- **Mixed precision**: Different precisions for different parts of the model\n",
    "\n",
    "### Quantization Benefits\n",
    "\n",
    "- **Memory efficiency**: 4-bit models use ~8x less memory than FP32\n",
    "- **Throughput improvements**: Lower precision calculations are faster\n",
    "- **Smaller models**: Enables local deployment and running larger models on consumer hardware\n",
    "- **Energy efficiency**: Less compute = less power consumption\n",
    "\n",
    "### Quantization Approaches\n",
    "\n",
    "1. **Post-Training Quantization (PTQ)**: Apply quantization after a model is fully trained\n",
    "2. **Quantization-Aware Training (QAT)**: Train with simulated quantization to minimize accuracy loss\n",
    "3. **Quantized Fine-Tuning (QLoRA)**: Fine-tune a pre-trained model using quantized weights and adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training with Quantization using BitsAndBytes\n",
    "\n",
    "The [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) library provides efficient quantization techniques for both training and inference. Let's explore how to use it for quantized training, particularly for fine-tuning large language models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required libraries\n",
    "!pip install torch transformers bitsandbytes peft trl accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading a Model with 4-bit Quantization\n",
    "\n",
    "Let's load a pre-trained model using 4-bit quantization with bitsandbytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,           # Load model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",   # Normalized float 4 - optimized for LLM weights\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute in fp16\n",
    "    bnb_4bit_use_double_quant=True   # Nested quantization for further memory savings\n",
    ")\n",
    "\n",
    "# Load a smaller model for demonstration purposes\n",
    "model_id = \"bigscience/bloom-1b7\"\n",
    "\n",
    "# Load model with quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Memory Comparison: Quantized vs. Full Precision\n",
    "\n",
    "Let's compare the memory usage between quantized and full precision models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 3)\n",
    "\n",
    "# Clear memory and cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "baseline_memory = get_memory_usage()\n",
    "print(f\"Baseline memory usage: {baseline_memory:.2f} GB\")\n",
    "\n",
    "# Load model in 4-bit\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "memory_4bit = get_memory_usage()\n",
    "print(f\"Memory with 4-bit model: {memory_4bit:.2f} GB (delta: {memory_4bit - baseline_memory:.2f} GB)\")\n",
    "\n",
    "# Clear memory\n",
    "del model_4bit\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model in FP16\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "memory_fp16 = get_memory_usage()\n",
    "print(f\"Memory with FP16 model: {memory_fp16:.2f} GB (delta: {memory_fp16 - baseline_memory:.2f} GB)\")\n",
    "\n",
    "# Memory savings\n",
    "savings = ((memory_fp16 - baseline_memory) - (memory_4bit - baseline_memory)) / (memory_fp16 - baseline_memory) * 100\n",
    "print(f\"Memory savings with 4-bit quantization: {savings:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 QLoRA: Quantized Low-Rank Adaptation\n",
    "\n",
    "QLoRA is a technique that combines quantization with Low-Rank Adaptation (LoRA) to enable efficient fine-tuning of LLMs. The base model weights remain quantized and frozen, while a small number of trainable adapter parameters are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=16,               # Rank of LoRA adaptation matrices\n",
    "    lora_alpha=32,      # LoRA scaling factor\n",
    "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",        # Don't train bias parameters\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for causal language modeling\n",
    "    # Target modules to apply LoRA to (model-specific)\n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]\n",
    ")\n",
    "\n",
    "# Create PEFT model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fine-tuning with Quantized Model\n",
    "\n",
    "Now, let's set up a basic fine-tuning process for our quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load a small sample dataset for demonstration\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "dataset = dataset.select(range(1000))  # Use a small subset\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"quote\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"quote\", \"author\"])\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,  # Use mixed precision training\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tune the model (commented out for notebook purposes)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Saving and Loading Quantized Models\n",
    "\n",
    "After fine-tuning, we can save and load our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the LoRA adapter weights only (much smaller than full model)\n",
    "model.save_pretrained(\"./lora_adapter\")\n",
    "\n",
    "# To use the model later:\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model with quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter\n",
    "peft_model_id = \"./lora_adapter\"\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "# Optionally merge the adapter weights with the base model\n",
    "# merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference Quantization with GGUF and llama.cpp\n",
    "\n",
    "For inference-only applications, we can use more aggressive quantization techniques. [llama.cpp](https://github.com/ggml-org/llama.cpp) is a popular C/C++ implementation that enables highly optimized inference on CPU and GPU with various quantization formats using the GGUF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install llama-cpp-python\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understanding GGUF Format\n",
    "\n",
    "GGUF (GGML Universal Format) is a file format designed for storing and sharing quantized models. It's the successor to GGML format and has improved support for metadata and different model architectures.\n",
    "\n",
    "Key features of GGUF:\n",
    "- Stores model architecture, weights, and tokenizer information\n",
    "- Supports multiple quantization methods (Q4_K, Q5_K, Q6_K, Q8_0, etc.)\n",
    "- Enables efficient CPU and GPU inference\n",
    "- Platform-independent\n",
    "\n",
    "Common quantization types in GGUF:\n",
    "- **Q4_0**: 4-bit quantization (simplest)\n",
    "- **Q4_K**: 4-bit quantization with K-quants (better accuracy)\n",
    "- **Q5_K**: 5-bit quantization with K-quants\n",
    "- **Q8_0**: 8-bit quantization (higher quality, larger size)\n",
    "- **F16**: 16-bit floating point (highest quality, largest size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Converting a Model to GGUF\n",
    "\n",
    "Converting models to GGUF typically involves using the conversion scripts in the llama.cpp repository. For demonstration, we'll show the command-line process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone llama.cpp repo and convert a model (commented out since this is for reference)\n",
    "\n",
    "'''\n",
    "# Clone llama.cpp repository\n",
    "!git clone https://github.com/ggml-org/llama.cpp\n",
    "!cd llama.cpp && mkdir build && cd build && cmake .. && make -j\n",
    "\n",
    "# Convert model to GGUF (assuming you have a HuggingFace model)\n",
    "!python llama.cpp/convert.py /path/to/hf/model --outfile model_base.gguf\n",
    "\n",
    "# Quantize to 4-bit\n",
    "!llama.cpp/build/bin/llama-quantize model_base.gguf model_q4_0.gguf q4_0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Using a Quantized GGUF Model with llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# For demonstration, we'll use a remote path to a pre-quantized GGUF model\n",
    "# In a real application, you'd use a local path to your converted model\n",
    "# model_path = \"model_q4_0.gguf\"\n",
    "\n",
    "# For demonstration, let's assume we've downloaded a quantized model\n",
    "model_path = \"./tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"  # Replace with your model path\n",
    "\n",
    "# If you don't have a model available, we'll provide code that\n",
    "# shows what you would do assuming you have the model\n",
    "\n",
    "'''\n",
    "# Initialize the model\n",
    "llm = Llama(\n",
    "    model_path=model_path,  # Path to the quantized model\n",
    "    n_ctx=2048,             # Context window size\n",
    "    n_batch=512,            # Batch size for prompt processing\n",
    "    n_gpu_layers=-1,        # Number of layers to offload to GPU (-1 = all)\n",
    "    verbose=False           # Verbose output\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Write a short poem about quantization:\"\n",
    "output = llm(prompt, max_tokens=200, temperature=0.7, top_p=0.95)\n",
    "\n",
    "# Print the result\n",
    "print(output[\"choices\"][0][\"text\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Benchmarking Different Quantization Levels\n",
    "\n",
    "Let's compare the performance of different quantization levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model_path, prompt, max_tokens=100):\n",
    "    \"\"\"Benchmark model performance\"\"\"\n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    \n",
    "    # Code for benchmarking, which would run if you have the models\n",
    "    '''\n",
    "    # Initialize model\n",
    "    start_time = time.time()\n",
    "    llm = Llama(model_path=model_path, n_ctx=2048, n_batch=512)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"  Load time: {load_time:.2f} seconds\")\n",
    "    \n",
    "    # Get memory usage\n",
    "    memory_usage = get_memory_usage()\n",
    "    print(f\"  Memory usage: {memory_usage:.2f} GB\")\n",
    "    \n",
    "    # Generate text\n",
    "    start_time = time.time()\n",
    "    output = llm(prompt, max_tokens=max_tokens, temperature=0.7)\n",
    "    generate_time = time.time() - start_time\n",
    "    tokens_per_second = max_tokens / generate_time\n",
    "    print(f\"  Generation time: {generate_time:.2f} seconds\")\n",
    "    print(f\"  Tokens per second: {tokens_per_second:.2f}\")\n",
    "    '''\n",
    "    \n",
    "    # For demonstration, we'll just show expected results\n",
    "    print(\"  Sample benchmark data (for demonstration)\")\n",
    "    if \"Q4_0\" in model_path:\n",
    "        print(\"  Load time: 1.25 seconds\")\n",
    "        print(\"  Memory usage: 0.8 GB\")\n",
    "        print(\"  Generation time: 5.32 seconds\")\n",
    "        print(\"  Tokens per second: 18.8\")\n",
    "    elif \"Q5_K\" in model_path:\n",
    "        print(\"  Load time: 1.47 seconds\")\n",
    "        print(\"  Memory usage: 0.95 GB\")\n",
    "        print(\"  Generation time: 5.15 seconds\")\n",
    "        print(\"  Tokens per second: 19.4\")\n",
    "    elif \"Q8_0\" in model_path:\n",
    "        print(\"  Load time: 1.78 seconds\")\n",
    "        print(\"  Memory usage: 1.45 GB\")\n",
    "        print(\"  Generation time: 4.98 seconds\")\n",
    "        print(\"  Tokens per second: 20.1\")\n",
    "    elif \"F16\" in model_path:\n",
    "        print(\"  Load time: 2.34 seconds\")\n",
    "        print(\"  Memory usage: 2.8 GB\")\n",
    "        print(\"  Generation time: 4.75 seconds\")\n",
    "        print(\"  Tokens per second: 21.0\")\n",
    "    print(\"\")\n",
    "\n",
    "# Benchmark different quantization levels (demonstration)\n",
    "benchmark_model(\"./model_Q4_0.gguf\", \"Explain the concept of neural networks:\") \n",
    "benchmark_model(\"./model_Q5_K.gguf\", \"Explain the concept of neural networks:\")\n",
    "benchmark_model(\"./model_Q8_0.gguf\", \"Explain the concept of neural networks:\")\n",
    "benchmark_model(\"./model_F16.gguf\", \"Explain the concept of neural networks:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Real-world Considerations\n",
    "\n",
    "When choosing a quantization level for your application, consider these factors:\n",
    "\n",
    "1. **Hardware Constraints**:\n",
    "   - Limited RAM: Use 4-bit or 5-bit quantization\n",
    "   - Limited CPU/GPU: Lower precision is typically faster\n",
    "   - Mobile/edge devices: Extreme quantization (Q2_K, Q3_K, Q4_0)\n",
    "\n",
    "2. **Task Requirements**:\n",
    "   - Complex reasoning: Higher precision (Q6_K, Q8_0, F16)\n",
    "   - Simple text generation: Lower precision may be sufficient\n",
    "   - Creative writing: May need higher precision\n",
    "\n",
    "3. **Quality vs. Performance Tradeoff**:\n",
    "   - Q4_0/Q4_K: Fastest, smallest, lowest quality\n",
    "   - Q5_K: Good balance for many applications\n",
    "   - Q8_0: Higher quality, larger size, slower\n",
    "   - F16: Highest quality, largest size, slowest\n",
    "\n",
    "4. **Specific Model Characteristics**:\n",
    "   - Some models are more robust to quantization than others\n",
    "   - Instruction-tuned models often maintain quality better at lower precision\n",
    "   - LLMs with more parameters may need higher precision to maintain quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Server Deployment with llama.cpp\n",
    "\n",
    "For a production deployment, you might want to run llama.cpp as a server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run llama.cpp as a server (command-line reference)\n",
    "\n",
    "'''\n",
    "# Start server\n",
    "!llama.cpp/build/bin/llama-server \\\n",
    "  -m model_q4_0.gguf \\\n",
    "  -c 2048 \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8080\n",
    "'''\n",
    "\n",
    "# Example client request code\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def query_llama_server(prompt, max_tokens=100):\n",
    "    \"\"\"Query the llama.cpp server\"\"\"\n",
    "    url = \"http://localhost:8080/completion\"\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"n_predict\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example request (commented out since server isn't running)\n",
    "# result = query_llama_server(\"Explain quantization in simple terms:\")\n",
    "# print(result[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choosing the Right Quantization Strategy\n",
    "\n",
    "### Training vs. Inference Quantization\n",
    "\n",
    "| Aspect | Training Quantization (bitsandbytes) | Inference Quantization (GGUF) |\n",
    "|--------|--------------------------------------|--------------------------------|\n",
    "| **Primary Goal** | Enable training/fine-tuning on consumer hardware | Maximize inference speed and minimize memory usage |\n",
    "| **Precision** | Usually 4-bit, 8-bit | 2-bit to 8-bit options |\n",
    "| **Implementation** | PyTorch-based, integrated with HF | C/C++ implementation |\n",
    "| **Adaptability** | Parameters can be updated | Fixed weights |\n",
    "| **Use Case** | Fine-tuning large models on limited hardware | Deploying models for inference on various devices |\n",
    "| **Quality Impact** | Minimal with QLoRA | Varies by quantization level |\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "1. **Are you fine-tuning or just running inference?**\n",
    "   - Fine-tuning: bitsandbytes with QLoRA\n",
    "   - Inference only: GGUF with llama.cpp\n",
    "\n",
    "2. **What hardware constraints do you have?**\n",
    "   - Consumer GPU (8-24GB): 4-bit quantization\n",
    "   - CPU only: GGUF with Q4_0 or Q4_K\n",
    "   - Edge device: GGUF with Q2_K or Q3_K\n",
    "\n",
    "3. **What is your quality tolerance?**\n",
    "   - Highest quality: FP16 or Q8_0\n",
    "   - Balanced: Q5_K or Q6_K\n",
    "   - Prioritize efficiency: Q4_0 or Q4_K\n",
    "\n",
    "4. **What is your target use case?**\n",
    "   - Research: Higher precision\n",
    "   - Production API: Balanced approach\n",
    "   - Local applications: Lower precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Quantization is a powerful technique that makes large models accessible on consumer hardware. Key takeaways:\n",
    "\n",
    "1. **For training and fine-tuning**:\n",
    "   - bitsandbytes with QLoRA enables efficient fine-tuning of large models\n",
    "   - 4-bit quantization can reduce memory usage by 75%+ compared to FP16\n",
    "   - Only a small fraction of parameters need to be updated\n",
    "\n",
    "2. **For inference**:\n",
    "   - GGUF format with llama.cpp provides highly optimized inference\n",
    "   - Multiple quantization levels allow balancing quality and efficiency\n",
    "   - Enables deployment on a wide range of hardware, from servers to mobile devices\n",
    "\n",
    "3. **Practical applications**:\n",
    "   - Run 70B parameter models on consumer GPUs\n",
    "   - Deploy models locally without cloud dependencies\n",
    "   - Reduce inference latency and cost in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}