{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to CUDA Kernel Tiling and GPU Profiling\n",
    "\n",
    "This notebook provides a gentle introduction to:\n",
    "1. Understanding GPU memory hierarchy\n",
    "2. The concept of tiling in CUDA\n",
    "3. How to implement tiled matrix multiplication\n",
    "4. Profiling CUDA kernels with NVIDIA Nsight Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding GPU Memory Hierarchy\n",
    "\n",
    "Before discussing tiling, it's important to understand the memory hierarchy in CUDA GPUs:\n",
    "\n",
    "| Memory Type | Scope | Lifetime | Speed | Size |\n",
    "|-------------|-------|----------|-------|-------|\n",
    "| Global Memory | All threads | Application | Slowest | Largest (Several GB) |\n",
    "| Shared Memory | Thread block | Block | Fast | Limited (KB per block) |\n",
    "| Registers | Thread | Thread | Fastest | Very limited |\n",
    "\n",
    "The goal of most optimizations is to minimize accesses to global memory (which is slow) and maximize the use of shared memory and registers (which are fast)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Tiling?\n",
    "\n",
    "Tiling is an optimization technique that breaks a large computation into smaller chunks (tiles) that can fit in faster memory. In the context of CUDA:\n",
    "\n",
    "1. **Load data from global memory to shared memory** in tiles\n",
    "2. **Process the data within the tile** using threads in the same block\n",
    "3. **Move to the next tile** until all data is processed\n",
    "\n",
    "This approach significantly reduces global memory accesses, which are a major bottleneck in GPU computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Explanation of Tiling in Matrix Multiplication\n",
    "\n",
    "For matrix multiplication C = A * B:\n",
    "\n",
    "**Without tiling:**\n",
    "- Each thread loads multiple elements from A and B from global memory\n",
    "- Performs multiplications and additions\n",
    "- Writes one element to C\n",
    "\n",
    "**With tiling:**\n",
    "- Threads in a block cooperatively load a tile of A and a tile of B into shared memory\n",
    "- Each thread uses these shared memory tiles for calculations\n",
    "- Move to the next tile until the entire row/column is processed\n",
    "- Write result to C\n",
    "\n",
    "```\n",
    "Without Tiling:                  With Tiling:\n",
    "+-------+                        +-------+\n",
    "|       |                        |       |\n",
    "| A     |                        | A     |\n",
    "|       |                        |       |\n",
    "+-------+                        +-------+\n",
    "                                   |    |\n",
    "+-------+     ==>     +-------+  +---+ +---+\n",
    "|       |                        |Tile| |Tile|\n",
    "| B     |                        +---+ +---+\n",
    "|       |                          |     |\n",
    "+-------+                        Shared Memory\n",
    "                                     |\n",
    "                                Fast Computation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Matrix Multiplication Without Tiling\n",
    "\n",
    "Let's first look at a basic CUDA kernel for matrix multiplication without tiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "// Matrix multiplication kernel - without tiling\n",
    "__global__ void matrixMulNaive(float* A, float* B, float* C, int N) {\n",
    "    // Calculate row and column indices for this thread\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Check if indices are within the matrix dimensions\n",
    "    if (row < N && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        // Compute the dot product of the row of A and column of B\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            sum += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Issues with Naive Implementation\n",
    "\n",
    "In this naive implementation:\n",
    "- Each thread reads N elements from matrix A and N elements from matrix B from global memory\n",
    "- For an N×N matrix multiplication, this results in 2×N×N×N global memory accesses\n",
    "- Global memory accesses are slow and become a bottleneck for performance\n",
    "- There is no data reuse between threads in the same block\n",
    "\n",
    "Let's examine the memory access pattern more closely:\n",
    "\n",
    "1. **Matrix A Access**:\n",
    "   - Threads in the same row access consecutive elements in the same row of A\n",
    "   - This is coalesced access, which is efficient\n",
    "\n",
    "2. **Matrix B Access**:\n",
    "   - Threads in the same column access elements from different rows in B\n",
    "   - This is strided access (column-wise), which is inefficient\n",
    "   - Each memory transaction fetches more data than needed, wasting bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Matrix Multiplication With Tiling\n",
    "\n",
    "Now let's implement the tiled version, which uses shared memory to reduce global memory accesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "// Matrix multiplication kernel - with tiling\n",
    "#define TILE_SIZE 16\n",
    "\n",
    "__global__ void matrixMulTiled(float* A, float* B, float* C, int N) {\n",
    "    // Shared memory for the tiles of A and B\n",
    "    __shared__ float sharedA[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float sharedB[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    // Calculate row and column indices for this thread\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    // Loop over all tiles needed to compute the element\n",
    "    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; t++) {\n",
    "        // Load the tiles from global memory to shared memory\n",
    "        if (row < N && t * TILE_SIZE + threadIdx.x < N) {\n",
    "            sharedA[threadIdx.y][threadIdx.x] = A[row * N + t * TILE_SIZE + threadIdx.x];\n",
    "        } else {\n",
    "            sharedA[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        if (col < N && t * TILE_SIZE + threadIdx.y < N) {\n",
    "            sharedB[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n",
    "        } else {\n",
    "            sharedB[threadIdx.y][threadIdx.x] = 0.0f;\n",
    "        }\n",
    "        \n",
    "        // Synchronize to make sure the tiles are loaded\n",
    "        __syncthreads();\n",
    "        \n",
    "        // Perform computation using the tiles\n",
    "        for (int k = 0; k < TILE_SIZE; k++) {\n",
    "            sum += sharedA[threadIdx.y][k] * sharedB[k][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        // Synchronize to make sure the computation is done\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // Write the result to global memory\n",
    "    if (row < N && col < N) {\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Aspects of the Tiled Implementation\n",
    "\n",
    "1. **Shared Memory Allocation**\n",
    "   - We allocate shared memory tiles for parts of matrices A and B\n",
    "   - The tile size affects performance and should be tuned for specific GPUs\n",
    "\n",
    "2. **Tile Loading**\n",
    "   - Each thread loads one element of the tile\n",
    "   - The loading pattern is designed to achieve coalesced memory access\n",
    "   - Boundary checks ensure we don't access out-of-bounds memory\n",
    "\n",
    "3. **Thread Synchronization**\n",
    "   - `__syncthreads()` ensures all threads in a block have loaded their data before computation begins\n",
    "   - This barrier is essential for correctness\n",
    "\n",
    "4. **Computation**\n",
    "   - Each thread computes the dot product using the shared memory tiles\n",
    "   - Multiple tiles are processed to compute the final result\n",
    "   - Shared memory access is much faster than global memory\n",
    "\n",
    "5. **Memory Access Pattern**\n",
    "   - The tiled approach significantly reduces global memory accesses\n",
    "   - Each element in the tile is reused multiple times from fast shared memory\n",
    "   - Both A and B are loaded in a coalesced pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Host Code for Testing the Kernels\n",
    "\n",
    "Let's implement host code to test and compare both kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Kernel definitions from above would go here\n",
    "\n",
    "// Utility function to check for CUDA errors\n",
    "#define CHECK_CUDA_ERROR(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA Error: %s at line %d\\n\", cudaGetErrorString(err), __LINE__); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "// Initialize a matrix with random values\n",
    "void initMatrix(float* matrix, int size) {\n",
    "    for (int i = 0; i < size * size; i++) {\n",
    "        matrix[i] = rand() / (float)RAND_MAX;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Main function\n",
    "int main() {\n",
    "    int N = 1024;  // Matrix size (N x N)\n",
    "    size_t bytes = N * N * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float* h_A = (float*)malloc(bytes);\n",
    "    float* h_B = (float*)malloc(bytes);\n",
    "    float* h_C_naive = (float*)malloc(bytes);\n",
    "    float* h_C_tiled = (float*)malloc(bytes);\n",
    "    \n",
    "    // Initialize input matrices\n",
    "    initMatrix(h_A, N);\n",
    "    initMatrix(h_B, N);\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float* d_A, *d_B, *d_C;\n",
    "    CHECK_CUDA_ERROR(cudaMalloc(&d_A, bytes));\n",
    "    CHECK_CUDA_ERROR(cudaMalloc(&d_B, bytes));\n",
    "    CHECK_CUDA_ERROR(cudaMalloc(&d_C, bytes));\n",
    "    \n",
    "    // Copy data from host to device\n",
    "    CHECK_CUDA_ERROR(cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice));\n",
    "    CHECK_CUDA_ERROR(cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Define grid and block dimensions\n",
    "    dim3 blockDim(16, 16);\n",
    "    dim3 gridDim((N + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n",
    "    \n",
    "    // Create CUDA events for timing\n",
    "    cudaEvent_t start, stop;\n",
    "    CHECK_CUDA_ERROR(cudaEventCreate(&start));\n",
    "    CHECK_CUDA_ERROR(cudaEventCreate(&stop));\n",
    "    float milliseconds = 0;\n",
    "    \n",
    "    // Run naive kernel\n",
    "    CHECK_CUDA_ERROR(cudaEventRecord(start));\n",
    "    matrixMulNaive<<<gridDim, blockDim>>>(d_A, d_B, d_C, N);\n",
    "    CHECK_CUDA_ERROR(cudaEventRecord(stop));\n",
    "    CHECK_CUDA_ERROR(cudaEventSynchronize(stop));\n",
    "    CHECK_CUDA_ERROR(cudaEventElapsedTime(&milliseconds, start, stop));\n",
    "    printf(\"Naive kernel execution time: %.3f ms\\n\", milliseconds);\n",
    "    \n",
    "    // Copy result back to host\n",
    "    CHECK_CUDA_ERROR(cudaMemcpy(h_C_naive, d_C, bytes, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Run tiled kernel\n",
    "    CHECK_CUDA_ERROR(cudaEventRecord(start));\n",
    "    matrixMulTiled<<<gridDim, blockDim>>>(d_A, d_B, d_C, N);\n",
    "    CHECK_CUDA_ERROR(cudaEventRecord(stop));\n",
    "    CHECK_CUDA_ERROR(cudaEventSynchronize(stop));\n",
    "    CHECK_CUDA_ERROR(cudaEventElapsedTime(&milliseconds, start, stop));\n",
    "    printf(\"Tiled kernel execution time: %.3f ms\\n\", milliseconds);\n",
    "    \n",
    "    // Copy result back to host\n",
    "    CHECK_CUDA_ERROR(cudaMemcpy(h_C_tiled, d_C, bytes, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Verify results (simplified check)\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N * N; i++) {\n",
    "        maxError = max(maxError, abs(h_C_naive[i] - h_C_tiled[i]));\n",
    "    }\n",
    "    printf(\"Max error between naive and tiled implementations: %f\\n\", maxError);\n",
    "    \n",
    "    // Free device memory\n",
    "    CHECK_CUDA_ERROR(cudaFree(d_A));\n",
    "    CHECK_CUDA_ERROR(cudaFree(d_B));\n",
    "    CHECK_CUDA_ERROR(cudaFree(d_C));\n",
    "    \n",
    "    // Free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C_naive);\n",
    "    free(h_C_tiled);\n",
    "    \n",
    "    // Destroy CUDA events\n",
    "    CHECK_CUDA_ERROR(cudaEventDestroy(start));\n",
    "    CHECK_CUDA_ERROR(cudaEventDestroy(stop));\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Performance Benefits of Tiling\n",
    "\n",
    "Let's analyze the theoretical performance improvement with tiling:\n",
    "\n",
    "### Global Memory Access Analysis\n",
    "\n",
    "**Without Tiling:**\n",
    "- Each thread reads N elements from A and N elements from B\n",
    "- For N×N threads, that's 2×N³ global memory accesses\n",
    "\n",
    "**With Tiling (using TILE_SIZE×TILE_SIZE tiles):**\n",
    "- Each block of TILE_SIZE×TILE_SIZE threads reads N/TILE_SIZE tiles\n",
    "- Each tile requires 2×TILE_SIZE² global memory accesses\n",
    "- Total global memory accesses: 2×N²×N/TILE_SIZE = 2×N³/TILE_SIZE\n",
    "\n",
    "**Theoretical Speedup:**\n",
    "- Ratio of global memory accesses: (2×N³) / (2×N³/TILE_SIZE) = TILE_SIZE\n",
    "- With TILE_SIZE = 16, we expect up to 16× fewer global memory accesses\n",
    "\n",
    "In practice, the speedup will be limited by other factors such as memory bandwidth, latency, thread synchronization overhead, and hardware occupancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Profiling with NVIDIA Nsight Compute\n",
    "\n",
    "NVIDIA Nsight Compute is a powerful profiling tool for analyzing CUDA kernel performance. Here's how to use it to profile and analyze our matrix multiplication kernels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Profiling Commands\n",
    "\n",
    "```bash\n",
    "# Compile with line information for source-level profiling\n",
    "nvcc -o matrix_multiply matrix_multiply.cu --lineinfo\n",
    "\n",
    "# Basic profiling\n",
    "ncu ./matrix_multiply\n",
    "\n",
    "# Save profiling results to a file\n",
    "ncu -o profile_results ./matrix_multiply\n",
    "\n",
    "# Comprehensive profiling with all metrics\n",
    "ncu --set full ./matrix_multiply\n",
    "\n",
    "# Profile specific kernel by name\n",
    "ncu --kernel-name matrixMulTiled ./matrix_multiply\n",
    "\n",
    "# Profile with specific metrics/sections\n",
    "ncu --section SpeedOfLight ./matrix_multiply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling Process Step by Step\n",
    "\n",
    "1. **Compile your code with debug information**\n",
    "   - Use the `--lineinfo` flag with nvcc to include source information\n",
    "   - This helps correlate performance metrics with specific lines of code\n",
    "\n",
    "2. **Run basic profiling**\n",
    "   - Start with `ncu ./your_application` to get an overview\n",
    "   - This provides a summary of kernel launches and basic metrics\n",
    "\n",
    "3. **Focus on specific kernels**\n",
    "   - Use `--kernel-name` to profile just the kernels you're interested in\n",
    "   - For multiple kernel launches, use `--kernel-id :::1` to profile only one instance\n",
    "\n",
    "4. **Collect specific metrics**\n",
    "   - Use built-in metric sets like `--set full` or `--section SpeedOfLight`\n",
    "   - For targeted analysis, specify individual metrics\n",
    "\n",
    "5. **Analyze results**\n",
    "   - Use the Nsight Compute UI (`ncu-ui`) to visualize and analyze results\n",
    "   - Compare different kernel implementations side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Metrics to Analyze\n",
    "\n",
    "When profiling our matrix multiplication kernels, pay attention to these key metrics:\n",
    "\n",
    "1. **Memory-related metrics:**\n",
    "   - Global Memory Load/Store Throughput\n",
    "   - Shared Memory Load/Store Throughput\n",
    "   - L1/L2 Cache Hit Rate\n",
    "   - Memory Bandwidth Utilization\n",
    "\n",
    "2. **Compute-related metrics:**\n",
    "   - SM (Streaming Multiprocessor) Utilization\n",
    "   - Warp Execution Efficiency\n",
    "   - Achieved Occupancy\n",
    "   - Instructions Per Cycle (IPC)\n",
    "\n",
    "3. **Limiting factors:**\n",
    "   - Compute Bound vs Memory Bound\n",
    "   - Stall Reasons (Memory, Execution, Synchronization)\n",
    "   - Memory Dependency\n",
    "   - Execution Dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Profiling Output\n",
    "\n",
    "Here's an example of what profiling output might look like for our kernels:\n",
    "\n",
    "```\n",
    "==== Naive Matrix Multiplication Kernel ====\n",
    "Memory Throughput: 80 GB/s (30% of peak)\n",
    "Compute Throughput: 600 GFLOPS (20% of peak)\n",
    "Primary Bottleneck: Memory Bandwidth\n",
    "Warp Stall Reasons:\n",
    "  - Memory Dependency: 70%\n",
    "  - Execution Dependency: 20%\n",
    "  - Other: 10%\n",
    "\n",
    "==== Tiled Matrix Multiplication Kernel ====\n",
    "Memory Throughput: 30 GB/s (11% of peak)\n",
    "Compute Throughput: 1800 GFLOPS (60% of peak)\n",
    "Primary Bottleneck: Compute\n",
    "Warp Stall Reasons:\n",
    "  - Execution Dependency: 60%\n",
    "  - Memory Dependency: 25%\n",
    "  - Synchronization: 10%\n",
    "  - Other: 5%\n",
    "```\n",
    "\n",
    "This example illustrates the shift from memory-bound to compute-bound execution when using tiling, which is a key indicator of successful memory optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding Profiling Results\n",
    "\n",
    "When analyzing profiling results, look for these patterns:\n",
    "\n",
    "1. **Memory-bound kernels:**\n",
    "   - High percentage of memory-related stalls\n",
    "   - Low arithmetic intensity (few computations per memory access)\n",
    "   - Low IPC (Instructions Per Cycle)\n",
    "   - Optimizations should focus on reducing memory accesses\n",
    "\n",
    "2. **Compute-bound kernels:**\n",
    "   - High percentage of execution-related stalls\n",
    "   - High arithmetic intensity\n",
    "   - Higher IPC\n",
    "   - Optimizations should focus on improving instruction-level parallelism\n",
    "\n",
    "3. **Synchronization-bound kernels:**\n",
    "   - High percentage of synchronization stalls\n",
    "   - Many `__syncthreads()` calls\n",
    "   - Optimizations should focus on reducing synchronization points\n",
    "\n",
    "The naive matrix multiplication is typically memory-bound, while the tiled version may become compute-bound, depending on the tile size and hardware capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optimization Strategies Beyond Basic Tiling\n",
    "\n",
    "While basic tiling provides significant performance improvements, there are several advanced techniques to further optimize our kernel:\n",
    "\n",
    "1. **Prefetching:**\n",
    "   - Load the next tile while computing on the current tile\n",
    "   - Overlaps memory transfers with computation\n",
    "   - Can reduce memory stalls\n",
    "\n",
    "2. **Loop Unrolling:**\n",
    "   - Explicitly unroll the inner computation loop\n",
    "   - Reduces loop overhead and allows for better instruction-level parallelism\n",
    "   - Example:\n",
    "     ```c\n",
    "     #pragma unroll\n",
    "     for (int k = 0; k < TILE_SIZE; k++) {\n",
    "         sum += sharedA[threadIdx.y][k] * sharedB[k][threadIdx.x];\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **Register Blocking/Thread Coarsening:**\n",
    "   - Have each thread compute multiple output elements\n",
    "   - Reduces shared memory accesses and synchronization points\n",
    "   - Increases register pressure but can improve performance\n",
    "\n",
    "4. **Vectorized Memory Access:**\n",
    "   - Use vector types (float4, etc.) for coalesced memory access\n",
    "   - Can significantly improve memory throughput\n",
    "   - Especially beneficial for older GPU architectures\n",
    "\n",
    "5. **Bank Conflict Avoidance:**\n",
    "   - Pad shared memory to avoid bank conflicts\n",
    "   - Example: `__shared__ float sharedA[TILE_SIZE][TILE_SIZE+1];`\n",
    "   - Especially important for matrix operations with strided access patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise: Experiment with Tile Sizes\n",
    "\n",
    "As an exercise, you can modify the code to experiment with different tile sizes and observe their impact on performance.\n",
    "\n",
    "Try changing the TILE_SIZE value in the code above to 8, 16, 32, or 64, and compare the execution times.\n",
    "\n",
    "Questions to consider:\n",
    "1. How does the tile size affect performance?\n",
    "2. Why might very small or very large tile sizes perform poorly?\n",
    "3. How do the optimal tile sizes relate to the hardware specifications of your GPU (warp size, shared memory size, etc.)?\n",
    "\n",
    "For example, you could modify the code to look like this:\n",
    "\n",
    "```c\n",
    "// Try different tile sizes\n",
    "#define TILE_SIZE_8 8\n",
    "#define TILE_SIZE_16 16\n",
    "#define TILE_SIZE_32 32\n",
    "#define TILE_SIZE_64 64\n",
    "\n",
    "// Create four different kernel functions with different tile sizes\n",
    "// ...\n",
    "\n",
    "// Then time each one and compare the results\n",
    "// ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Common Pitfalls and Debugging Tips\n",
    "\n",
    "When implementing tiled CUDA kernels, watch out for these common issues:\n",
    "\n",
    "1. **Incorrect Synchronization**\n",
    "   - Missing `__syncthreads()` can lead to race conditions\n",
    "   - Placing them inside conditional blocks can cause deadlocks\n",
    "\n",
    "2. **Out-of-Bounds Memory Access**\n",
    "   - Always check array boundaries, especially for non-square matrices or tile sizes that don't evenly divide the matrix dimensions\n",
    "   - Use proper padding or boundary checks\n",
    "\n",
    "3. **Bank Conflicts**\n",
    "   - Shared memory is organized into banks, and simultaneous access to the same bank by different threads in a warp causes serialization\n",
    "   - Use padding to avoid bank conflicts\n",
    "\n",
    "4. **Occupancy Issues**\n",
    "   - Using too much shared memory or registers per thread can reduce occupancy\n",
    "   - Use the occupancy calculator or NVIDIA Nsight Compute to analyze occupancy\n",
    "\n",
    "5. **Debugging Tips**\n",
    "   - Start with small matrices to verify correctness\n",
    "   - Use `printf` in your kernel for debugging (only in small test cases)\n",
    "   - Use CUDA error checking macros\n",
    "   - Profile and optimize one step at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we've explored the concept of tiling as a fundamental optimization technique for GPU programming. Key takeaways include:\n",
    "\n",
    "1. **Memory hierarchy awareness** is crucial for GPU performance optimization\n",
    "2. **Tiling** reduces global memory accesses by leveraging faster shared memory\n",
    "3. **Thread synchronization** is essential for correctness in tiled implementations\n",
    "4. **Performance profiling** helps identify bottlenecks and guide optimizations\n",
    "5. **Advanced optimization techniques** can further improve performance beyond basic tiling\n",
    "\n",
    "By understanding and applying these concepts, you can significantly improve the performance of your CUDA kernels, especially for memory-bound applications like matrix multiplication.\n",
    "\n",
    "Tiling is just one of many optimization techniques in the GPU programmer's toolkit, but it's one of the most fundamental and widely applicable approaches for improving memory performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. NVIDIA CUDA Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/\n",
    "2. NVIDIA Nsight Compute Documentation: https://docs.nvidia.com/nsight-compute/\n",
    "3. CUDA Best Practices Guide: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/\n",
    "4. Matrix-Matrix Multiplication on the GPU: https://www.quantstart.com/articles/Matrix-Matrix-Multiplication-on-the-GPU-with-Nvidia-CUDA/\n",
    "5. How to Optimize a CUDA Matmul Kernel: https://siboehm.com/articles/22/CUDA-MMM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}