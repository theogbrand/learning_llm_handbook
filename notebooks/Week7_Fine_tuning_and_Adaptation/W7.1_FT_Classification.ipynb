{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning for Text Classification: Spam Detection\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained language model for text classification tasks, specifically focusing on spam detection. By the end of this tutorial, you'll understand the key concepts and implementation steps involved in adapting large language models for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Fine-tuning a pre-trained language model for text classification involves the following main steps:\n",
    "\n",
    "1. **Data preparation**: Creating a balanced, tokenized dataset\n",
    "2. **Model adaptation**: Modifying a pre-trained model for classification tasks\n",
    "3. **Selective fine-tuning**: Choosing which model parameters to train\n",
    "4. **Training and evaluation**: Fine-tuning the model and measuring its performance\n",
    "5. **Inference**: Using the fine-tuned model to classify new texts\n",
    "\n",
    "Let's implement these steps for a spam classification example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tiktoken  # OpenAI's tokenizer\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download and Prepare the Dataset\n",
    "\n",
    "We'll use the SMS Spam Collection dataset, which contains text messages labeled as spam or ham (not spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the dataset\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Downloading the file\n",
    "    print(f\"Downloading dataset from {url}...\")\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "# Set up paths\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# Create the extraction directory if it doesn't exist\n",
    "os.makedirs(extracted_path, exist_ok=True)\n",
    "\n",
    "# Download and extract data\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    backup_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(backup_url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "\n",
    "# Display the first few samples\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "class_distribution = df[\"Label\"].value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a balanced dataset\n",
    "\n",
    "The dataset is imbalanced, with many more non-spam (ham) messages than spam messages. For optimal training, we'll create a balanced dataset by randomly sampling an equal number of ham messages to match the number of spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    # Shuffle the balanced dataset\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string labels to numeric values (0 for ham, 1 for spam)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "# Split data: 70% training, 10% validation, 20% test\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "# Save the splits to CSV files for convenience\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} examples\")\n",
    "print(f\"Validation set: {len(validation_df)} examples\")\n",
    "print(f\"Test set: {len(test_df)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenize and prepare the data\n",
    "\n",
    "Now we need to tokenize the text data and prepare it for the model. Since text messages have different lengths, we'll need to pad shorter messages or truncate longer ones to a consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer (using the GPT-2 tokenizer from tiktoken)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Verify the padding token ID (typically <|endoftext|>)\n",
    "pad_token_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "print(f\"Padding token ID: {pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch Dataset for the spam classification task\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        # Find the maximum length if not provided\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        return max(len(encoded_text) for encoded_text in self.encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for training, validation, and testing\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=None  # Will find the longest sequence in training data\n",
    ")\n",
    "\n",
    "print(f\"Maximum sequence length in training data: {train_dataset.max_length}\")\n",
    "\n",
    "# Use the same max_length for validation and test for consistency\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training, validation, and testing\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,  # Drop the last incomplete batch to ensure all batches have the same size\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up the pre-trained model for classification\n",
    "\n",
    "We'll use a pre-trained GPT-style model and adapt it for text classification. In a real-world scenario, you would load an existing pre-trained model like GPT-2. For this tutorial, we'll create a simplified version that mimics the architecture of GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Create query, key, value projections\n",
    "        self.W_query = nn.Linear(d_model, d_model)\n",
    "        self.W_key = nn.Linear(d_model, d_model)\n",
    "        self.W_value = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        q = self.W_query(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.W_key(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.W_value(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply causal mask (for auto-regressive behavior)\n",
    "        seq_length = scores.size(-1)\n",
    "        causal_mask = torch.triu(torch.ones(seq_length, seq_length, device=x.device) * float('-inf'), diagonal=1)\n",
    "        scores = scores + causal_mask\n",
    "        \n",
    "        # Apply padding mask if provided\n",
    "        if mask is not None:\n",
    "            # Expand mask to match attention scores dimensions\n",
    "            expanded_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(expanded_mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax and get weighted values\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        # Reshape and project back to original dimensions\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out_proj(context)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention block with residual connection and layer normalization\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-forward block with residual connection and layer normalization\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size=50257, d_model=768, n_layers=12, n_heads=12, \n",
    "                 d_ff=3072, max_seq_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "        self.drop_emb = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.trf_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output head (initially for next token prediction, will be replaced for classification)\n",
    "        self.out_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            \n",
    "    def forward(self, idx):\n",
    "        # Get sequence length and create position indices\n",
    "        seq_len = idx.size(1)\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        \n",
    "        # Get token and position embeddings and add them\n",
    "        tok_embeddings = self.tok_emb(idx)\n",
    "        pos_embeddings = self.pos_emb(pos)\n",
    "        x = self.drop_emb(tok_embeddings + pos_embeddings)\n",
    "        \n",
    "        # Create attention mask (1 for tokens, 0 for padding)\n",
    "        mask = (idx != 50256).to(idx.device)  # Use pad_token_id (50256 for GPT-2)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, mask)\n",
    "            \n",
    "        # Apply final layer norm\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Apply output head\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# In a real-world scenario, you would load pre-trained weights here\n",
    "model = GPTModel(\n",
    "    vocab_size=50257,     # GPT-2 vocabulary size\n",
    "    d_model=768,          # Embedding dimension\n",
    "    n_layers=12,          # Number of transformer layers \n",
    "    n_heads=12,           # Number of attention heads\n",
    "    d_ff=3072,            # Feed-forward dimension\n",
    "    max_seq_len=train_dataset.max_length,  # Max sequence length from our dataset\n",
    "    dropout=0.1           # Dropout rate\n",
    ")\n",
    "\n",
    "# In this tutorial, we're not loading actual pre-trained weights for simplicity.\n",
    "# In practice, you would load weights from a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt the model for text classification\n",
    "\n",
    "Now we'll modify the model for our binary classification task (spam vs. not spam). We'll:\n",
    "1. Freeze most of the model parameters\n",
    "2. Replace the output layer with a binary classification head\n",
    "3. Unfreeze the last transformer block and final layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the output layer with a binary classification head\n",
    "num_classes = 2  # Binary classification: spam or not spam\n",
    "model.out_head = nn.Linear(in_features=768, out_features=num_classes)\n",
    "\n",
    "# Unfreeze the last transformer block and final layer norm for fine-tuning\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Count trainable parameters vs. total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model output format\n",
    "\n",
    "Let's test our model with a small batch of data to make sure the output format is correct for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Get a sample batch from the training data loader\n",
    "sample_inputs, sample_labels = next(iter(train_loader))\n",
    "sample_inputs = sample_inputs.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(sample_inputs)\n",
    "\n",
    "print(f\"Input shape: {sample_inputs.shape}\")  # [batch_size, sequence_length]\n",
    "print(f\"Output shape: {outputs.shape}\")       # [batch_size, sequence_length, num_classes]\n",
    "\n",
    "# For classification, we typically use the output corresponding to the last token\n",
    "last_token_output = outputs[:, -1, :]\n",
    "print(f\"Last token output shape: {last_token_output.shape}\")  # [batch_size, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Loss and Evaluation Functions\n",
    "\n",
    "We'll need functions to calculate loss and evaluate the model's accuracy on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"Calculate loss for a single batch\"\"\"\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = F.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"Calculate average loss over multiple batches\"\"\"\n",
    "    total_loss = 0.\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"Calculate classification accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i < num_batches:\n",
    "                input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "                predicted_labels = torch.argmax(logits, dim=1)\n",
    "                num_examples += predicted_labels.shape[0]\n",
    "                correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    return correct_predictions / num_examples\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    \"\"\"Evaluate model on training and validation sets\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate initial (pre-training) accuracy\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Initial training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Initial validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Initial test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial accuracy should be around 50% (random chance for a balanced binary dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Fine-tune the Model\n",
    "\n",
    "Now we'll fine-tune our model on the spam classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                     eval_freq, eval_iter):\n",
    "    \"\"\"Train the classifier model\"\"\"\n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        # Training phase\n",
    "        for input_batch, target_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate gradients\n",
    "            optimizer.step()  # Update weights\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "            # Evaluate periodically\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Step {global_step:06d}: Train loss {train_loss:.4f}, Val loss {val_loss:.4f}\")\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train accuracy: {train_accuracy*100:.2f}%\")\n",
    "        print(f\"  Val accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'examples_seen': examples_seen\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.1\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),  # Only optimize trainable parameters\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "training_results = train_classifier(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model\n",
    "\n",
    "Let's visualize our training progress and evaluate the model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_results['train_losses'], label='Training Loss')\n",
    "plt.plot(training_results['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Evaluation Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_results['train_accs'], label='Training Accuracy')\n",
    "plt.plot(training_results['val_accs'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the full datasets\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Final training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Final validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Final test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report and confusion matrix for the test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def get_predictions(data_loader, model, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_batch, target_batch in data_loader:\n",
    "            input_batch = input_batch.to(device)\n",
    "            logits = model(input_batch)[:, -1, :]\n",
    "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(target_batch.numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "# Get predictions for the test set\n",
    "y_pred, y_true = get_predictions(test_loader, model, device)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Ham', 'Spam']))\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Ham', 'Spam'])\n",
    "plt.yticks(tick_marks, ['Ham', 'Spam'])\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Use the Fine-tuned Model for Inference\n",
    "\n",
    "Now let's use our fine-tuned model to classify some example messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_message(text, model, tokenizer, device, max_length):\n",
    "    \"\"\"Classify a message as spam or ham\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text)[:max_length]\n",
    "    tokens = tokens + [50256] * (max_length - len(tokens))  # Pad to max_length\n",
    "    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    # Translate prediction to label\n",
    "    label = \"spam\" if prediction == 1 else \"ham\"\n",
    "    confidence = probs[0][prediction].item()\n",
    "    \n",
    "    return label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on some example messages\n",
    "examples = [\n",
    "    \"Hey, are we still on for dinner tonight?\",\n",
    "    \"URGENT! You have won a $1000 gift card. Click here to claim your prize: http://bit.ly/claim\",\n",
    "    \"Remember to pick up milk on your way home.\",\n",
    "    \"Congratulations! You've been selected for a FREE iPhone 14. Just pay shipping and handling.\",\n",
    "    \"The meeting has been moved to 3pm tomorrow.\",\n",
    "    \"FINAL NOTICE: Your car warranty is about to expire. Call now to extend!\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    label, confidence = classify_message(\n",
    "        text, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "    )\n",
    "    print(f\"Message: {text}\")\n",
    "    print(f\"Classification: {label.upper()} (confidence: {confidence:.4f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save and Load the Model\n",
    "\n",
    "Finally, let's save our fine-tuned model so we can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = \"spam_classifier_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'max_length': train_dataset.max_length,\n",
    "    'vocab_size': 50257,\n",
    "    'd_model': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'd_ff': 3072\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the saved model\n",
    "def load_spam_classifier(model_path, device):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Create a new model with the same architecture\n",
    "    model = GPTModel(\n",
    "        vocab_size=checkpoint['vocab_size'],\n",
    "        d_model=checkpoint['d_model'],\n",
    "        n_heads=checkpoint['n_heads'],\n",
    "        n_layers=checkpoint['n_layers'],\n",
    "        d_ff=checkpoint['d_ff'],\n",
    "        max_seq_len=checkpoint['max_length']\n",
    "    )\n",
    "    \n",
    "    # Replace the output head for binary classification\n",
    "    model.out_head = nn.Linear(in_features=checkpoint['d_model'], out_features=2)\n",
    "    \n",
    "    # Load the saved state dict\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['max_length']\n",
    "\n",
    "# Load the saved model and test it\n",
    "loaded_model, max_length = load_spam_classifier(save_path, device)\n",
    "\n",
    "# Test the loaded model on an example\n",
    "example = \"URGENT: Your account has been compromised. Call this number to verify your identity.\"\n",
    "label, confidence = classify_message(example, loaded_model, tokenizer, device, max_length)\n",
    "\n",
    "print(f\"Message: {example}\")\n",
    "print(f\"Classification: {label.upper()} (confidence: {confidence:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've demonstrated how to fine-tune a pre-trained language model for text classification using spam detection as an example. We covered:\n",
    "\n",
    "1. **Data preparation**: Loading, balancing, splitting, and tokenizing the dataset\n",
    "2. **Model adaptation**: Modifying a pre-trained model for classification by replacing the output layer\n",
    "3. **Selective fine-tuning**: Freezing most parameters while only training the last block and classification head\n",
    "4. **Training and evaluation**: Fine-tuning the model and measuring its performance\n",
    "5. **Inference**: Using the fine-tuned model to classify new text messages\n",
    "\n",
    "Key insights from this tutorial:\n",
    "\n",
    "- **Transfer learning**: We leveraged the knowledge in a pre-trained language model instead of training from scratch, which is much more efficient.\n",
    "- **Selective fine-tuning**: By only fine-tuning a small portion of the model, we preserved its general language understanding while adapting it to our specific task.\n",
    "- **Causal attention**: We used the last token's representation for classification because it has access to the entire input sequence due to the causal attention mechanism.\n",
    "- **Classification head**: We replaced the language modeling head with a simple binary classification head.\n",
    "\n",
    "This approach can be extended to other text classification tasks by adapting the dataset and possibly the model architecture. For more complex tasks, you might want to fine-tune more layers or use larger pre-trained models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}