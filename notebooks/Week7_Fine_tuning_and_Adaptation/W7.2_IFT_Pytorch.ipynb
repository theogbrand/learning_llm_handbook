{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5.2: Instruction Fine-Tuning with PyTorch\n",
    "\n",
    "**Resource Required**: GPU with at least 16GB VRAM (24GB recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "* Understand the concept of instruction fine-tuning (IFT)\n",
    "* Learn how to prepare data for instruction fine-tuning\n",
    "* Implement a pure PyTorch training loop for fine-tuning a pre-trained language model\n",
    "* Monitor training metrics and evaluate the fine-tuned model\n",
    "\n",
    "ðŸ’¡ **NOTE**: In this notebook, we'll implement instruction fine-tuning using pure PyTorch. Later in the course, we'll demonstrate how to do the same with HuggingFace libraries, which can simplify the process significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: What is Instruction Fine-Tuning?\n",
    "\n",
    "Instruction Fine-Tuning (IFT) is a critical technique that transforms a base language model into a model that can follow human instructions. This process is what enables models like ChatGPT, Claude, and Gemini to respond helpfully to user queries.\n",
    "\n",
    "### Key concepts:\n",
    "\n",
    "1. **Base Models vs. Instruction-Tuned Models**:\n",
    "   - **Base models** are pre-trained on a large corpus of text, but only learn to predict the next token in a sequence\n",
    "   - **Instruction-tuned models** are specifically trained to understand and follow instructions\n",
    "\n",
    "2. **Instruction Tuning Process**:\n",
    "   - Start with a pre-trained language model\n",
    "   - Fine-tune using a dataset of instruction-response pairs\n",
    "   - Train the model to generate helpful responses to instructions\n",
    "\n",
    "3. **Benefits**:\n",
    "   - Makes language models more helpful, honest, and harmless\n",
    "   - Enables task-specific capabilities without massive parameter updates\n",
    "   - Reduces the likelihood of generating harmful or nonsensical content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "Let's install the necessary packages for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers==4.40.1 torch==2.2.0 tqdm==4.66.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    default_data_collator,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if CUDA is available and set the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the Instruction Dataset\n",
    "\n",
    "We'll use the Alpaca dataset, which contains instructions, inputs, and outputs generated using GPT-4. This dataset is specifically designed for instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is already downloaded at the following path\n",
    "dataset_file = \"data/alpaca_gpt4_data.json\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(dataset_file, \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataset structure\n",
    "print(f\"Total examples: {len(alpaca)}\")\n",
    "print(\"\\nExample data point:\")\n",
    "print(json.dumps(alpaca[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset into Train and Evaluation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "random.seed(SEED)\n",
    "random.shuffle(alpaca)\n",
    "\n",
    "# Split into train and eval\n",
    "train_dataset = alpaca[:-1000]  # Use all but the last 1000 examples for training\n",
    "eval_dataset = alpaca[-1000:]   # Use the last 1000 examples for evaluation\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the Prompts for Instruction Fine-Tuning\n",
    "\n",
    "We need to format our data in a specific way for instruction tuning. We'll create prompt templates for examples with and without additional input context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_no_input(row):\n",
    "    \"\"\"Format a prompt for examples without additional input.\"\"\"\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "def prompt_with_input(row):\n",
    "    \"\"\"Format a prompt for examples with additional input context.\"\"\"\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "def create_alpaca_prompt(row):\n",
    "    \"\"\"Create a prompt based on whether the example has input or not.\"\"\"\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_with_input(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a prompt without input\n",
    "example_no_input = alpaca[0]  # This example doesn't have input\n",
    "print(\"Example prompt without input:\")\n",
    "print(prompt_no_input(example_no_input))\n",
    "\n",
    "# Find an example with input\n",
    "example_with_input = None\n",
    "for example in alpaca:\n",
    "    if example[\"input\"] != \"\":\n",
    "        example_with_input = example\n",
    "        break\n",
    "\n",
    "if example_with_input:\n",
    "    print(\"\\nExample prompt with input:\")\n",
    "    print(prompt_with_input(example_with_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataset for Training\n",
    "\n",
    "We need to:\n",
    "1. Format all examples with our prompt templates\n",
    "2. Append EOS token to all outputs\n",
    "3. Combine prompts and outputs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompts for all examples\n",
    "train_prompts = [create_alpaca_prompt(row) for row in train_dataset]\n",
    "eval_prompts = [create_alpaca_prompt(row) for row in eval_dataset]\n",
    "\n",
    "# Helper function to add EOS token to outputs\n",
    "def pad_eos(dataset):\n",
    "    EOS_TOKEN = \"</s>\"  # End of sequence token for LLaMA models\n",
    "    return [f\"{row['output']}{EOS_TOKEN}\" for row in dataset]\n",
    "\n",
    "# Add EOS token to outputs\n",
    "train_outputs = pad_eos(train_dataset)\n",
    "eval_outputs = pad_eos(eval_dataset)\n",
    "\n",
    "# Combine prompts and outputs for training\n",
    "train_examples = [{\"prompt\": p, \"output\": o, \"combined\": p + o} \n",
    "                  for p, o in zip(train_prompts, train_outputs)]\n",
    "eval_examples = [{\"prompt\": p, \"output\": o, \"combined\": p + o} \n",
    "                 for p, o in zip(eval_prompts, eval_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example of the complete instruction-response pair\n",
    "print(\"Complete instruction-response example:\")\n",
    "print(train_examples[0][\"combined\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization and Dataset Preparation\n",
    "\n",
    "We'll now load the tokenizer and convert our text data into tokenized inputs for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model to use - small enough to run on most GPUs\n",
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Data Packing\n",
    "\n",
    "To train more efficiently, we'll pack multiple short examples into longer sequences. This increases training efficiency by reducing padding and allowing the model to learn from more examples per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence length\n",
    "max_sequence_len = 1024\n",
    "\n",
    "def pack_examples(dataset, max_seq_len=max_sequence_len):\n",
    "    \"\"\"Pack multiple examples into fixed-length sequences.\"\"\"\n",
    "    # Tokenize all examples\n",
    "    tokenized_inputs = tokenizer([ex[\"combined\"] for ex in dataset])[\"input_ids\"]\n",
    "    \n",
    "    # Concatenate all tokenized inputs\n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tokenized_inputs:\n",
    "        all_token_ids.extend(tokenized_input)\n",
    "    \n",
    "    print(f\"Total number of tokens: {len(all_token_ids)}\")\n",
    "    \n",
    "    # Pack tokens into fixed-length sequences\n",
    "    packed_dataset = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            # Create input_ids and labels (shifted by 1 for next-token prediction)\n",
    "            packed_dataset.append({\n",
    "                \"input_ids\": input_ids[:-1], \n",
    "                \"labels\": input_ids[1:]\n",
    "            })\n",
    "    \n",
    "    return packed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack the datasets\n",
    "train_ds_packed = pack_examples(train_examples)\n",
    "eval_ds_packed = pack_examples(eval_examples)\n",
    "\n",
    "print(f\"Number of packed training sequences: {len(train_ds_packed)}\")\n",
    "print(f\"Number of packed evaluation sequences: {len(eval_ds_packed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "DataLoaders provide batched data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # Adjust based on your GPU memory\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a batch to verify our data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a batch\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"Batch keys: {sample_batch.keys()}\")\n",
    "print(f\"Input shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {sample_batch['labels'].shape}\")\n",
    "\n",
    "# Decode the first example in the batch to see what it looks like\n",
    "print(\"\\nSample input text (first 250 chars):\")\n",
    "print(tokenizer.decode(sample_batch[\"input_ids\"][0])[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Setup and Training Configuration\n",
    "\n",
    "Now we'll load the pre-trained model and set up our training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training configuration\n",
    "config = SimpleNamespace(\n",
    "    model_id='meta-llama/Llama-2-7b-hf',\n",
    "    precision=\"bf16\",         # bf16 is faster and uses less memory than fp32\n",
    "    n_freeze=24,              # Number of layers to freeze (out of 32 for LLaMA-7B)\n",
    "    learning_rate=2e-4,\n",
    "    n_eval_samples=5,         # Number of samples to generate during evaluation\n",
    "    max_seq_len=max_sequence_len,\n",
    "    epochs=3,\n",
    "    gradient_accumulation_steps=2,  # Simulate larger batch sizes\n",
    "    batch_size=batch_size,\n",
    "    gradient_checkpointing=True,    # Save memory at the cost of speed\n",
    "    freeze_embeddings=True,         # Freeze the embedding layer\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps\n",
    "print(f\"Total training steps: {config.total_train_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=\"auto\",  # Automatically determine best device mapping\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    use_cache=False,  # Disable KV cache for training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count parameters\n",
    "def param_count(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters()) / 1_000_000\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000\n",
    "    print(f\"Total params: {total_params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Count parameters before freezing\n",
    "print(\"Parameter count before freezing:\")\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze Parts of the Model\n",
    "\n",
    "To reduce the computational and memory requirements, we'll freeze most of the model and only fine-tune the last few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Unfreeze the output layer (LM head)\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Unfreeze the last N transformer layers\n",
    "for param in model.model.layers[config.n_freeze:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Freeze embeddings to save memory\n",
    "if config.freeze_embeddings:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False)\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Count parameters after freezing\n",
    "print(\"Parameter count after freezing:\")\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Optimizer and Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=config.learning_rate, \n",
    "    betas=(0.9, 0.99), \n",
    "    eps=1e-5\n",
    ")\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_training_steps=config.total_train_steps,\n",
    "    num_warmup_steps=config.total_train_steps // 10,  # 10% of steps for warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def loss_fn(logits, labels):\n",
    "    \"\"\"Cross-entropy loss for next token prediction.\"\"\"\n",
    "    return torch.nn.functional.cross_entropy(logits.view(-1, logits.shape[-1]), labels.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation Utilities\n",
    "\n",
    "Let's create functions to evaluate our model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup generation configuration\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "gen_config.max_new_tokens = 256\n",
    "gen_config.temperature = 0.7\n",
    "gen_config.top_p = 0.9\n",
    "gen_config.do_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    \"\"\"Generate a response from the model for a given prompt.\"\"\"\n",
    "    # Tokenize the prompt\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].to(device)\n",
    "    \n",
    "    # Generate a response\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(\n",
    "            tokenized_prompt, \n",
    "            max_new_tokens=max_new_tokens, \n",
    "            generation_config=gen_config\n",
    "        )\n",
    "    \n",
    "    # Decode and return only the new tokens (not the prompt)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to move batches to the device\n",
    "def to_device(batch, device):\n",
    "    \"\"\"Move a batch of tensors to the specified device.\"\"\"\n",
    "    return {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple token-level accuracy metric\n",
    "class TokenAccuracy:\n",
    "    \"\"\"Track token-level prediction accuracy.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.correct = 0.0\n",
    "    \n",
    "    def update(self, logits, labels):\n",
    "        \"\"\"Update accuracy with predictions from a batch.\"\"\"\n",
    "        predictions = logits.argmax(dim=-1).view(-1).cpu()\n",
    "        labels = labels.view(-1).cpu()\n",
    "        \n",
    "        # Only consider non-padding tokens\n",
    "        mask = labels != -100\n",
    "        filtered_predictions = predictions[mask]\n",
    "        filtered_labels = labels[mask]\n",
    "        \n",
    "        correct = (filtered_predictions == filtered_labels).sum()\n",
    "        self.count += len(filtered_labels)\n",
    "        self.correct += correct\n",
    "        \n",
    "        # Return batch accuracy\n",
    "        return correct.item() / len(filtered_labels) if len(filtered_labels) > 0 else 0.0\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Compute the overall accuracy.\"\"\"\n",
    "        return self.correct / self.count if self.count > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device, num_samples=5):\n",
    "    \"\"\"Evaluate the model on the validation set and generate sample outputs.\"\"\"\n",
    "    model.eval()\n",
    "    eval_accuracy = TokenAccuracy()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Compute loss and accuracy on validation set\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        batch = to_device(batch, device)\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            outputs = model(**batch)\n",
    "            loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        eval_accuracy.update(outputs.logits, batch[\"labels\"])\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Generate sample outputs\n",
    "    samples = []\n",
    "    for i in range(min(num_samples, len(eval_dataset))):\n",
    "        prompt = eval_dataset[i][\"prompt\"]\n",
    "        target = eval_dataset[i][\"output\"]\n",
    "        generated = generate_response(prompt)\n",
    "        samples.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"target\": target,\n",
    "            \"generated\": generated\n",
    "        })\n",
    "    \n",
    "    # Return metrics and samples\n",
    "    metrics = {\n",
    "        \"eval_loss\": total_loss / num_batches,\n",
    "        \"eval_accuracy\": eval_accuracy.compute()\n",
    "    }\n",
    "    \n",
    "    # Set model back to training mode\n",
    "    model.train()\n",
    "    \n",
    "    return metrics, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model's initial generation capabilities before fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model's initial generation (before fine-tuning)\n",
    "test_prompt = eval_dataset[0][\"prompt\"]\n",
    "print(\"Test prompt:\")\n",
    "print(test_prompt)\n",
    "\n",
    "print(\"\\nGenerated response (before fine-tuning):\")\n",
    "test_response = generate_response(test_prompt)\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop Implementation\n",
    "\n",
    "Now we'll implement the PyTorch training loop for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, config, device):\n",
    "    \"\"\"Train the model using a PyTorch training loop.\"\"\"\n",
    "    # Initialize metrics tracking\n",
    "    accuracy_tracker = TokenAccuracy()\n",
    "    best_eval_loss = float('inf')\n",
    "    train_step = 0\n",
    "    \n",
    "    # Dictionary to store metrics\n",
    "    metrics_history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"eval_loss\": [],\n",
    "        \"eval_accuracy\": [],\n",
    "        \"learning_rate\": []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(config.epochs):\n",
    "        # Training phase\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Process batches\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Training epoch {epoch+1}\")):\n",
    "            # Move batch to device\n",
    "            batch = to_device(batch, device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                outputs = model(**batch)\n",
    "                loss = loss_fn(outputs.logits, batch[\"labels\"]) / config.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights after accumulating gradients\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Update metrics\n",
    "                batch_loss = loss.item() * config.gradient_accumulation_steps\n",
    "                epoch_loss += batch_loss\n",
    "                batch_accuracy = accuracy_tracker.update(outputs.logits, batch[\"labels\"])\n",
    "                \n",
    "                # Print metrics\n",
    "                if train_step % 50 == 0:  # Print every 50 steps\n",
    "                    print(f\"Step {train_step}: Loss = {batch_loss:.4f}, Accuracy = {batch_accuracy:.4f}, LR = {scheduler.get_last_lr()[0]:.8f}\")\n",
    "                \n",
    "                # Store metrics\n",
    "                metrics_history[\"train_loss\"].append(batch_loss)\n",
    "                metrics_history[\"train_accuracy\"].append(batch_accuracy)\n",
    "                metrics_history[\"learning_rate\"].append(scheduler.get_last_lr()[0])\n",
    "                \n",
    "                train_step += 1\n",
    "                num_batches += 1\n",
    "        \n",
    "        # Compute epoch metrics\n",
    "        epoch_loss /= num_batches if num_batches > 0 else 1\n",
    "        epoch_accuracy = accuracy_tracker.compute()\n",
    "        print(f\"Epoch {epoch+1} completed: Loss = {epoch_loss:.4f}, Accuracy = {epoch_accuracy:.4f}\")\n",
    "        \n",
    "        # Evaluation phase\n",
    "        print(\"\\nRunning evaluation...\")\n",
    "        eval_metrics, samples = evaluate_model(model, eval_dataloader, device, config.n_eval_samples)\n",
    "        \n",
    "        # Store evaluation metrics\n",
    "        metrics_history[\"eval_loss\"].append(eval_metrics[\"eval_loss\"])\n",
    "        metrics_history[\"eval_accuracy\"].append(eval_metrics[\"eval_accuracy\"])\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        print(f\"Evaluation: Loss = {eval_metrics['eval_loss']:.4f}, Accuracy = {eval_metrics['eval_accuracy']:.4f}\")\n",
    "        \n",
    "        # Print sample generations\n",
    "        print(\"\\nSample generations:\")\n",
    "        for i, sample in enumerate(samples):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"Prompt: {sample['prompt'][:100]}...\")\n",
    "            print(f\"Generated: {sample['generated'][:100]}...\")\n",
    "        \n",
    "        # Save best model\n",
    "        if eval_metrics[\"eval_loss\"] < best_eval_loss:\n",
    "            best_eval_loss = eval_metrics[\"eval_loss\"]\n",
    "            print(f\"\\nNew best model found! Eval loss: {best_eval_loss:.4f}\")\n",
    "            \n",
    "            # We're not saving the model in this notebook to save space\n",
    "            # But you can uncomment the following lines to save the model\n",
    "            # model_save_path = Path(f\"models/alpaca_ft_epoch_{epoch+1}\")\n",
    "            # model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "            # model.save_pretrained(model_save_path, safe_serialization=True)\n",
    "            # tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    return model, metrics_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training\n",
    "\n",
    "Now let's run the training loop to fine-tune our model on the Alpaca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "model, metrics_history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    config=config,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Fine-tuned Model\n",
    "\n",
    "Let's see how our fine-tuned model performs on some test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model on a few examples\n",
    "print(\"Testing fine-tuned model:\\n\")\n",
    "for i in range(5):\n",
    "    # Get a random example from the evaluation set\n",
    "    example = random.choice(eval_dataset)\n",
    "    prompt = example[\"prompt\"]\n",
    "    target = example[\"output\"]\n",
    "    \n",
    "    # Generate a response\n",
    "    generated = generate_response(prompt)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\nTarget response: {target}\")\n",
    "    print(f\"\\nGenerated response: {generated}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot Training Metrics\n",
    "\n",
    "Let's visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import matplotlib for plotting\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics_history[\"train_loss\"])\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    # Plot training and evaluation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics_history[\"train_accuracy\"], label=\"Train\")\n",
    "    \n",
    "    # Add evaluation accuracy points\n",
    "    eval_x = [i * (len(metrics_history[\"train_accuracy\"]) // len(metrics_history[\"eval_accuracy\"])) for i in range(len(metrics_history[\"eval_accuracy\"]))]\n",
    "    plt.plot(eval_x, metrics_history[\"eval_accuracy\"], 'o-', label=\"Eval\")\n",
    "    \n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot learning rate schedule\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(metrics_history[\"learning_rate\"])\n",
    "    plt.title(\"Learning Rate Schedule\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Matplotlib not installed. Skipping plots.\")\n",
    "    print(\"\\nFinal metrics:\")\n",
    "    print(f\"Training loss: {metrics_history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Training accuracy: {metrics_history['train_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Evaluation loss: {metrics_history['eval_loss'][-1]:.4f}\")\n",
    "    print(f\"Evaluation accuracy: {metrics_history['eval_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've implemented instruction fine-tuning using pure PyTorch loops. We've covered:\n",
    "\n",
    "1. **Data Preparation**: Formatting instruction-response pairs for training\n",
    "2. **Efficient Training**: Using techniques like gradient accumulation, mixed precision, and parameter freezing\n",
    "3. **Model Evaluation**: Tracking metrics and sample generations during training\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- Instruction fine-tuning can dramatically improve a model's ability to follow directions and generate helpful responses\n",
    "- Training only a subset of model parameters (LoRA or partial fine-tuning) can significantly reduce computational requirements\n",
    "- Data formatting and prompt engineering are crucial for effective instruction tuning\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll explore how to achieve similar results using the HuggingFace Trainer API, which simplifies the process and adds features like distributed training.\n",
    "\n",
    "You could improve this implementation by:\n",
    "- Using gradient clipping to prevent exploding gradients\n",
    "- Implementing early stopping\n",
    "- Using techniques like LoRA (Low-Rank Adaptation) for more efficient fine-tuning\n",
    "- Exploring different instruction formats or datasets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}