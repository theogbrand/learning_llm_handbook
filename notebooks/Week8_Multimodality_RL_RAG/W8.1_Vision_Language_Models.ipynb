{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8.1: Large Vision Language Models - Core Intuitions\n",
    "\n",
    "**Resource Requirements**: Google Colab (T4) GPU or Provisioned GPU with >8GB VRAM\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How Vision Transformers (ViT) convert images into sequences of patches\n",
    "- The evolution from CLIP to SigLIP for vision-language alignment\n",
    "- How vision and language models are integrated through cross-attention mechanisms\n",
    "- The architecture of PaliGemma as a concrete example of a Vision Language Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the necessary packages for our exploration of Vision Language Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers pillow numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Vision Transformers (ViT)\n",
    "\n",
    "Vision Transformers revolutionized computer vision by applying the transformer architecture to images. The key insight is treating an image as a sequence of patches, similar to how text is treated as a sequence of tokens.\n",
    "\n",
    "### Core Concepts:\n",
    "1. **Patch Embedding**: Divide image into fixed-size patches\n",
    "2. **Positional Encoding**: Add spatial information to patches\n",
    "3. **Self-Attention**: Allow patches to attend to each other\n",
    "4. **Transformer Blocks**: Stack multiple attention + MLP layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Patch Embedding: Converting Images to Sequences\n",
    "\n",
    "The first step in a Vision Transformer is converting an image into a sequence of patch embeddings. Let's implement this step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Converts an image into a sequence of patch embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # Use Conv2d to extract patches and project to embedding dimension\n",
    "        # Kernel size = patch size, stride = patch size for non-overlapping patches\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=num_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            padding=\"valid\"  # No padding\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        # Input shape: [Batch_Size, Channels, Height, Width]\n",
    "        batch_size, channels, height, width = pixel_values.shape\n",
    "        \n",
    "        # Apply convolution to extract patches\n",
    "        # Output shape: [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W]\n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        \n",
    "        # Flatten spatial dimensions and transpose\n",
    "        # Shape: [Batch_Size, Embed_Dim, Num_Patches]\n",
    "        patch_embeds = patch_embeds.flatten(2)\n",
    "        \n",
    "        # Transpose to sequence format\n",
    "        # Final shape: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        patch_embeds = patch_embeds.transpose(1, 2)\n",
    "        \n",
    "        return patch_embeds\n",
    "\n",
    "# Demonstrate patch embedding\n",
    "patch_embed = PatchEmbedding()\n",
    "dummy_image = torch.randn(1, 3, 224, 224)  # Batch of 1 RGB image\n",
    "patches = patch_embed(dummy_image)\n",
    "print(f\"Input image shape: {dummy_image.shape}\")\n",
    "print(f\"Output patches shape: {patches.shape}\")\n",
    "print(f\"Number of patches: {patches.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Positional Embeddings: Adding Spatial Information\n",
    "\n",
    "Since transformers are permutation-invariant, we need to add positional information to help the model understand the spatial arrangement of patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEmbeddings(nn.Module):\n",
    "    \"\"\"Complete embedding layer with patch embedding and positional encoding\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(image_size, patch_size, num_channels, embed_dim)\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # Learnable positional embeddings for each patch position\n",
    "        self.position_embedding = nn.Embedding(self.num_patches, embed_dim)\n",
    "        \n",
    "        # Register buffer for position IDs (not learnable)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_patches).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        # Get patch embeddings\n",
    "        embeddings = self.patch_embedding(pixel_values)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Demonstrate complete embedding\n",
    "vision_embed = VisionEmbeddings()\n",
    "embedded_patches = vision_embed(dummy_image)\n",
    "print(f\"Embedded patches with positions shape: {embedded_patches.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Multi-Head Self-Attention for Vision\n",
    "\n",
    "The core of the Vision Transformer is the self-attention mechanism, which allows patches to attend to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention for vision transformers\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5  # 1/sqrt(d_k) for scaled dot-product\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # [Batch, Seq_Len, Embed_Dim] -> [Batch, Num_Heads, Seq_Len, Head_Dim]\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention scores using Q * K^T / sqrt(d_k)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n",
    "        \n",
    "        # Apply softmax to get attention probabilities\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "        \n",
    "        # Reshape back to [Batch, Seq_Len, Embed_Dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "\n",
    "# Demonstrate attention\n",
    "attention = VisionAttention()\n",
    "attended_patches, attention_weights = attention(embedded_patches)\n",
    "print(f\"Attention output shape: {attended_patches.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Complete Vision Transformer Block\n",
    "\n",
    "A Vision Transformer block combines attention with an MLP and uses residual connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionMLP(nn.Module):\n",
    "    \"\"\"Feed-forward network (MLP) for vision transformer\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformerBlock(nn.Module):\n",
    "    \"\"\"Complete transformer block with attention and MLP\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # Pre-normalization architecture\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = VisionAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = VisionMLP(embed_dim, mlp_ratio, dropout)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # Self-attention block with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states, _ = self.attention(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        # MLP block with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "# Stack multiple transformer blocks\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Complete Vision Transformer model\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, patch_size=16, num_channels=3, \n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.embeddings = VisionEmbeddings(image_size, patch_size, num_channels, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        # Embed patches\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states)\n",
    "            \n",
    "        # Final normalization\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "# Create a small ViT model\n",
    "vit = VisionTransformer(depth=3)  # Using 3 layers for demonstration\n",
    "vision_features = vit(dummy_image)\n",
    "print(f\"Vision Transformer output shape: {vision_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Patch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how an image is divided into patches\n",
    "def visualize_patches(image_size=224, patch_size=16):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Create a sample image with gradient\n",
    "    x = np.linspace(0, 1, image_size)\n",
    "    y = np.linspace(0, 1, image_size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    image = np.sin(5 * X) * np.cos(5 * Y)\n",
    "    \n",
    "    # Show original image\n",
    "    ax1.imshow(image, cmap='viridis')\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Show patches\n",
    "    ax2.imshow(image, cmap='viridis')\n",
    "    \n",
    "    # Draw patch boundaries\n",
    "    for i in range(0, image_size, patch_size):\n",
    "        ax2.axhline(i, color='red', linewidth=0.5)\n",
    "        ax2.axvline(i, color='red', linewidth=0.5)\n",
    "    \n",
    "    ax2.set_title(f'Image divided into {patch_size}x{patch_size} patches')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    fig.suptitle(f'Total number of patches: {num_patches}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Contrastive Learning - From CLIP to SigLIP\n",
    "\n",
    "Contrastive Language-Image Pre-training (CLIP) and its successor SigLIP learn to align vision and language representations in a shared embedding space. This enables zero-shot image classification and forms the foundation for many vision-language models.\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Dual Encoders**: Separate encoders for images and text\n",
    "2. **Contrastive Loss**: Learn to match correct image-text pairs\n",
    "3. **SigLIP Innovation**: Sigmoid loss instead of softmax for better efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Building a CLIP-style Model\n",
    "\n",
    "Let's implement a simplified CLIP architecture to understand the core concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    \"\"\"Simplified CLIP model with vision and text encoders\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_dim=768, text_dim=512, projection_dim=256):\n",
    "        super().__init__()\n",
    "        # Vision encoder (using our ViT)\n",
    "        self.vision_encoder = VisionTransformer(depth=3)\n",
    "        \n",
    "        # Text encoder (simplified - in practice would be a transformer)\n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Embedding(50000, text_dim),  # Vocabulary size 50k\n",
    "            nn.LSTM(text_dim, text_dim, batch_first=True),\n",
    "        )\n",
    "        \n",
    "        # Projection heads to shared space\n",
    "        self.vision_projection = nn.Linear(vision_dim, projection_dim)\n",
    "        self.text_projection = nn.Linear(text_dim, projection_dim)\n",
    "        \n",
    "        # Temperature parameter for contrastive loss\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        \n",
    "    def encode_image(self, pixel_values):\n",
    "        # Encode image to features\n",
    "        vision_features = self.vision_encoder(pixel_values)\n",
    "        # Use mean pooling to get single vector per image\n",
    "        vision_features = vision_features.mean(dim=1)\n",
    "        # Project to shared space and normalize\n",
    "        vision_embeds = self.vision_projection(vision_features)\n",
    "        vision_embeds = F.normalize(vision_embeds, dim=-1)\n",
    "        return vision_embeds\n",
    "    \n",
    "    def encode_text(self, input_ids):\n",
    "        # Encode text to features\n",
    "        text_embeds = self.text_encoder[0](input_ids)  # Embedding\n",
    "        output, (h_n, c_n) = self.text_encoder[1](text_embeds)  # LSTM\n",
    "        # Use the last hidden state from LSTM\n",
    "        text_features = h_n[-1]  # Shape: [batch_size, text_dim]\n",
    "        # Project to shared space and normalize\n",
    "        text_embeds = self.text_projection(text_features)\n",
    "        text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "        return text_embeds\n",
    "    \n",
    "    def forward(self, pixel_values, input_ids):\n",
    "        # Encode both modalities\n",
    "        image_embeds = self.encode_image(pixel_values)\n",
    "        text_embeds = self.encode_text(input_ids)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        logit_scale = self.temperature.exp()\n",
    "        logits_per_image = torch.matmul(image_embeds, text_embeds.t()) * logit_scale\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        \n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "# Create CLIP model\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "# Dummy inputs\n",
    "dummy_images = torch.randn(4, 3, 224, 224)  # Batch of 4 images\n",
    "dummy_text = torch.randint(0, 50000, (4, 20))  # Batch of 4 text sequences\n",
    "\n",
    "logits_per_image, logits_per_text = clip_model(dummy_images, dummy_text)\n",
    "print(f\"Similarity matrix shape (image->text): {logits_per_image.shape}\")\n",
    "print(f\"Similarity matrix shape (text->image): {logits_per_text.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Contrastive Loss Implementation\n",
    "\n",
    "The key to CLIP's training is the contrastive loss that pushes matching pairs together and non-matching pairs apart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_contrastive_loss(logits_per_image, logits_per_text):\n",
    "    \"\"\"Compute CLIP's symmetric cross-entropy loss\"\"\"\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    \n",
    "    # Labels: diagonal elements are positive pairs\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    \n",
    "    # Cross entropy loss for image->text\n",
    "    loss_i2t = F.cross_entropy(logits_per_image, labels)\n",
    "    \n",
    "    # Cross entropy loss for text->image  \n",
    "    loss_t2i = F.cross_entropy(logits_per_text, labels)\n",
    "    \n",
    "    # Total loss is average of both directions\n",
    "    loss = (loss_i2t + loss_t2i) / 2\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Compute loss for our dummy batch\n",
    "loss = clip_contrastive_loss(logits_per_image, logits_per_text)\n",
    "print(f\"Contrastive loss: {loss.item():.4f}\")\n",
    "\n",
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(logits_per_image.detach().numpy(), cmap='RdBu_r')\n",
    "plt.colorbar(label='Similarity')\n",
    "plt.xlabel('Text Index')\n",
    "plt.ylabel('Image Index')\n",
    "plt.title('Image-Text Similarity Matrix\\n(Diagonal should be high after training)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SigLIP: Improving Efficiency with Sigmoid Loss\n",
    "\n",
    "SigLIP replaces CLIP's softmax-based loss with a sigmoid loss, allowing more efficient training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siglip_loss(logits_per_image, logits_per_text):\n",
    "    \"\"\"SigLIP's sigmoid loss - more efficient than CLIP's softmax loss\"\"\"\n",
    "    batch_size = logits_per_image.shape[0]\n",
    "    \n",
    "    # Create target matrix: 1 for matching pairs, -1 for non-matching\n",
    "    targets = torch.eye(batch_size, device=logits_per_image.device) * 2 - 1\n",
    "    \n",
    "    # Sigmoid loss for image->text\n",
    "    loss_i2t = F.logsigmoid(targets * logits_per_image).mean()\n",
    "    \n",
    "    # Sigmoid loss for text->image\n",
    "    loss_t2i = F.logsigmoid(targets * logits_per_text).mean()\n",
    "    \n",
    "    # Total loss\n",
    "    loss = -(loss_i2t + loss_t2i) / 2\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Compare losses\n",
    "clip_loss = clip_contrastive_loss(logits_per_image, logits_per_text)\n",
    "siglip_loss_value = siglip_loss(logits_per_image, logits_per_text)\n",
    "\n",
    "print(f\"CLIP loss: {clip_loss.item():.4f}\")\n",
    "print(f\"SigLIP loss: {siglip_loss_value.item():.4f}\")\n",
    "print(\"\\nKey advantage: SigLIP doesn't require all negative pairs in a batch,\")\n",
    "print(\"enabling more efficient distributed training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Cross-Attention and Multimodal Integration\n",
    "\n",
    "The final piece is understanding how vision and language models are integrated. PaliGemma demonstrates a simple yet effective approach: treating image patches as special tokens in the language model's input sequence.\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Projection Layer**: Align vision features to language model space\n",
    "2. **Token Merging**: Combine image and text tokens in a single sequence  \n",
    "3. **Unified Attention**: Let all tokens attend to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multimodal Projector\n",
    "\n",
    "First, we need to project vision features to the language model's embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalProjector(nn.Module):\n",
    "    \"\"\"Projects vision features to language model embedding space\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_hidden_size=768, text_hidden_size=2048):\n",
    "        super().__init__()\n",
    "        # Simple linear projection\n",
    "        self.linear = nn.Linear(vision_hidden_size, text_hidden_size)\n",
    "        \n",
    "    def forward(self, image_features):\n",
    "        # Project each patch embedding to text embedding dimension\n",
    "        # Input: [Batch_Size, Num_Patches, Vision_Hidden_Size]\n",
    "        # Output: [Batch_Size, Num_Patches, Text_Hidden_Size]\n",
    "        hidden_states = self.linear(image_features)\n",
    "        return hidden_states\n",
    "\n",
    "# Example projection\n",
    "projector = MultiModalProjector(vision_hidden_size=768, text_hidden_size=2048)\n",
    "projected_features = projector(vision_features)\n",
    "print(f\"Vision features shape: {vision_features.shape}\")\n",
    "print(f\"Projected features shape: {projected_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Token Merging Strategy\n",
    "\n",
    "PaliGemma's approach treats image patches as special tokens that are prepended to the text sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vision_and_text_tokens(\n",
    "    image_features,\n",
    "    text_embeddings,\n",
    "    input_ids,\n",
    "    image_token_id=32000,  # Special token ID for <image>\n",
    "    pad_token_id=0\n",
    "):\n",
    "    \"\"\"Merge image features with text embeddings based on special tokens\"\"\"\n",
    "    \n",
    "    batch_size, seq_length = input_ids.shape\n",
    "    _, _, embed_dim = image_features.shape\n",
    "    \n",
    "    # Scale image features to match text embedding magnitude\n",
    "    # This is important for stable training\n",
    "    scaled_image_features = image_features / (embed_dim ** 0.5)\n",
    "    \n",
    "    # Create output tensor\n",
    "    merged_embeddings = torch.zeros(\n",
    "        batch_size, seq_length, embed_dim,\n",
    "        dtype=text_embeddings.dtype,\n",
    "        device=text_embeddings.device\n",
    "    )\n",
    "    \n",
    "    # Create masks for different token types\n",
    "    text_mask = (input_ids != image_token_id) & (input_ids != pad_token_id)\n",
    "    image_mask = input_ids == image_token_id\n",
    "    \n",
    "    # Fill in text embeddings\n",
    "    text_positions = text_mask.nonzero()\n",
    "    if len(text_positions) > 0:\n",
    "        merged_embeddings[text_mask] = text_embeddings[text_mask]\n",
    "    \n",
    "    # Fill in image features at image token positions\n",
    "    # In practice, we need to handle the mapping carefully\n",
    "    # This is a simplified version\n",
    "    for batch_idx in range(batch_size):\n",
    "        image_positions = (image_mask[batch_idx]).nonzero().squeeze(-1)\n",
    "        if len(image_positions) > 0:\n",
    "            num_image_tokens = min(len(image_positions), scaled_image_features.shape[1])\n",
    "            merged_embeddings[batch_idx, image_positions[:num_image_tokens]] = \\\n",
    "                scaled_image_features[batch_idx, :num_image_tokens]\n",
    "    \n",
    "    return merged_embeddings\n",
    "\n",
    "# Demonstrate token merging\n",
    "# Create dummy inputs\n",
    "batch_size = 2\n",
    "num_patches = 16\n",
    "text_seq_len = 10\n",
    "embed_dim = 512\n",
    "\n",
    "# Image features from vision encoder + projector\n",
    "dummy_image_features = torch.randn(batch_size, num_patches, embed_dim)\n",
    "\n",
    "# Text embeddings \n",
    "dummy_text_embeddings = torch.randn(batch_size, text_seq_len + num_patches, embed_dim)\n",
    "\n",
    "# Input IDs with image tokens at the beginning\n",
    "image_token_id = 32000\n",
    "dummy_input_ids = torch.cat([\n",
    "    torch.full((batch_size, num_patches), image_token_id),\n",
    "    torch.randint(1, 30000, (batch_size, text_seq_len))\n",
    "], dim=1)\n",
    "\n",
    "merged = merge_vision_and_text_tokens(\n",
    "    dummy_image_features,\n",
    "    dummy_text_embeddings,\n",
    "    dummy_input_ids,\n",
    "    image_token_id\n",
    ")\n",
    "\n",
    "print(f\"Input IDs shape: {dummy_input_ids.shape}\")\n",
    "print(f\"Merged embeddings shape: {merged.shape}\")\n",
    "print(f\"First few input IDs: {dummy_input_ids[0, :20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Simplified Vision-Language Model\n",
    "\n",
    "Let's put it all together in a simplified vision-language model inspired by PaliGemma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVisionLanguageModel(nn.Module):\n",
    "    \"\"\"Simplified vision-language model inspired by PaliGemma\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_config, text_config):\n",
    "        super().__init__()\n",
    "        # Vision encoder\n",
    "        self.vision_encoder = VisionTransformer(\n",
    "            embed_dim=vision_config['hidden_size'],\n",
    "            depth=vision_config['num_layers']\n",
    "        )\n",
    "        \n",
    "        # Multimodal projector\n",
    "        self.projector = MultiModalProjector(\n",
    "            vision_hidden_size=vision_config['hidden_size'],\n",
    "            text_hidden_size=text_config['hidden_size']\n",
    "        )\n",
    "        \n",
    "        # Text embedding layer\n",
    "        self.text_embeddings = nn.Embedding(\n",
    "            text_config['vocab_size'],\n",
    "            text_config['hidden_size']\n",
    "        )\n",
    "        \n",
    "        # Transformer decoder (simplified)\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=text_config['hidden_size'],\n",
    "                nhead=text_config['num_heads'],\n",
    "                dim_feedforward=text_config['hidden_size'] * 4,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=text_config['num_layers']\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.lm_head = nn.Linear(\n",
    "            text_config['hidden_size'],\n",
    "            text_config['vocab_size']\n",
    "        )\n",
    "        \n",
    "        self.image_token_id = text_config['image_token_id']\n",
    "        \n",
    "    def forward(self, pixel_values, input_ids):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        \n",
    "        # 1. Encode images\n",
    "        vision_features = self.vision_encoder(pixel_values)\n",
    "        \n",
    "        # 2. Project to text space  \n",
    "        image_features = self.projector(vision_features)\n",
    "        \n",
    "        # 3. Get text embeddings\n",
    "        text_embeds = self.text_embeddings(input_ids)\n",
    "        \n",
    "        # 4. Merge image and text embeddings\n",
    "        merged_embeds = merge_vision_and_text_tokens(\n",
    "            image_features,\n",
    "            text_embeds,\n",
    "            input_ids,\n",
    "            self.image_token_id\n",
    "        )\n",
    "        \n",
    "        # 5. Pass through decoder\n",
    "        # Create causal mask for autoregressive generation\n",
    "        seq_len = merged_embeds.shape[1]\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len) * float('-inf'),\n",
    "            diagonal=1\n",
    "        ).to(merged_embeds.device)\n",
    "        \n",
    "        hidden_states = self.decoder(\n",
    "            merged_embeds,\n",
    "            merged_embeds,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "        \n",
    "        # 6. Project to vocabulary\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Configure and create model\n",
    "vision_config = {\n",
    "    'hidden_size': 768,\n",
    "    'num_layers': 3\n",
    "}\n",
    "\n",
    "text_config = {\n",
    "    'hidden_size': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 3,\n",
    "    'vocab_size': 32001,\n",
    "    'image_token_id': 32000\n",
    "}\n",
    "\n",
    "vlm = SimpleVisionLanguageModel(vision_config, text_config)\n",
    "\n",
    "# Test forward pass\n",
    "output_logits = vlm(dummy_images[:2], dummy_input_ids)\n",
    "print(f\"Output logits shape: {output_logits.shape}\")\n",
    "print(f\"Can generate text autoregressively using these logits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Understanding Cross-Modal Attention\n",
    "\n",
    "Let's visualize how image and text tokens attend to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cross_modal_attention(num_image_tokens=16, num_text_tokens=10):\n",
    "    \"\"\"Visualize attention patterns between image and text tokens\"\"\"\n",
    "    \n",
    "    total_tokens = num_image_tokens + num_text_tokens\n",
    "    \n",
    "    # Create a sample attention matrix\n",
    "    # In practice, this would come from the model\n",
    "    attention_matrix = torch.rand(total_tokens, total_tokens)\n",
    "    \n",
    "    # Apply causal mask (each token can only attend to previous tokens)\n",
    "    causal_mask = torch.triu(torch.ones_like(attention_matrix), diagonal=1)\n",
    "    attention_matrix.masked_fill_(causal_mask.bool(), 0)\n",
    "    \n",
    "    # Normalize rows\n",
    "    attention_matrix = F.softmax(attention_matrix + (causal_mask * -1e9), dim=-1)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(attention_matrix.numpy(), cmap='Blues')\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xlabel('Tokens (Image + Text)')\n",
    "    ax.set_ylabel('Tokens (Image + Text)')\n",
    "    ax.set_title('Cross-Modal Attention Pattern\\n(Causal: each token attends only to previous tokens)')\n",
    "    \n",
    "    # Add separators\n",
    "    ax.axvline(x=num_image_tokens-0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=num_image_tokens-0.5, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add region labels\n",
    "    ax.text(num_image_tokens/2, -1, 'Image Tokens', ha='center', va='bottom', color='red')\n",
    "    ax.text(num_image_tokens + num_text_tokens/2, -1, 'Text Tokens', ha='center', va='bottom', color='blue')\n",
    "    ax.text(-1, num_image_tokens/2, 'Image\\nTokens', ha='right', va='center', color='red', rotation=90)\n",
    "    ax.text(-1, num_image_tokens + num_text_tokens/2, 'Text\\nTokens', ha='right', va='center', color='blue', rotation=90)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_cross_modal_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "We've explored the three core components of Vision Language Models:\n",
    "\n",
    "### 1. Vision Transformers (ViT)\n",
    "- Images are divided into patches and treated as sequences\n",
    "- Positional embeddings preserve spatial information\n",
    "- Self-attention enables global reasoning across patches\n",
    "\n",
    "### 2. Contrastive Learning (CLIP/SigLIP)\n",
    "- Dual encoders learn aligned representations for images and text\n",
    "- Contrastive loss pushes matching pairs together\n",
    "- SigLIP improves efficiency with sigmoid loss\n",
    "\n",
    "### 3. Multimodal Integration\n",
    "- Vision features are projected to language model space\n",
    "- Image patches become special tokens in the text sequence\n",
    "- Unified attention enables cross-modal understanding\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "This architecture enables:\n",
    "- **Image Captioning**: Generate descriptions of images\n",
    "- **Visual Question Answering**: Answer questions about images\n",
    "- **Multimodal Reasoning**: Combine visual and textual information\n",
    "- **Zero-shot Image Classification**: Classify images without training\n",
    "\n",
    "The elegance of modern VLMs like PaliGemma lies in their simplicity - by treating images as token sequences, we can leverage the full power of language models for multimodal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "To dive deeper into these concepts:\n",
    "- Original Vision Transformer paper: \"An Image is Worth 16x16 Words\"\n",
    "- CLIP paper: \"Learning Transferable Visual Models From Natural Language Supervision\"\n",
    "- SigLIP paper: \"Sigmoid Loss for Language Image Pre-Training\"\n",
    "- PaliGemma technical report for a complete VLM implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
