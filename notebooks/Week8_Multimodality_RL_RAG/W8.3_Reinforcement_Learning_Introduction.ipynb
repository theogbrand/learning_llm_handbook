{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8.3: Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "**Resource Required**: Google Colab (A100) GPU or Provisioned GPU (RunPod 1x A100 recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "* Understand how the RL agent/action/environment paradigm works in the context of autoregressive transformer models\n",
    "* Learn how the RLHF algorithm works and how it fits on top of PPO (Proximal Policy Optimization)\n",
    "* Understand value heads and how they turn transformers into actor-critic networks\n",
    "* Implement key components of RLHF including KL penalty, advantage estimation, and value heads\n",
    "* Train a transformer model using RLHF with simple reward functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ **NOTE**: This notebook requires substantial GPU memory. We recommend using an A100 GPU. If you're using a less powerful GPU (e.g., A10), set `LOW_GPU_MEM = True` in the code below to use smaller models and batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to RLHF\n",
    "\n",
    "Reinforcement Learning from Human Feedback (RLHF) is a technique for training language models to behave in ways that align with human preferences. It builds on top of standard reinforcement learning algorithms like PPO (Proximal Policy Optimization) but applies them to the unique setting of autoregressive language models.\n",
    "\n",
    "The key insight of RLHF is that we can treat text generation as a sequential decision-making problem where:\n",
    "- The **agent** is our language model\n",
    "- The **environment** is the context/prompt and the text generated so far\n",
    "- **Actions** are the tokens the model generates\n",
    "- **Rewards** come from a reward model trained on human preferences\n",
    "\n",
    "Let's start by installing the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers>=4.40.1 accelerate>=0.27.2 torch einops jaxtyping matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt  # Added for GRPO visualization\n",
    "import random  # Added for countdown problem generation\n",
    "import re  # Added for reward function pattern matching\n",
    "\n",
    "# Configuration for low GPU memory\n",
    "LOW_GPU_MEM = False  # Set to True if using less powerful GPU\n",
    "BASE_MODEL = \"gpt2-small\" if LOW_GPU_MEM else \"gpt2-medium\"\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using model: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RL Concepts in Language Models\n",
    "\n",
    "### States, Actions, and Episodes\n",
    "\n",
    "In the context of language models:\n",
    "\n",
    "- **States** ($s_t$): The entire sequence of tokens generated up to time $t$. This includes both the initial prompt and all tokens generated so far.\n",
    "- **Actions** ($a_t$): The next token to generate. The action space is the entire vocabulary of the model.\n",
    "- **Transitions**: Given state $s_t$ (sequence) and action $a_t$ (new token), the next state is simply the concatenation: $s_{t+1} = [s_t \\; a_t]$\n",
    "- **Episodes**: Each episode starts with a prompt and continues for a fixed number of generated tokens.\n",
    "\n",
    "Let's visualize this with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simple GPT-2 model to demonstrate\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Example of states and actions in language modeling\n",
    "prompt = \"The weather today is\"\n",
    "print(f\"Initial state s_0: '{prompt}'\")\n",
    "print(f\"Action space size: {model.config.vocab_size} (entire vocabulary)\\n\")\n",
    "\n",
    "# Generate one token at a time to show the state transitions\n",
    "current_text = prompt\n",
    "for i in range(3):\n",
    "    inputs = tokenizer(current_text, return_tensors=\"pt\").to(device)\n",
    "    with t.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        next_token_id = outputs.logits[0, -1, :].argmax()\n",
    "        next_token = tokenizer.decode([next_token_id])\n",
    "    \n",
    "    print(f\"Step {i+1}:\")\n",
    "    print(f\"  Current state s_{i}: '{current_text}'\")\n",
    "    print(f\"  Action a_{i} (next token): '{next_token}'\")\n",
    "    current_text += next_token\n",
    "    print(f\"  New state s_{i+1}: '{current_text}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards and Value Functions\n",
    "\n",
    "In RLHF:\n",
    "- **Rewards** $r_t$ are typically computed only at the end of the generation (episode), based on the complete sequence\n",
    "- **Value function** $V(s_t)$ estimates the expected future reward from state $s_t$\n",
    "- Since we only get rewards at the end, there's no discounting (effectively $\\gamma = 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The RLHF Algorithm\n",
    "\n",
    "RLHF consists of three main stages:\n",
    "\n",
    "1. **Supervised Fine-Tuning (SFT)**: Train the base model on high-quality human demonstrations\n",
    "2. **Reward Model Training**: Train a model to predict human preferences between pairs of outputs\n",
    "3. **RL Fine-Tuning**: Use PPO to optimize the model to maximize the reward while staying close to the original model\n",
    "\n",
    "Today we'll focus on stage 3, using a predefined reward function instead of training one from human feedback.\n",
    "\n",
    "### Key Components of RLHF:\n",
    "\n",
    "1. **PPO Objective**: Maximize expected reward while constraining policy changes\n",
    "2. **KL Penalty**: Prevent the model from diverging too far from the reference model\n",
    "3. **Value Head**: Estimate future rewards for advantage calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PPO Intuitions\n",
    "\n",
    "PPO (Proximal Policy Optimization) is the RL algorithm underlying RLHF. The key ideas are:\n",
    "\n",
    "1. **Clipped Objective**: Prevent large policy updates that could destabilize training\n",
    "2. **Advantage Estimation**: Use the difference between actual returns and value estimates\n",
    "3. **Multiple Update Epochs**: Reuse collected data for multiple gradient updates\n",
    "\n",
    "The PPO objective function is:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio\n",
    "- $\\hat{A}_t$ is the advantage estimate\n",
    "- $\\epsilon$ is the clipping parameter (typically 0.2)\n",
    "\n",
    "Let's implement a simple advantage calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def compute_advantages(\n",
    "    values: Float[Tensor, \"batch seq_len\"],\n",
    "    rewards: Float[Tensor, \"batch\"],\n",
    "    prefix_len: int,\n",
    ") -> Float[Tensor, \"batch gen_len\"]:\n",
    "    \"\"\"\n",
    "    Compute advantages for RLHF using simple one-step estimation.\n",
    "    \n",
    "    In RLHF, advantages are computed as:\n",
    "    A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\n",
    "    \n",
    "    Where Q(s_t, a_t) is approximated by V(s_{t+1}) for all tokens\n",
    "    except the last one, where it equals the actual reward.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = values.shape\n",
    "    gen_len = seq_len - prefix_len\n",
    "    \n",
    "    # For tokens before the last, Q = V(next state)\n",
    "    # For the last token, Q = reward\n",
    "    q_values = t.cat([\n",
    "        values[:, prefix_len:],  # V(s_{t+1}) for t < T\n",
    "        rewards[:, None]         # r for t = T\n",
    "    ], dim=1)\n",
    "    \n",
    "    # V(s_t) for all generated tokens\n",
    "    v_values = values[:, prefix_len-1:-1]\n",
    "    \n",
    "    # Advantages = Q - V\n",
    "    advantages = q_values - v_values\n",
    "    \n",
    "    return advantages\n",
    "\n",
    "# Example\n",
    "batch_size, seq_len, prefix_len = 2, 10, 3\n",
    "values = t.randn(batch_size, seq_len)\n",
    "rewards = t.randn(batch_size)\n",
    "advantages = compute_advantages(values, rewards, prefix_len)\n",
    "print(f\"Values shape: {values.shape}\")\n",
    "print(f\"Rewards shape: {rewards.shape}\")\n",
    "print(f\"Advantages shape: {advantages.shape}\")\n",
    "print(f\"Expected advantages shape: ({batch_size}, {seq_len - prefix_len})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Value Heads and Actor-Critic Architecture\n",
    "\n",
    "In RLHF, we use a **value head** - a small neural network attached to the transformer's final layer that estimates the value function. This creates an actor-critic architecture where:\n",
    "\n",
    "- **Actor**: The language model itself (generates actions/tokens)\n",
    "- **Critic**: The value head (estimates future rewards)\n",
    "\n",
    "Both share the same transformer backbone, which allows them to share learned representations.\n",
    "\n",
    "Let's implement a transformer with a value head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithValueHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer model with an additional value head for RLHF.\n",
    "    \n",
    "    The value head is attached to the final layer's output (after LayerNorm)\n",
    "    and produces a scalar value estimate for each token position.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_name: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load the base transformer model\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "        self.config = self.base_model.config\n",
    "        \n",
    "        # Create value head: a 2-layer MLP\n",
    "        # Input: hidden states from transformer (d_model)\n",
    "        # Output: scalar value estimate\n",
    "        d_model = self.config.hidden_size\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: Int[Tensor, \"batch seq_len\"],\n",
    "        attention_mask: Float[Tensor, \"batch seq_len\"] = None\n",
    "    ) -> tuple[Float[Tensor, \"batch seq_len vocab_size\"], Float[Tensor, \"batch seq_len\"]]:\n",
    "        \"\"\"\n",
    "        Forward pass through both the language model and value head.\n",
    "        \n",
    "        Returns:\n",
    "            logits: Token predictions from the language model\n",
    "            values: Value estimates for each position\n",
    "        \"\"\"\n",
    "        # Get transformer outputs\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Extract hidden states from the final layer\n",
    "        # Shape: (batch, seq_len, d_model)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        \n",
    "        # Compute value estimates\n",
    "        # Shape: (batch, seq_len)\n",
    "        values = self.value_head(hidden_states).squeeze(-1)\n",
    "        \n",
    "        return outputs.logits, values\n",
    "\n",
    "# Create and test the model\n",
    "model_with_value_head = TransformerWithValueHead(\"gpt2\").to(device)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = t.randint(0, 1000, (2, 10)).to(device)\n",
    "logits, values = model_with_value_head(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Logits shape: {logits.shape} (batch_size, seq_len, vocab_size)\")\n",
    "print(f\"Values shape: {values.shape} (batch_size, seq_len)\")\n",
    "print(f\"\\nValue head parameters: {sum(p.numel() for p in model_with_value_head.value_head.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. KL Divergence Penalty\n",
    "\n",
    "A crucial component of RLHF is the KL divergence penalty, which prevents the model from deviating too far from the reference model. This helps maintain coherent outputs while optimizing for rewards.\n",
    "\n",
    "The KL penalty is computed as:\n",
    "\n",
    "$$D_{KL}(\\pi_{\\text{new}} || \\pi_{\\text{ref}}) = \\sum_{t} \\mathbb{E}_{a \\sim \\pi_{\\text{new}}} \\left[ \\log \\frac{\\pi_{\\text{new}}(a|s_t)}{\\pi_{\\text{ref}}(a|s_t)} \\right]$$\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kl_penalty(\n",
    "    logits: Float[Tensor, \"batch gen_len vocab_size\"],\n",
    "    ref_logits: Float[Tensor, \"batch gen_len vocab_size\"],\n",
    "    kl_coef: float = 0.1,\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    Calculate KL divergence between new and reference policies.\n",
    "    \n",
    "    This penalizes the model for generating tokens that would be\n",
    "    very unlikely under the reference model.\n",
    "    \"\"\"\n",
    "    # Convert logits to log probabilities (numerically stable)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    ref_log_probs = ref_logits.log_softmax(dim=-1)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = log_probs.exp()\n",
    "    \n",
    "    # KL divergence: sum_a p(a) log(p(a)/q(a))\n",
    "    kl_div = (probs * (log_probs - ref_log_probs)).sum(dim=-1)\n",
    "    \n",
    "    # Average over batch and sequence\n",
    "    kl_penalty = kl_coef * kl_div.mean()\n",
    "    \n",
    "    return kl_penalty\n",
    "\n",
    "# Example\n",
    "batch_size, gen_len, vocab_size = 2, 5, 100\n",
    "logits = t.randn(batch_size, gen_len, vocab_size)\n",
    "ref_logits = t.randn(batch_size, gen_len, vocab_size)\n",
    "\n",
    "kl_penalty = calc_kl_penalty(logits, ref_logits)\n",
    "print(f\"KL penalty: {kl_penalty.item():.4f}\")\n",
    "\n",
    "# When logits are identical, KL should be 0\n",
    "kl_penalty_same = calc_kl_penalty(logits, logits)\n",
    "print(f\"KL penalty (identical distributions): {kl_penalty_same.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simple RLHF Training Loop\n",
    "\n",
    "Now let's put everything together in a simplified RLHF training loop. We'll use a toy reward function that counts periods (.) in the generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple reward function: count periods\n",
    "def reward_fn_period_count(texts: list[str]) -> Float[Tensor, \"batch\"]:\n",
    "    \"\"\"Reward function that counts periods in generated text.\"\"\"\n",
    "    rewards = t.tensor([text.count('.') for text in texts], dtype=t.float32)\n",
    "    return rewards.to(device)\n",
    "\n",
    "@dataclass\n",
    "class RLHFConfig:\n",
    "    \"\"\"Configuration for RLHF training.\"\"\"\n",
    "    # Model\n",
    "    base_model: str = \"gpt2\"\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 4 if LOW_GPU_MEM else 8\n",
    "    num_epochs: int = 10\n",
    "    learning_rate: float = 1e-5\n",
    "    \n",
    "    # Generation\n",
    "    max_gen_len: int = 20\n",
    "    temperature: float = 1.0\n",
    "    \n",
    "    # RLHF specific\n",
    "    kl_coef: float = 0.1\n",
    "    clip_coef: float = 0.2\n",
    "    value_coef: float = 0.5\n",
    "    \n",
    "    # Prompts\n",
    "    prompts: list[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.prompts is None:\n",
    "            self.prompts = [\n",
    "                \"The weather today is\",\n",
    "                \"I think that\",\n",
    "                \"Once upon a time\",\n",
    "                \"In my opinion\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rlhf_step(model, ref_model, tokenizer, config, optimizer):\n",
    "    \"\"\"\n",
    "    Perform one RLHF training step.\n",
    "    \n",
    "    This is a simplified version that demonstrates the key concepts.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # 1. Generate samples from current policy\n",
    "    generated_texts = []\n",
    "    all_input_ids = []\n",
    "    all_values = []\n",
    "    all_logits = []\n",
    "    \n",
    "    for prompt in config.prompts:\n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "        input_ids = inputs.input_ids\n",
    "        \n",
    "        # Generate with current model\n",
    "        with t.no_grad():\n",
    "            outputs = model.base_model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=config.max_gen_len,\n",
    "                temperature=config.temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "            \n",
    "            # Get full generated sequence\n",
    "            generated_ids = outputs.sequences\n",
    "            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            generated_texts.append(generated_text)\n",
    "            \n",
    "            # Get values and logits for generated sequence\n",
    "            logits, values = model(generated_ids)\n",
    "            \n",
    "            all_input_ids.append(generated_ids)\n",
    "            all_values.append(values)\n",
    "            all_logits.append(logits)\n",
    "    \n",
    "    # 2. Compute rewards\n",
    "    rewards = reward_fn_period_count(generated_texts)\n",
    "    \n",
    "    # 3. Compute advantages (simplified)\n",
    "    advantages_list = []\n",
    "    for i, (values, reward) in enumerate(zip(all_values, rewards)):\n",
    "        prompt_len = len(tokenizer.encode(config.prompts[i]))\n",
    "        advantages = compute_advantages(values, reward.unsqueeze(0), prompt_len)\n",
    "        advantages_list.append(advantages)\n",
    "    \n",
    "    # 4. Compute loss components\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(len(config.prompts)):\n",
    "        # Get reference model logits\n",
    "        with t.no_grad():\n",
    "            ref_outputs = ref_model(all_input_ids[i])\n",
    "            ref_logits = ref_outputs.logits if hasattr(ref_outputs, 'logits') else ref_outputs[0]\n",
    "        \n",
    "        # Compute KL penalty\n",
    "        gen_start = len(tokenizer.encode(config.prompts[i]))\n",
    "        kl_loss = calc_kl_penalty(\n",
    "            all_logits[i][:, gen_start:],\n",
    "            ref_logits[:, gen_start:],\n",
    "            config.kl_coef\n",
    "        )\n",
    "        \n",
    "        # Value function loss (simplified)\n",
    "        value_loss = config.value_coef * advantages_list[i].pow(2).mean()\n",
    "        \n",
    "        total_loss += kl_loss + value_loss\n",
    "    \n",
    "    # 5. Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss.item(),\n",
    "        'mean_reward': rewards.mean().item(),\n",
    "        'generated_texts': generated_texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and training\n",
    "config = RLHFConfig()\n",
    "model = TransformerWithValueHead(config.base_model).to(device)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(config.base_model).to(device)\n",
    "ref_model.eval()  # Reference model stays frozen\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Optimizer with different learning rates for base model and value head\n",
    "optimizer = t.optim.Adam([\n",
    "    {'params': model.base_model.parameters(), 'lr': config.learning_rate},\n",
    "    {'params': model.value_head.parameters(), 'lr': config.learning_rate * 10}\n",
    "])\n",
    "\n",
    "print(\"Starting RLHF training...\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config.num_epochs):\n",
    "    results = simple_rlhf_step(model, ref_model, tokenizer, config, optimizer)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(f\"  Loss: {results['loss']:.4f}\")\n",
    "    print(f\"  Mean Reward: {results['mean_reward']:.2f}\")\n",
    "    print(f\"  Sample: {results['generated_texts'][0][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Observing RLHF Behavior\n",
    "\n",
    "As training progresses, you should observe:\n",
    "\n",
    "1. **Increasing rewards**: The model learns to generate more periods\n",
    "2. **Behavioral changes**: Sentences may become shorter or the model may find creative ways to include periods\n",
    "3. **KL constraint effect**: The model won't completely collapse into generating only periods due to the KL penalty\n",
    "\n",
    "Common strategies the model might learn:\n",
    "- Shorter sentences\n",
    "- Abbreviations (e.g., \"U.S.\", \"Dr.\")\n",
    "- Decimal numbers\n",
    "- Lists with periods\n",
    "\n",
    "Let's examine the model's behavior before and after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(prompt, ref_model, trained_model, tokenizer, max_length=50):\n",
    "    \"\"\"Compare outputs from reference and RLHF-trained models.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate from reference model\n",
    "    with t.no_grad():\n",
    "        ref_output = ref_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        ref_text = tokenizer.decode(ref_output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Generate from trained model\n",
    "        trained_output = trained_model.base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        trained_text = tokenizer.decode(trained_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(f\"Reference model output:\\n{ref_text}\\n\")\n",
    "    print(f\"Period count: {ref_text.count('.')}\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"RLHF model output:\\n{trained_text}\\n\")\n",
    "    print(f\"Period count: {trained_text.count('.')}\")\n",
    "\n",
    "# Compare outputs\n",
    "test_prompts = [\n",
    "    \"The most important thing about science is\",\n",
    "    \"I went to the store and bought\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    compare_models(prompt, ref_model, model, tokenizer)\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, we've covered the fundamentals of RLHF:\n",
    "\n",
    "1. **RL Framework for Language**: How text generation maps to the RL paradigm with states (sequences), actions (tokens), and rewards\n",
    "\n",
    "2. **PPO Algorithm**: The core RL algorithm used in RLHF, with its clipped objective and advantage estimation\n",
    "\n",
    "3. **Value Heads**: How to extend transformers with value estimation capabilities for actor-critic training\n",
    "\n",
    "4. **KL Penalty**: The crucial constraint that keeps RLHF models from diverging too far from sensible outputs\n",
    "\n",
    "5. **Implementation**: A simplified but functional RLHF training loop demonstrating these concepts\n",
    "\n",
    "### Real-World RLHF\n",
    "\n",
    "In practice, RLHF systems are more complex:\n",
    "- **Human feedback**: Reward models are trained on actual human preferences\n",
    "- **Scale**: Training happens on much larger models with more sophisticated infrastructure\n",
    "- **Safety**: Additional constraints ensure helpful, harmless, and honest outputs\n",
    "- **Efficiency**: Techniques like LoRA reduce computational requirements\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT paper)\n",
    "- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)\n",
    "- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) (DPO - a simpler alternative to RLHF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. GRPO: A Simpler Approach to RL for LLMs\n",
    "\n",
    "### What is GRPO?\n",
    "\n",
    "**Group Relative Policy Optimization (GRPO)** is a simplified variant of PPO that has gained popularity in recent LLM training, particularly with models like DeepSeek-R1. The key insight is that instead of computing advantages for each token individually, GRPO computes a single advantage score for entire responses and uses group-based normalization.\n",
    "\n",
    "### The GRPO Recipe\n",
    "\n",
    "Here's the high-level procedure:\n",
    "\n",
    "1. **Start** with a base LLM and a dataset containing problem prompts paired with their final answers\n",
    "2. For each training iteration:\n",
    "   - Sample a batch of prompts from the dataset\n",
    "   - For each prompt, sample G responses from the model (forming a \"group\")\n",
    "   - Compute a reward for each response\n",
    "   - Normalize rewards within each group to calculate advantages\n",
    "   - Update the model using these advantages\n",
    "\n",
    "### Key Differences from Standard PPO\n",
    "\n",
    "1. **Response-Level Advantages**: While PPO typically computes advantages token-by-token, GRPO assigns the same advantage to all tokens in a response\n",
    "2. **Group Normalization**: For each prompt, multiple responses are generated (a \"group\"), and advantages are normalized within this group\n",
    "3. **Simpler Implementation**: No need for complex value function bootstrapping or GAE (Generalized Advantage Estimation)\n",
    "4. **Fully Online**: Each batch of data is used for only one gradient update\n",
    "\n",
    "Let's understand GRPO through a concrete example - the Countdown task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Countdown Task: A Perfect Example for GRPO\n",
    "\n",
    "\"\"\"\n",
    "The Countdown game is a numerical puzzle where you must reach a target number \n",
    "using a set of given numbers and basic arithmetic operations (+, -, *, /).\n",
    "Each number can only be used once.\n",
    "\n",
    "Example:\n",
    "    Target: 622\n",
    "    Available Numbers: [25, 3, 6, 100]\n",
    "    Solution: (100 Ã— 6) + (25 âˆ’ 3) = 622\n",
    "\n",
    "This task is ideal for GRPO because:\n",
    "1. Rewards are clear (correct answer = 1, wrong = 0)\n",
    "2. The task encourages multi-step reasoning\n",
    "3. Models can learn to verify and self-correct\n",
    "\"\"\"\n",
    "\n",
    "# Let's create a simple countdown problem generator\n",
    "def generate_countdown_problem():\n",
    "    \"\"\"Generate a random countdown problem.\"\"\"\n",
    "    # Generate random numbers\n",
    "    numbers = [random.choice([1, 2, 5, 10, 25, 50, 75, 100]) for _ in range(4)]\n",
    "    \n",
    "    # Create a target by randomly combining some numbers\n",
    "    # (This is simplified - real countdown uses more complex generation)\n",
    "    ops = ['+', '-', '*']\n",
    "    op1, op2 = random.choices(ops, k=2)\n",
    "    a, b, c = random.sample(numbers, 3)\n",
    "    \n",
    "    if op1 == '+':\n",
    "        temp = a + b\n",
    "    elif op1 == '-':\n",
    "        temp = abs(a - b)  # Keep positive\n",
    "    else:  # '*'\n",
    "        temp = a * b\n",
    "        \n",
    "    if op2 == '+':\n",
    "        target = temp + c\n",
    "    elif op2 == '-':\n",
    "        target = abs(temp - c)\n",
    "    else:  # '*'\n",
    "        target = temp * c\n",
    "    \n",
    "    return numbers, target\n",
    "\n",
    "# Generate an example problem\n",
    "numbers, target = generate_countdown_problem()\n",
    "print(f\"Countdown Problem:\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Available Numbers: {numbers}\")\n",
    "print(f\"\\nGoal: Create an equation using these numbers to reach {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPO Advantage Computation\n",
    "\n",
    "The core of GRPO is its group-relative advantage computation. Let's implement and visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_advantages(\n",
    "    rewards: Float[Tensor, \"group_size\"],\n",
    ") -> Float[Tensor, \"group_size\"]:\n",
    "    \"\"\"\n",
    "    Compute GRPO advantages using group-relative normalization.\n",
    "    \n",
    "    For each prompt x with generated responses y_1, ..., y_G and rewards R_1, ..., R_G:\n",
    "    1. Compute mean: Î¼ = mean(R_1, ..., R_G)\n",
    "    2. Compute std: Ïƒ = std(R_1, ..., R_G)\n",
    "    3. Compute advantage for response i: A_i = (R_i - Î¼) / Ïƒ\n",
    "    \n",
    "    This normalization encourages responses better than average and\n",
    "    discourages those worse than average.\n",
    "    \"\"\"\n",
    "    mean_reward = rewards.mean()\n",
    "    std_reward = rewards.std() + 1e-4  # Add epsilon for numerical stability\n",
    "    \n",
    "    # Normalize rewards to get advantages\n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    \n",
    "    return advantages\n",
    "\n",
    "# Let's see how GRPO advantages work in different scenarios\n",
    "print(\"GRPO Advantage Examples for Countdown Task:\\n\")\n",
    "\n",
    "# Scenario 1: Binary rewards (correct/incorrect)\n",
    "print(\"1. Binary rewards (typical for countdown):\")\n",
    "rewards_binary = t.tensor([1.0, 1.0, 0.0, 0.0, 0.0])  # 2 correct, 3 incorrect\n",
    "advantages_binary = compute_grpo_advantages(rewards_binary)\n",
    "print(f\"   Rewards: {rewards_binary.tolist()}\")\n",
    "print(f\"   Advantages: {[f'{a:.3f}' for a in advantages_binary.tolist()]}\")\n",
    "print(f\"   â†’ Correct answers get positive advantage, incorrect get negative\\n\")\n",
    "\n",
    "# Scenario 2: All incorrect (hard problem)\n",
    "print(\"2. All incorrect responses:\")\n",
    "rewards_all_wrong = t.tensor([0.0, 0.0, 0.0, 0.0])\n",
    "advantages_all_wrong = compute_grpo_advantages(rewards_all_wrong)\n",
    "print(f\"   Rewards: {rewards_all_wrong.tolist()}\")\n",
    "print(f\"   Advantages: {advantages_all_wrong.tolist()}\")\n",
    "print(f\"   â†’ No learning signal when all responses are equally bad\\n\")\n",
    "\n",
    "# Scenario 3: All correct (easy problem)\n",
    "print(\"3. All correct responses:\")\n",
    "rewards_all_correct = t.tensor([1.0, 1.0, 1.0, 1.0])\n",
    "advantages_all_correct = compute_grpo_advantages(rewards_all_correct)\n",
    "print(f\"   Rewards: {rewards_all_correct.tolist()}\")\n",
    "print(f\"   Advantages: {advantages_all_correct.tolist()}\")\n",
    "print(f\"   â†’ No learning signal when all responses are equally good\\n\")\n",
    "\n",
    "# Scenario 4: One standout response\n",
    "print(\"4. One exceptional response:\")\n",
    "rewards_one_great = t.tensor([1.0, 0.0, 0.0, 0.0])\n",
    "advantages_one_great = compute_grpo_advantages(rewards_one_great)\n",
    "print(f\"   Rewards: {rewards_one_great.tolist()}\")\n",
    "print(f\"   Advantages: {[f'{a:.3f}' for a in advantages_one_great.tolist()]}\")\n",
    "print(f\"   â†’ Strong positive signal for the correct response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Functions for Countdown\n",
    "\n",
    "Now let's implement the reward functions used in R1-style training. These rewards encourage both correctness and good reasoning format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completion: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward proper formatting: <think>...</think>\\\\n<answer>...</answer>\n",
    "    \n",
    "    This encourages the model to:\n",
    "    1. Show its reasoning process in <think> tags\n",
    "    2. Provide a clean final answer in <answer> tags\n",
    "    \"\"\"\n",
    "    # Check for the expected format\n",
    "    pattern = r\"<think>.*?</think>\\s*\\n\\s*<answer>.*?</answer>\"\n",
    "    if re.search(pattern, completion, re.DOTALL):\n",
    "        # Check if answer contains only valid mathematical expression\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", completion, re.DOTALL)\n",
    "        if answer_match:\n",
    "            answer_content = answer_match.group(1).strip()\n",
    "            # Only numbers, operators, parentheses, and whitespace allowed\n",
    "            if re.match(r\"^[\\d+\\-*/().\\s]+$\", answer_content):\n",
    "                return 1.0  # Perfect format\n",
    "            else:\n",
    "                return 0.5  # Good structure, but answer has extra text\n",
    "    return 0.0  # Wrong format\n",
    "\n",
    "def equation_reward_func(completion: str, nums: list[int], target: int) -> float:\n",
    "    \"\"\"\n",
    "    Reward correct mathematical solutions.\n",
    "    \n",
    "    Checks:\n",
    "    1. The equation evaluates to the target\n",
    "    2. All and only the given numbers are used\n",
    "    3. Each number is used at most once\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract answer from tags\n",
    "        match = re.search(r\"<answer>(.*?)</answer>\", completion)\n",
    "        if not match:\n",
    "            return 0.0\n",
    "            \n",
    "        equation = match.group(1).strip()\n",
    "        \n",
    "        # Extract all numbers from the equation\n",
    "        used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n",
    "        \n",
    "        # Check if all numbers are used exactly once\n",
    "        if sorted(used_numbers) != sorted(nums):\n",
    "            return 0.0\n",
    "            \n",
    "        # Safely evaluate the equation\n",
    "        try:\n",
    "            result = eval(equation, {\"__builtins__\": {}}, {})\n",
    "            if abs(float(result) - float(target)) < 1e-5:\n",
    "                return 1.0\n",
    "        except:\n",
    "            return 0.0\n",
    "            \n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "# Test the reward functions\n",
    "print(\"Testing Reward Functions:\\n\")\n",
    "\n",
    "# Good example\n",
    "good_completion = \"\"\"<think>\n",
    "I need to make 24 using [2, 3, 4, 6].\n",
    "Let me try: 6 * 4 = 24. Yes, that works!\n",
    "</think>\n",
    "<answer>6 * 4</answer>\"\"\"\n",
    "\n",
    "print(\"1. Well-formatted correct answer:\")\n",
    "print(f\"   Format reward: {format_reward_func(good_completion)}\")\n",
    "print(f\"   Equation reward: {equation_reward_func(good_completion, [2, 3, 4, 6], 24)}\")\n",
    "\n",
    "# Bad format example\n",
    "bad_format = \"The answer is 6 * 4 = 24\"\n",
    "print(\"\\n2. Correct answer, wrong format:\")\n",
    "print(f\"   Format reward: {format_reward_func(bad_format)}\")\n",
    "print(f\"   Equation reward: {equation_reward_func(bad_format, [2, 3, 4, 6], 24)}\")\n",
    "\n",
    "# Wrong answer example\n",
    "wrong_answer = \"\"\"<think>\n",
    "Let me calculate...\n",
    "</think>\n",
    "<answer>2 + 3 + 4</answer>\"\"\"\n",
    "\n",
    "print(\"\\n3. Well-formatted wrong answer:\")\n",
    "print(f\"   Format reward: {format_reward_func(wrong_answer)}\")\n",
    "print(f\"   Equation reward: {equation_reward_func(wrong_answer, [2, 3, 4, 6], 24)}\")\n",
    "\n",
    "# Complex correct example\n",
    "complex_correct = \"\"\"<think>\n",
    "I have [10, 25, 5, 2] and need to make 100.\n",
    "Let me think: 25 * 5 = 125, too big.\n",
    "Actually, (25 - 5) * 10 / 2 = 20 * 10 / 2 = 200 / 2 = 100!\n",
    "</think>\n",
    "<answer>(25 - 5) * 10 / 2</answer>\"\"\"\n",
    "\n",
    "print(\"\\n4. Complex reasoning with correct answer:\")\n",
    "print(f\"   Format reward: {format_reward_func(complex_correct)}\")\n",
    "print(f\"   Equation reward: {equation_reward_func(complex_correct, [10, 25, 5, 2], 100)}\")\n",
    "\n",
    "# Demonstrate combined reward\n",
    "def compute_total_reward(completion: str, nums: list[int], target: int) -> float:\n",
    "    \"\"\"Combined reward function as used in R1-style training.\"\"\"\n",
    "    format_r = format_reward_func(completion)\n",
    "    equation_r = equation_reward_func(completion, nums, target)\n",
    "    return format_r + equation_r\n",
    "\n",
    "print(f\"\\n   Total reward: {compute_total_reward(complex_correct, [10, 25, 5, 2], 100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPO Episode Creation\n",
    "\n",
    "In GRPO, episode creation is different from standard RL. We generate multiple responses per prompt and assign uniform advantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grpo_training_episodes(\n",
    "    samples: list[dict],\n",
    "    all_generations: list[list[int]], \n",
    "    tokenizer,\n",
    "    generations_per_sample: int = 4\n",
    ") -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Create training episodes for GRPO from generated responses.\n",
    "    \n",
    "    Key aspects:\n",
    "    1. Groups responses by prompt (generations_per_sample per prompt)\n",
    "    2. Computes rewards and normalizes them within each group\n",
    "    3. Assigns the same advantage to ALL tokens in a response\n",
    "    \n",
    "    This is the key difference from PPO: uniform advantages across tokens!\n",
    "    \"\"\"\n",
    "    # Group indices for responses\n",
    "    groups = [\n",
    "        list(range(i, i + generations_per_sample))\n",
    "        for i in range(0, len(all_generations), generations_per_sample)\n",
    "    ]\n",
    "    \n",
    "    all_query_ids = []\n",
    "    all_response_ids = []\n",
    "    all_advantages = []\n",
    "    \n",
    "    stats = {\n",
    "        'rewards': [],\n",
    "        'format_rewards': [],\n",
    "        'equation_rewards': [],\n",
    "        'response_lengths': []\n",
    "    }\n",
    "    \n",
    "    for sample, group_indices in zip(samples, groups):\n",
    "        # Get responses for this group\n",
    "        response_ids = [all_generations[i] for i in group_indices]\n",
    "        responses = tokenizer.batch_decode(response_ids, skip_special_tokens=False)\n",
    "        \n",
    "        # Compute rewards for each response\n",
    "        rewards = []\n",
    "        format_rewards = []\n",
    "        equation_rewards = []\n",
    "        \n",
    "        for resp in responses:\n",
    "            f_reward = format_reward_func(resp)\n",
    "            e_reward = equation_reward_func(resp, sample['nums'], sample['target'])\n",
    "            total_reward = f_reward + e_reward\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "            format_rewards.append(f_reward)\n",
    "            equation_rewards.append(e_reward)\n",
    "        \n",
    "        # Convert to tensors and compute GRPO advantages\n",
    "        rewards_tensor = t.tensor(rewards, dtype=t.float32)\n",
    "        advantages = compute_grpo_advantages(rewards_tensor)\n",
    "        \n",
    "        # Create episodes with uniform advantages\n",
    "        for i, (resp_ids, adv) in enumerate(zip(response_ids, advantages)):\n",
    "            # CRITICAL: Assign same advantage to ALL tokens in the response\n",
    "            uniform_advantages = [adv.item()] * len(resp_ids)\n",
    "            \n",
    "            all_query_ids.append(sample['input_ids'])\n",
    "            all_response_ids.append(resp_ids)\n",
    "            all_advantages.append(uniform_advantages)\n",
    "            \n",
    "            stats['response_lengths'].append(len(resp_ids))\n",
    "        \n",
    "        # Record statistics\n",
    "        stats['rewards'].extend(rewards)\n",
    "        stats['format_rewards'].extend(format_rewards)\n",
    "        stats['equation_rewards'].extend(equation_rewards)\n",
    "    \n",
    "    episodes = {\n",
    "        'all_query_token_ids': all_query_ids,\n",
    "        'all_response_token_ids': all_response_ids,\n",
    "        'all_advantages': all_advantages\n",
    "    }\n",
    "    \n",
    "    return episodes, stats\n",
    "\n",
    "# Demonstrate episode creation with a mock example\n",
    "print(\"GRPO Episode Creation Example:\\n\")\n",
    "\n",
    "# Mock data\n",
    "mock_sample = {\n",
    "    'input_ids': [1, 2, 3, 4, 5],  # Tokenized prompt\n",
    "    'nums': [2, 3, 4, 6],\n",
    "    'target': 24\n",
    "}\n",
    "\n",
    "# Mock generated responses (token IDs)\n",
    "mock_generations = [\n",
    "    [10, 11, 12, 13],  # Response 1\n",
    "    [20, 21, 22],      # Response 2  \n",
    "    [30, 31, 32, 33, 34],  # Response 3\n",
    "    [40, 41]           # Response 4\n",
    "]\n",
    "\n",
    "# Mock tokenizer decode (for demonstration)\n",
    "class MockTokenizer:\n",
    "    def batch_decode(self, ids, skip_special_tokens=False):\n",
    "        # Return different quality responses\n",
    "        return [\n",
    "            \"<think>6 * 4 = 24</think>\\n<answer>6 * 4</answer>\",  # Perfect\n",
    "            \"<think>Let me see...</think>\\n<answer>2 + 3</answer>\",  # Wrong\n",
    "            \"The answer is 24\",  # Bad format\n",
    "            \"<think>Hmm</think>\\n<answer>4 * 6</answer>\"  # Good\n",
    "        ]\n",
    "\n",
    "mock_tokenizer = MockTokenizer()\n",
    "\n",
    "# Create episodes\n",
    "episodes, stats = create_grpo_training_episodes(\n",
    "    [mock_sample], \n",
    "    mock_generations,\n",
    "    mock_tokenizer,\n",
    "    generations_per_sample=4\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(episodes['all_response_token_ids'])} episodes\\n\")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"Episode {i+1}:\")\n",
    "    print(f\"  Response length: {stats['response_lengths'][i]} tokens\")\n",
    "    print(f\"  Rewards: format={stats['format_rewards'][i]:.1f}, \"\n",
    "          f\"equation={stats['equation_rewards'][i]:.1f}, \"\n",
    "          f\"total={stats['rewards'][i]:.1f}\")\n",
    "    print(f\"  Advantage: {episodes['all_advantages'][i][0]:.3f}\")\n",
    "    print(f\"  All tokens get same advantage: {len(episodes['all_advantages'][i])} Ã— {episodes['all_advantages'][i][0]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPO vs PPO: Visualizing the Difference\n",
    "\n",
    "Let's visualize how GRPO's uniform advantage assignment differs from PPO's token-level advantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing GRPO vs PPO advantage assignment\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Simulated token sequence for a countdown solution\n",
    "token_labels = ['<think>', 'I', 'need', 'to', 'calculate', '25', '*', '4', '=', '100', '</think>', '\\\\n', '<answer>', '25', '*', '4', '</answer>']\n",
    "num_tokens = len(token_labels)\n",
    "\n",
    "# GRPO: Successful response (all tokens get same positive advantage)\n",
    "grpo_advantages_success = [0.8] * num_tokens\n",
    "ax1.bar(range(num_tokens), grpo_advantages_success, color='green', alpha=0.7)\n",
    "ax1.set_xticks(range(num_tokens))\n",
    "ax1.set_xticklabels(token_labels, rotation=45, ha='right')\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.set_ylabel('Advantage')\n",
    "ax1.set_title('GRPO: Successful Response (Correct Answer)\\\\nAll tokens get the same positive advantage')\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# GRPO: Failed response (all tokens get same negative advantage)\n",
    "token_labels_fail = ['<think>', 'Maybe', 'it\\\\'s', '25', '+', '4', '?', '</think>', '\\\\n', '<answer>', '25', '+', '4', '</answer>']\n",
    "num_tokens_fail = len(token_labels_fail)\n",
    "grpo_advantages_fail = [-0.8] * num_tokens_fail\n",
    "ax2.bar(range(num_tokens_fail), grpo_advantages_fail, color='red', alpha=0.7)\n",
    "ax2.set_xticks(range(num_tokens_fail))\n",
    "ax2.set_xticklabels(token_labels_fail, rotation=45, ha='right')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.set_ylabel('Advantage')\n",
    "ax2.set_xlabel('Token Position')\n",
    "ax2.set_title('GRPO: Failed Response (Wrong Answer)\\\\nAll tokens get the same negative advantage')\n",
    "ax2.set_ylim(-1.5, 1.5)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nKey GRPO Characteristics:\")\n",
    "print(\"â€¢ âœ… Correct responses: ALL tokens are reinforced equally\")\n",
    "print(\"â€¢ âŒ Wrong responses: ALL tokens are discouraged equally\")\n",
    "print(\"â€¢ ðŸ“Š The model learns which COMPLETE responses lead to success\")\n",
    "print(\"â€¢ ðŸŽ¯ This encourages coherent, multi-step reasoning patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPO Policy Gradient\n",
    "\n",
    "Since GRPO uses fresh samples for each update, the policy gradient simplifies significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_loss(\n",
    "    logits: Float[Tensor, \"batch seq_len vocab_size\"],\n",
    "    ref_logits: Float[Tensor, \"batch seq_len vocab_size\"],\n",
    "    advantages: Float[Tensor, \"batch seq_len\"],\n",
    "    labels: Int[Tensor, \"batch seq_len\"],\n",
    "    kl_coef: float = 0.001\n",
    ") -> tuple[Float[Tensor, \"\"], dict]:\n",
    "    \"\"\"\n",
    "    Compute GRPO loss with KL penalty.\n",
    "    \n",
    "    Key insight: In the fully online setting (one update per batch),\n",
    "    the importance ratio Ï€/Ï€_old = 1, so PPO reduces to vanilla policy gradient!\n",
    "    \n",
    "    Loss = -E[log Ï€(a|s) * A] + Î² * KL(Ï€ || Ï€_ref)\n",
    "    \"\"\"\n",
    "    # Get log probabilities for taken actions\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    ref_log_probs = ref_logits.log_softmax(dim=-1)\n",
    "    \n",
    "    # Gather log probs of actual tokens\n",
    "    batch_size, seq_len = labels.shape\n",
    "    token_log_probs = log_probs.gather(\n",
    "        dim=-1, \n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    \n",
    "    ref_token_log_probs = ref_log_probs.gather(\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    \n",
    "    # Create mask for valid tokens (not padding)\n",
    "    mask = (labels != -100).float()\n",
    "    \n",
    "    # Policy gradient loss (REINFORCE with advantages)\n",
    "    pg_loss = -(token_log_probs * advantages * mask).sum() / mask.sum()\n",
    "    \n",
    "    # KL penalty using the k3 estimator\n",
    "    # KL = E[Ï€_ref/Ï€ - log(Ï€_ref/Ï€) - 1]\n",
    "    ratio = (ref_token_log_probs - token_log_probs).exp()\n",
    "    kl_penalty = (ratio - ratio.log() - 1) * mask\n",
    "    kl_loss = kl_coef * kl_penalty.sum() / mask.sum()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = pg_loss + kl_loss\n",
    "    \n",
    "    metrics = {\n",
    "        'pg_loss': pg_loss.item(),\n",
    "        'kl_penalty': kl_loss.item() / kl_coef,  # Report actual KL\n",
    "        'kl_loss': kl_loss.item()\n",
    "    }\n",
    "    \n",
    "    return total_loss, metrics\n",
    "\n",
    "# Demonstrate the loss calculation\n",
    "print(\"GRPO Loss Calculation Example:\\n\")\n",
    "\n",
    "# Mock data\n",
    "batch_size, seq_len, vocab_size = 2, 10, 100\n",
    "logits = t.randn(batch_size, seq_len, vocab_size)\n",
    "ref_logits = t.randn(batch_size, seq_len, vocab_size)\n",
    "\n",
    "# Advantages: one response has positive, one has negative\n",
    "advantages = t.tensor([\n",
    "    [0.5] * seq_len,  # Good response\n",
    "    [-0.5] * seq_len  # Bad response\n",
    "])\n",
    "\n",
    "# Labels (token IDs, with -100 for padding)\n",
    "labels = t.randint(0, vocab_size, (batch_size, seq_len))\n",
    "labels[:, -2:] = -100  # Last 2 positions are padding\n",
    "\n",
    "# Compute loss\n",
    "loss, metrics = compute_grpo_loss(logits, ref_logits, advantages, labels)\n",
    "\n",
    "print(f\"Total loss: {loss:.4f}\")\n",
    "print(f\"Policy gradient loss: {metrics['pg_loss']:.4f}\")\n",
    "print(f\"KL penalty: {metrics['kl_penalty']:.4f}\")\n",
    "print(f\"KL loss component: {metrics['kl_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"â€¢ PG loss encourages/discourages actions based on advantages\")\n",
    "print(\"â€¢ KL penalty prevents model from deviating too far from reference\")\n",
    "print(\"â€¢ The balance is controlled by kl_coef\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emergent Behaviors from GRPO\n",
    "\n",
    "One of the most exciting aspects of GRPO (as demonstrated in DeepSeek-R1) is how it leads to emergent reasoning behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating emergent behaviors in GRPO training\n",
    "print(\"Emergent Behaviors in GRPO Training:\\n\")\n",
    "\n",
    "# Example responses showing progression over training\n",
    "print(\"Early Training (Simple attempts):\")\n",
    "early_responses = [\n",
    "    \"<think>25 * 4 = 100</think>\\n<answer>25 * 4</answer>\",\n",
    "    \"<think>Try multiplication</think>\\n<answer>5 * 20</answer>\",\n",
    "    \"<think>100 = 100</think>\\n<answer>100</answer>\",\n",
    "    \"<think>25 + 75</think>\\n<answer>25 + 75</answer>\"\n",
    "]\n",
    "\n",
    "# Simulate rewards for early training\n",
    "early_problem = {'nums': [25, 4, 75, 1], 'target': 100}\n",
    "print(f\"\\nProblem: Make {early_problem['target']} using {early_problem['nums']}\")\n",
    "\n",
    "for i, resp in enumerate(early_responses):\n",
    "    f_r = format_reward_func(resp)\n",
    "    e_r = equation_reward_func(resp, early_problem['nums'], early_problem['target'])\n",
    "    print(f\"  Response {i+1}: format={f_r}, equation={e_r}, total={f_r + e_r}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Later Training (Emergent verification & self-correction):\")\n",
    "later_responses = [\n",
    "    \"\"\"<think>\n",
    "I need to make 100 using [25, 4, 75, 1].\n",
    "Let me try: 25 * 4 = 100. \n",
    "Wait, let me verify: 25 * 4 = 100 âœ“\n",
    "</think>\n",
    "<answer>25 * 4</answer>\"\"\",\n",
    "    \n",
    "    \"\"\"<think>\n",
    "Target is 100 with [25, 4, 75, 1].\n",
    "Maybe 75 + 25 = 100? Yes!\n",
    "Actually, let me double-check: 75 + 25 = 100 âœ“\n",
    "</think>\n",
    "<answer>75 + 25</answer>\"\"\",\n",
    "    \n",
    "    \"\"\"<think>\n",
    "I have [25, 4, 75, 1] to make 100.\n",
    "First attempt: 25 * 4 = 100\n",
    "Hmm, but I also have 75 and 1 unused.\n",
    "Actually, the problem says use the given numbers, not all.\n",
    "So 25 * 4 = 100 is correct!\n",
    "</think>\n",
    "<answer>25 * 4</answer>\"\"\",\n",
    "    \n",
    "    \"\"\"<think>\n",
    "Need 100 from [25, 4, 75, 1].\n",
    "Let's see... 75 + 25 = 100\n",
    "Or alternatively: 25 * 4 = 100\n",
    "Both work! I'll go with the simpler one.\n",
    "</think>\n",
    "<answer>25 * 4</answer>\"\"\"\n",
    "]\n",
    "\n",
    "for i, resp in enumerate(later_responses):\n",
    "    f_r = format_reward_func(resp)\n",
    "    e_r = equation_reward_func(resp, early_problem['nums'], early_problem['target'])\n",
    "    print(f\"  Response {i+1}: format={f_r}, equation={e_r}, total={f_r + e_r}\")\n",
    "    \n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Observed Emergent Behaviors:\")\n",
    "print(\"1. âœ“ **Self-verification**: Model starts checking its own answers\")\n",
    "print(\"2. ðŸ”„ **Backtracking**: Model corrects itself when noticing errors\")  \n",
    "print(\"3. ðŸ¤” **Multiple attempts**: Model tries different approaches\")\n",
    "print(\"4. ðŸ“ **Explanation**: Model explains its reasoning process\")\n",
    "print(\"5. ðŸŽ¯ **Meta-reasoning**: Model reasons about the problem constraints\")\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"These behaviors were NOT explicitly programmed or instructed!\")\n",
    "print(\"They emerge naturally from the GRPO training process when\")\n",
    "print(\"the reward structure incentivizes correct final answers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRPO Training Loop Structure\n",
    "\n",
    "Here's the structure of a GRPO training loop, showing how all components fit together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for GRPO training loop\n",
    "print(\"GRPO Training Loop Structure:\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "for iteration in range(num_iterations):\n",
    "    # 1. Sample batch of prompts from dataset\n",
    "    prompts = sample_prompts(dataset, batch_size)\n",
    "    \n",
    "    # 2. Generate multiple responses per prompt\n",
    "    all_responses = []\n",
    "    for prompt in prompts:\n",
    "        for _ in range(generations_per_sample):\n",
    "            response = model.generate(prompt, temperature=1.0)\n",
    "            all_responses.append(response)\n",
    "    \n",
    "    # 3. Create training episodes\n",
    "    episodes, stats = create_grpo_training_episodes(\n",
    "        prompts, \n",
    "        all_responses,\n",
    "        generations_per_sample\n",
    "    )\n",
    "    \n",
    "    # 4. Compute loss and update model\n",
    "    # Key: Only ONE gradient update per batch!\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch in make_batches(episodes):\n",
    "        # Get model and reference model outputs\n",
    "        logits = model(batch.input_ids)\n",
    "        with torch.no_grad():\n",
    "            ref_logits = ref_model(batch.input_ids)\n",
    "        \n",
    "        # Compute GRPO loss\n",
    "        loss = compute_grpo_loss(\n",
    "            logits, \n",
    "            ref_logits,\n",
    "            batch.advantages,\n",
    "            batch.labels,\n",
    "            kl_coef\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # 5. Log metrics\n",
    "    log_metrics(stats)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nKey Differences from Standard PPO:\")\n",
    "print(\"1. **Generation**: G responses per prompt (not just 1)\")\n",
    "print(\"2. **Advantages**: Computed at response level, not token level\")\n",
    "print(\"3. **Updates**: Single gradient step per batch (fully online)\")\n",
    "print(\"4. **Simplicity**: No value function training or bootstrapping\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Show a complete mini example\n",
    "print(\"Mini GRPO Training Example:\\n\")\n",
    "\n",
    "# Setup\n",
    "@dataclass\n",
    "class GRPOConfig:\n",
    "    model_name: str = \"gpt2\"\n",
    "    generations_per_sample: int = 4\n",
    "    kl_coef: float = 0.001\n",
    "    learning_rate: float = 1e-5\n",
    "    batch_size: int = 2\n",
    "\n",
    "config = GRPOConfig()\n",
    "\n",
    "# Create countdown prompts\n",
    "def create_countdown_prompt(nums: list[int], target: int) -> str:\n",
    "    return f\"\"\"Using the numbers {nums}, create an equation that equals {target}.\n",
    "You can use basic arithmetic operations (+, -, *, /) and each number can only be used once.\n",
    "Show your work in <think> </think> tags. \n",
    "Return the final equation in <answer> </answer> tags.\n",
    "\n",
    "Let me solve this step by step.\n",
    "<think>\"\"\"\n",
    "\n",
    "# Example training data\n",
    "training_problems = [\n",
    "    {'nums': [2, 3, 4, 6], 'target': 24},\n",
    "    {'nums': [5, 10, 25, 2], 'target': 100}\n",
    "]\n",
    "\n",
    "print(\"Training Problems:\")\n",
    "for i, prob in enumerate(training_problems):\n",
    "    print(f\"{i+1}. Make {prob['target']} using {prob['nums']}\")\n",
    "    \n",
    "print(\"\\nPrompt format:\")\n",
    "print(create_countdown_prompt([2, 3, 4, 6], 24)[:200] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nGRPO Training Process:\")\n",
    "print(\"1. Generate 4 responses for each problem\")\n",
    "print(\"2. Compute rewards (format + correctness)\")  \n",
    "print(\"3. Normalize rewards within each group â†’ advantages\")\n",
    "print(\"4. Update model to increase probability of good responses\")\n",
    "print(\"5. KL penalty prevents excessive deviation\")\n",
    "\n",
    "print(\"\\nExpected Outcomes:\")\n",
    "print(\"â€¢ Model learns to format responses correctly\")\n",
    "print(\"â€¢ Model discovers successful solution patterns\")\n",
    "print(\"â€¢ Emergent behaviors like verification appear\")\n",
    "print(\"â€¢ Performance improves on held-out test problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Limitations of Task-Specific GRPO: Why Countdown Success Doesn't Generalize\n",
    "\n",
    "While the GRPO implementation we've explored shows impressive results on countdown tasks and mathematical reasoning, it's crucial to understand why this success doesn't automatically translate to general reasoning capabilities. This section examines the fundamental limitations when optimizing GRPO for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-Specific Reward Functions Create Narrow Optimization\n",
    "\n",
    "The countdown task uses two specific reward functions that create a very narrow optimization target:\n",
    "\n",
    "1. **Format Reward**: Enforces strict `<think>...</think><answer>...</answer>` structure\n",
    "2. **Equation Reward**: Validates mathematical correctness and number usage\n",
    "\n",
    "```python\n",
    "# Highly specific reward structure\n",
    "def compute_countdown_reward(completion, nums, target):\n",
    "    format_reward = check_format(completion)  # Binary: 0 or 1\n",
    "    equation_reward = check_equation(completion, nums, target)  # Binary: 0 or 1\n",
    "    return format_reward + equation_reward  # Total: 0, 1, or 2\n",
    "```\n",
    "\n",
    "This creates several issues for generalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Overfitting to Format Rather Than Reasoning\n",
    "\n",
    "GRPO with uniform advantages teaches the model to maximize reward by any means necessary. In countdown tasks, the model learns that:\n",
    "\n",
    "- Always outputting the exact format gets partial reward (0.5-1.0)\n",
    "- Correct equations in the right format get maximum reward (2.0)\n",
    "- Any deviation from format gets zero reward\n",
    "\n",
    "This leads to \"format hacking\" where the model prioritizes structure over reasoning quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Model learns to game the format reward\n",
    "example_responses = [\n",
    "    # Response 1: Good reasoning, wrong format â†’ 0 reward\n",
    "    \"Let me think about this. I'll try 25 Ã— 4 = 100, then 100 - 3 = 97. Actually, let me try...\",\n",
    "    \n",
    "    # Response 2: No reasoning, correct format â†’ 1 reward\n",
    "    \"<think>numbers</think>\\n<answer>random equation</answer>\",\n",
    "    \n",
    "    # Response 3: Minimal reasoning, correct answer â†’ 2 reward\n",
    "    \"<think>try multiply</think>\\n<answer>25 Ã— 4 - 3</answer>\"\n",
    "]\n",
    "\n",
    "# GRPO will prefer Response 2 and 3 over Response 1, even though 1 shows better reasoning process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Binary Rewards Discourage Exploration\n",
    "\n",
    "The countdown task uses binary rewards (0 or 1 for each component), which creates a sparse reward landscape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse reward problem in countdown tasks\n",
    "def visualize_reward_sparsity():\n",
    "    \"\"\"\n",
    "    In countdown, there's no gradient between \"almost correct\" and \"totally wrong\"\n",
    "    \"\"\"\n",
    "    attempts = [\n",
    "        (\"25 Ã— 4 - 3\", 97, 97),      # Correct â†’ reward = 2\n",
    "        (\"25 Ã— 4 - 2\", 98, 97),      # Off by 1 â†’ reward = 1 (format only)\n",
    "        (\"25 Ã— 3 + 6\", 81, 97),      # Off by 16 â†’ reward = 1 (format only)\n",
    "        (\"Invalid syntax\", None, 97), # Syntax error â†’ reward = 0\n",
    "    ]\n",
    "    \n",
    "    print(\"Equation        | Result | Target | Reward | Learning Signal\")\n",
    "    print(\"-\" * 60)\n",
    "    for eq, result, target in attempts:\n",
    "        if result is None:\n",
    "            reward = 0\n",
    "        elif result == target:\n",
    "            reward = 2\n",
    "        else:\n",
    "            reward = 1  # Format is correct\n",
    "        \n",
    "        signal = \"STRONG\" if reward == 2 else (\"WEAK\" if reward == 1 else \"NONE\")\n",
    "        print(f\"{eq:15} | {str(result):6} | {target:6} | {reward:6} | {signal}\")\n",
    "\n",
    "visualize_reward_sparsity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Uniform Advantages Inhibit General Reasoning\n",
    "\n",
    "GRPO's uniform advantage assignment means all tokens in a response get the same credit. This is particularly problematic for general reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem with uniform advantages in general reasoning\n",
    "general_reasoning_example = \"\"\"\n",
    "Question: Why do leaves change color in autumn?\n",
    "\n",
    "Model Response:\n",
    "<think>\n",
    "Leaves change color because of temperature. [INCORRECT]\n",
    "Actually, it's due to chlorophyll breakdown. [CORRECT]\n",
    "This reveals other pigments like carotenoids. [CORRECT]\n",
    "The process is triggered by shorter days. [CORRECT]\n",
    "Trees also stop producing chlorophyll. [CORRECT]\n",
    "</think>\n",
    "<answer>\n",
    "Leaves change color due to chlorophyll breakdown revealing other pigments.\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# In GRPO with uniform advantages:\n",
    "# - ALL tokens get positive advantage if answer is correct\n",
    "# - INCLUDING the incorrect statement about temperature!\n",
    "# - Model doesn't learn which parts of reasoning were actually helpful\n",
    "\n",
    "print(\"With uniform advantages:\")\n",
    "print(\"- Correct final answer â†’ All tokens rewarded equally\")\n",
    "print(\"- Incorrect reasoning steps get same positive advantage\")\n",
    "print(\"- Model learns: 'Any thinking + correct answer = good'\")\n",
    "print(\"- No pressure to improve reasoning quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Mathematical vs General Reasoning Requirements\n",
    "\n",
    "Mathematical reasoning (like countdown) has fundamentally different properties than general reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of task properties\n",
    "task_comparison = {\n",
    "    \"Property\": [\"Countdown/Math\", \"General Reasoning\"],\n",
    "    \"Answer Space\": [\"Single correct answer\", \"Multiple valid answers\"],\n",
    "    \"Verification\": [\"Algorithmic (eval)\", \"Requires human judgment\"],\n",
    "    \"Reasoning Path\": [\"Any path to correct result\", \"Quality of path matters\"],\n",
    "    \"Partial Credit\": [\"Binary (right/wrong)\", \"Degrees of correctness\"],\n",
    "    \"Format Importance\": [\"Just for parsing\", \"Part of communication\"],\n",
    "}\n",
    "\n",
    "# Display comparison\n",
    "for prop, values in task_comparison.items():\n",
    "    if prop == \"Property\":\n",
    "        print(f\"{'Property':<20} | {'Countdown/Math':<25} | {'General Reasoning':<25}\")\n",
    "        print(\"-\" * 75)\n",
    "    else:\n",
    "        print(f\"{prop:<20} | {values[0]:<25} | {values[1]:<25}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generalization Gap: Why Countdown Success Doesn't Transfer\n",
    "\n",
    "When we train GRPO specifically for countdown tasks, the model learns highly specialized behaviors that don't generalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the model actually learns from countdown-specific GRPO\n",
    "learned_behaviors = {\n",
    "    \"Positive Behaviors (Countdown-Specific)\": [\n",
    "        \"âœ“ Always use <think></think> tags\",\n",
    "        \"âœ“ Try multiple arithmetic combinations\",\n",
    "        \"âœ“ Verify equations evaluate correctly\",\n",
    "        \"âœ“ Use each number exactly once\",\n",
    "        \"âœ“ Backtrack when result doesn't match\",\n",
    "    ],\n",
    "    \n",
    "    \"Missing Behaviors (Needed for General Reasoning)\": [\n",
    "        \"âœ— Explain WHY a solution works\",\n",
    "        \"âœ— Consider multiple valid perspectives\",\n",
    "        \"âœ— Build on previous knowledge\",\n",
    "        \"âœ— Admit uncertainty appropriately\",\n",
    "        \"âœ— Provide context and nuance\",\n",
    "        \"âœ— Adapt explanation to audience\",\n",
    "    ],\n",
    "    \n",
    "    \"Harmful Behaviors (Over-optimization)\": [\n",
    "        \"âš  Rigid adherence to format over clarity\",\n",
    "        \"âš  Preference for arithmetic over logic\",\n",
    "        \"âš  Binary thinking (right/wrong only)\",\n",
    "        \"âš  Ignoring reasoning quality if answer is correct\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, behaviors in learned_behaviors.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for behavior in behaviors:\n",
    "        print(f\"  {behavior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways: Requirements for General Reasoning with GRPO\n",
    "\n",
    "To make GRPO work for general reasoning (not just mathematical tasks), we would need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements for general reasoning with GRPO\n",
    "requirements = {\n",
    "    \"1. Richer Reward Functions\": [\n",
    "        \"- Continuous rewards (not just binary)\",\n",
    "        \"- Multiple evaluation criteria\",\n",
    "        \"- Partial credit for good reasoning\",\n",
    "        \"- Human feedback or strong reward models\"\n",
    "    ],\n",
    "    \n",
    "    \"2. Better Credit Assignment\": [\n",
    "        \"- Token-level advantages (like PPO)\",\n",
    "        \"- Process rewards during reasoning\",\n",
    "        \"- Identifying which steps helped/hurt\",\n",
    "        \"- Not uniform advantages across responses\"\n",
    "    ],\n",
    "    \n",
    "    \"3. Diverse Training Tasks\": [\n",
    "        \"- Not just mathematical problems\",\n",
    "        \"- Open-ended questions\",\n",
    "        \"- Creative and analytical tasks\",\n",
    "        \"- Real-world reasoning scenarios\"\n",
    "    ],\n",
    "    \n",
    "    \"4. Evaluation Beyond Correctness\": [\n",
    "        \"- Reasoning quality metrics\",\n",
    "        \"- Explanation clarity\",\n",
    "        \"- Appropriate uncertainty\",\n",
    "        \"- Adaptability to context\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for requirement, details in requirements.items():\n",
    "    print(f\"\\n{requirement}:\")\n",
    "    for detail in details:\n",
    "        print(f\"  {detail}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"While GRPO excels at specific tasks like countdown through emergent behaviors\")\n",
    "print(\"(self-verification, backtracking), these behaviors are task-specific optimizations.\")\n",
    "print(\"General reasoning requires fundamentally different reward structures and training\")\n",
    "print(\"approaches that value reasoning quality, not just final correctness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The R1-Zero Insight: Task-Specific Excellence vs General Intelligence\n",
    "\n",
    "The R1-Zero demonstrates an important principle: **task-specific optimization can produce impressive emergent behaviors** (like self-verification and backtracking in mathematical reasoning), but these behaviors are deeply tied to the reward structure and task characteristics.\n",
    "\n",
    "The success of GRPO on countdown tasks shows:\n",
    "- âœ… GRPO can train models without human feedback\n",
    "- âœ… Emergent behaviors arise from simple reward signals\n",
    "- âœ… Efficient training is possible with proper implementation\n",
    "\n",
    "But it also reveals fundamental limitations:\n",
    "- âŒ Task-specific rewards create narrow capabilities\n",
    "- âŒ Uniform advantages don't teach reasoning quality\n",
    "- âŒ Binary rewards discourage exploration\n",
    "- âŒ Mathematical verification doesn't transfer to general reasoning\n",
    "\n",
    "**The key lesson**: While GRPO represents an important step toward self-improving AI systems, achieving general reasoning capabilities requires moving beyond task-specific optimization to more sophisticated reward modeling and credit assignment mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: GRPO vs PPO\n",
    "\n",
    "**GRPO (Group Relative Policy Optimization)**:\n",
    "- âœ… Simpler to implement and understand\n",
    "- âœ… No value function needed\n",
    "- âœ… Effective for response-level rewards\n",
    "- âœ… Encourages emergent reasoning behaviors\n",
    "- âŒ Less fine-grained credit assignment\n",
    "- âŒ Requires multiple generations per prompt\n",
    "\n",
    "**PPO (Proximal Policy Optimization)**:\n",
    "- âœ… Token-level credit assignment\n",
    "- âœ… More flexible and general\n",
    "- âœ… Can handle dense rewards\n",
    "- âœ… Established track record\n",
    "- âŒ More complex implementation\n",
    "- âŒ Requires value function training\n",
    "\n",
    "### Key Takeaways on GRPO\n",
    "\n",
    "1. **Simplicity is Powerful**: GRPO shows that simpler approaches can yield impressive results\n",
    "2. **Emergent Behaviors**: With the right reward structure, models develop sophisticated reasoning patterns\n",
    "3. **Group Normalization**: Comparing responses within groups provides strong learning signals\n",
    "4. **R1-Zero Success**: GRPO enabled DeepSeek-R1 to develop reasoning abilities without human feedback\n",
    "5. **Practical Choice**: Use GRPO when you have clear response-level rewards and want simpler implementation\n",
    "\n",
    "### The Countdown Task as a Learning Tool\n",
    "\n",
    "The countdown task perfectly illustrates GRPO's strengths:\n",
    "- **Clear success criteria**: Solutions either work or don't\n",
    "- **Multiple valid approaches**: Encourages exploration\n",
    "- **Reasoning required**: Not just pattern matching\n",
    "- **Verifiable**: Models can check their own work\n",
    "\n",
    "This makes it an ideal testbed for understanding how GRPO enables models to develop complex reasoning behaviors from simple reward signals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
